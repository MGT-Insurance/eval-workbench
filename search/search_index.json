{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Eval Workbench","text":"<p> Evaluation tooling and metrics for MGT agent workflows. Built on top of Axion. Eval Workbench separates the core evaluation framework from individual implementations \u2014 enabling better tracking, custom sharable tooling, and a clear separation of concerns. </p> Philosophy <p>       Measure what matters. Automate the rest. </p> <p>       Eval Workbench provides a structured approach to agent evaluation \u2014 from Slack conversation analysis to underwriting recommendation scoring. Define metrics once, run them everywhere.     </p> 1 <p>Shared metric registry</p> <p>Metrics organized by scope \u2014 shared Slack metrics, Athena recommendation metrics \u2014 all versioned, documented, and reusable.</p> 2 <p>YAML-driven monitoring</p> <p>Production monitoring and alerting configured through simple YAML files. No code changes needed to adjust thresholds or add new checks.</p> 3 <p>Langfuse integration</p> <p>Prompt management, trace collection, and webhook-driven cache invalidation with Slack notifications \u2014 all built in.</p> 4 <p>Separation of concerns</p> <p>Core framework stays clean. Implementation-specific logic lives in its own space. Extend without coupling.</p> <p>Get started View on GitHub</p>"},{"location":"#component-arsenal","title":"Component Arsenal","text":"Metric Registry Shared &amp; Scoped Evaluation Metrics <p>Evaluation metrics organized by scope \u2014 shared Slack metrics, Athena recommendation metrics, and more. Versioned, documented, and reusable across implementations.</p> Langfuse Integration Prompt Management &amp; Tracing <p>Prompt management, trace collection, and webhook-driven cache invalidation with Slack notifications. Full observability for every evaluation run.</p> Monitoring &amp; Alerting YAML-Driven Production Pipelines <p>Production monitoring and alerting with YAML-driven configuration. Define thresholds, schedules, and notification channels without touching code.</p> Slack Analytics Conversation Analysis &amp; KPI Tracking <p>Multi-agent evaluation across Slack-based workflows \u2014 sentiment, escalation, resolution, compliance, and more. Every conversation scored automatically.</p> Knowledge Graph Memory Persistent Decision Context <p>Decision memory system for persisting and recalling evaluation context across sessions. Build institutional knowledge from every evaluation cycle.</p> Architecture &amp; Deep Dives System Design, Schema &amp; Runtime Flows <p>Comprehensive documentation of system internals \u2014 database schema, async pipelines, and the full evaluation lifecycle from ingestion to scoring.</p>"},{"location":"#tech-stack","title":"Tech Stack","text":"Component Technology Evaluation framework Axion Prompt management Langfuse Configuration Pydantic Settings Database Neon / PostgreSQL Task orchestration Python async Documentation MkDocs Material"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install the repo in editable mode so all imports resolve:</p> <pre><code>pip install -e .\n</code></pre> <p>Set up pre-commit hooks:</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre> <p>Then head to Getting Started for environment setup and configuration details.</p>"},{"location":"deep-dives/","title":"Deep Dives","text":"<p> System internals, database schema, and runtime flows. These guides go beyond the surface \u2014 covering how the evaluation pipeline is wired together, how data flows from ingestion to scoring, and how persistence is structured. </p> 1 <p>Architecture</p> <p>System diagram, runtime flows, design patterns, and the shared-core vs implementation boundary. The full picture of how Eval Workbench orchestrates evaluation pipelines.</p> <p>Explore</p> 2 <p>Database</p> <p>Neon/PostgreSQL integration \u2014 synchronous and async connection managers, DataFrame I/O, connection pooling, and the QueueExecutor for concurrent task execution.</p> <p>Explore</p> 3 <p>Schema</p> <p>The 3-stage evaluation data model \u2014 from raw input in <code>evaluation_dataset</code>, to metric scores in <code>evaluation_results</code>, to the unified <code>evaluation_view</code>.</p> <p>Explore</p>"},{"location":"deep-dives/architecture/","title":"Architecture","text":"<p> A thin, implementation-focused layer built on top of Axion \u2014 providing online monitoring, offline analysis, prompt management with cache invalidation, and optional Neon/Postgres persistence. </p>"},{"location":"deep-dives/architecture/#key-directories","title":"Key Directories","text":"Shared Core <code>src/shared/</code> <p>Cross-implementation primitives \u2014 monitoring pipeline, Langfuse helpers, Slack helpers, DB helpers, and shared metrics.</p> Implementations <code>src/implementations/athena/</code> <p>Concrete implementation \u2014 extractors, metrics, Langfuse prompt patterns, and YAML monitoring configs.</p> Scripts <code>scripts/</code> <p>Runnable entrypoints, notably <code>scripts/run_monitoring.py</code> for triggering evaluation runs.</p> Automation <code>.github/workflows/</code> <p>CI + scheduled monitoring automation. Hourly cron jobs and manual dispatch for production runs.</p>"},{"location":"deep-dives/architecture/#system-diagram","title":"System Diagram","text":"<pre><code>                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502                 GitHub Actions               \u2502\n                         \u2502                                              \u2502\n                         \u2502  CI: lint/test (push/PR)                      \u2502\n                         \u2502  Scheduled Monitoring: cron (hourly)          \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                                         \u2502 runs\n                                         \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502          scripts/run_monitoring.py         \u2502\n                          \u2502  - parse args/env                           \u2502\n                          \u2502  - load YAML config                          \u2502\n                          \u2502  - set up ScoredItemsStore (optional)        \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                           \u2502\n                                           \u2502 constructs from YAML\n                                           \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502       shared.monitoring.OnlineMonitor      \u2502\n                          \u2502  Responsibilities:                           \u2502\n                          \u2502  - load config (YAML + overrides)            \u2502\n                          \u2502  - fetch from source                          \u2502\n                          \u2502  - deduplicate via ScoredItemsStore           \u2502\n                          \u2502  - sample items (strategy)                    \u2502\n                          \u2502  - run Axion evaluation_runner                \u2502\n                          \u2502  - publish results (Langfuse / Neon)          \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502               \u2502\n                          fetch items \u2502               \u2502 publish\n                                      \u2502               \u2502\n                                      \u25bc               \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 shared.monitoring.sources   \u2502   \u2502        Publishing Sinks       \u2502\n                \u2502                             \u2502   \u2502                              \u2502\n                \u2502  DataSource (interface)     \u2502   \u2502  Langfuse (experiments/obs)   \u2502\n                \u2502   \u251c\u2500 LangfuseDataSource     \u2502   \u2502   - results.publish_as_...     \u2502\n                \u2502   \u2514\u2500 SlackDataSource        \u2502   \u2502                              \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                              \u2502\n                           \u2502           \u2502          \u2502  Neon/Postgres (optional)     \u2502\n                           \u2502           \u2502          \u2502   - shared.database.neon      \u2502\n                 Langfuse  \u2502           \u2502 Slack    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  traces   \u2502           \u2502 messages/threads\n                           \u2502           \u2502\n                           \u25bc           \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  Langfuse API / Axion   \u2502   \u2502            Slack API            \u2502\n          \u2502  - LangfuseTraceLoader  \u2502   \u2502  - shared.slack.* helpers       \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u2502 per-trace extraction\n                      \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502             Implementations (example: Athena)                       \u2502\n     \u2502                                                                    \u2502\n     \u2502  src/implementations/athena/                                        \u2502\n     \u2502   - extractors/*: Trace -&gt; Axion DatasetItem                         \u2502\n     \u2502   - metrics/*: scoring logic (LLM + heuristics)                      \u2502\n     \u2502   - langfuse/prompt_patterns.py: regex extraction patterns           \u2502\n     \u2502   - config/*.yaml: monitor definitions (source, sampling, publish)  \u2502\n     \u2502                                                                    \u2502\n     \u2502  Metric registry (import-time registration)                          \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u2502 calls into\n                                \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502            Axion (external)        \u2502\n                     \u2502  - DatasetItem                     \u2502\n                     \u2502  - evaluation_runner               \u2502\n                     \u2502  - tracing integration             \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>   **Prompt lifecycle / cache invalidation side-channel:**  Langfuse Prompt UI/API sends a `prompt.updated` webhook to the FastAPI app (`shared.langfuse.webhook:app`), which invalidates the local prompt cache, optionally fires a notify URL callback, and posts a Slack alert asynchronously."},{"location":"deep-dives/architecture/#core-runtime-flows","title":"Core Runtime Flows","text":""},{"location":"deep-dives/architecture/#1-scheduled-online-monitoring","title":"1. Scheduled online monitoring","text":"**Trigger:** `.github/workflows/monitoring.yml` runs hourly (cron) or via manual dispatch.  <ol> <li>GitHub Actions runs <code>python scripts/run_monitoring.py &lt;config&gt;.yaml</code></li> <li><code>scripts/run_monitoring.py</code> loads <code>src/implementations/athena/config/&lt;config&gt;.yaml</code> and optionally enables dedup via <code>data/scored_items.csv</code> (cached across workflow runs)</li> <li><code>OnlineMonitor.from_yaml(...)</code> builds a <code>DataSource</code>, a <code>SamplingStrategy</code>, and a metric configuration mapping to registered metrics</li> <li><code>OnlineMonitor.run_async(...)</code> fetches items, filters already-scored items, samples the remainder, and runs Axion <code>evaluation_runner(...)</code></li> <li>If configured, results are published to Langfuse (experiments/observability) and optionally to Neon/Postgres (normalized tables)</li> </ol> <p>Why this works well</p> <p>The pipeline is idempotent (dedup) and safe to run repeatedly on a schedule. Inputs are pluggable (source/extractor) and scoring is configurable (metrics_config).</p>"},{"location":"deep-dives/architecture/#2-localdev-monitoring","title":"2. Local/dev monitoring","text":"**Trigger:** `shared.monitoring.scheduler.MonitoringScheduler` (APScheduler) for local environments.  <p>Add one or more YAML configs with either <code>interval_minutes</code> or <code>cron</code> (crontab string). The scheduler calls <code>monitor.run_async(publish=True)</code> on schedule \u2014 mirroring the GitHub Actions cron path in a long-lived process.</p>"},{"location":"deep-dives/architecture/#3-offline-trace-analysis","title":"3. Offline trace analysis","text":"<p>The trace wrapper layer (<code>shared.langfuse.trace</code>) supports a \"domain-friendly\" view of Langfuse traces:</p> <ul> <li><code>TraceCollection</code> wraps a list of trace payloads</li> <li><code>Trace</code> groups observations into named \"steps\"</li> <li><code>PromptPatternsBase</code> provides regex-based extraction for dot-access: <code>trace.recommendation.variables.some_field</code></li> </ul> <p>This builds <code>DatasetItem</code> objects for evaluation and/or exploratory analysis.</p>"},{"location":"deep-dives/architecture/#4-prompt-management-and-cache-invalidation","title":"4. Prompt management and cache invalidation","text":"<p><code>shared.langfuse.prompt.LangfusePromptManager</code> caches prompt fetches (TTL) and supports explicit invalidation.</p> <ol> <li>A prompt changes in Langfuse (create/update/delete)</li> <li>Langfuse POSTs to <code>shared.langfuse.webhook:app</code> at <code>/webhooks/langfuse</code></li> <li>The webhook verifies a signature, marks the prompt stale, optionally notifies an external listener, and posts a Slack alert asynchronously</li> </ol> <p>Why this matters</p> <p>Keeps \"prompt-as-code\" consumers consistent with the latest prompt versions without restarting long-running processes.</p>"},{"location":"deep-dives/architecture/#architectural-boundaries","title":"Architectural Boundaries","text":"<p> Shared core vs Implementation. <code>src/shared/</code> is the platform \u2014 stable abstractions (monitoring pipeline, data source interface, settings, DB clients). <code>src/implementations/&lt;name&gt;/</code> is product-specific glue \u2014 extractors, metrics, and YAML configs. This separation makes adding a new implementation trivial. </p>"},{"location":"deep-dives/architecture/#design-patterns","title":"Design Patterns","text":"S <p>Strategy</p> <p>Data acquisition: <code>DataSource</code> with <code>LangfuseDataSource</code> and <code>SlackDataSource</code>. Sampling: strategy implementations from config. Extraction: <code>PromptPatternsBase</code> subclasses per implementation.</p> F <p>Factory + Config-as-Code</p> <p><code>OnlineMonitor.from_yaml(...)</code> translates YAML into concrete objects \u2014 loading dotted-path extractors, choosing sources and sampling strategies. Runtime topology is controlled by config, not code edits.</p> R <p>Plugin Registry</p> <p>Implementation metrics registered in a module-level registry. The monitor imports the registry to populate it before evaluation. The runner refers to metrics by name while the implementation decides what exists.</p> A <p>Adapter / Ports-and-Adapters</p> <p>Input ports: <code>DataSource.fetch_items()</code> returning an Axion <code>Dataset</code>. Output ports: publish-to-Langfuse, push-to-DB. Keeps orchestration stable as integrations evolve.</p>"},{"location":"deep-dives/architecture/#idempotency-dedup","title":"Idempotency + Dedup","text":"<p><code>ScoredItemsStore</code> is a simple append-only CSV store keyed by <code>(source_key, monitor_key, item_id)</code>. GitHub Actions caches it across runs, making hourly monitoring \"pick up where it left off.\"</p>"},{"location":"deep-dives/architecture/#settings-layering-12-factor","title":"Settings Layering (12-Factor)","text":"<p>Settings are loaded via Pydantic Settings in two layers:</p> <ul> <li>Repo root <code>.env</code> (global defaults)</li> <li>Per-implementation <code>.env</code> overrides</li> </ul> <p>One codebase supports multiple deployments/implementations with minimal drift.</p>"},{"location":"deep-dives/architecture/#extractors","title":"Extractors","text":"<p>Extractors convert raw data into Axion <code>DatasetItem</code> objects. Each data source (Langfuse traces, database rows, Slack messages) needs an extractor function to normalize the data into a standardized format for evaluation.</p>"},{"location":"deep-dives/architecture/#function-signature-pattern","title":"Function Signature Pattern","text":"<p>All extractors follow the same pattern \u2014 a function that takes a source-specific input and returns a <code>DatasetItem</code>:</p> <pre><code>from axion.dataset import DatasetItem\n\n# Langfuse trace extractor\ndef extract_recommendation(trace: Trace) -&gt; DatasetItem:\n    return DatasetItem(\n        id=trace.id,\n        input={\"query\": ..., \"context\": ...},\n        expected_output=...,\n        actual_output=...,\n        additional_input={...},\n        metadata={...},\n    )\n\n# Database row extractor\ndef extract_recommendation_from_row(row: dict) -&gt; DatasetItem:\n    return DatasetItem(\n        id=row[\"id\"],\n        input={\"quote_locator\": row[\"quote_locator\"]},\n        actual_output=row[\"recommendation\"],\n        metadata={...},\n    )\n</code></pre>"},{"location":"deep-dives/architecture/#athena-extractors","title":"Athena Extractors","text":"<p>Recommendation extractor (<code>implementations/athena/extractors/recommendation.py</code>)</p> <ul> <li><code>extract_recommendation(trace)</code> \u2014 Converts a Langfuse trace from the recommendation step into a DatasetItem. Extracts quote locator, underwriting flags, recommendations, citations, and latency.</li> <li><code>extract_recommendation_from_row(row)</code> \u2014 Alternative extractor for database rows from <code>athena_cases</code>. Sorts recommendation entries by timestamp, extracts the most recent.</li> </ul> <p>Location extraction extractor (<code>implementations/athena/extractors/location_extraction.py</code>)</p> <ul> <li><code>extract_location_extraction(trace)</code> \u2014 Converts a Langfuse trace from the location-extraction step. Extracts quote data, product initiate, and generation output.</li> </ul>"},{"location":"deep-dives/architecture/#writing-a-new-extractor","title":"Writing a New Extractor","text":"<ol> <li>Create a file in <code>implementations/&lt;name&gt;/extractors/</code></li> <li>Write a function matching the signature: <code>(source_data) -&gt; DatasetItem</code></li> <li>Reference it in your monitoring YAML config:</li> </ol> <pre><code>source:\n  type: langfuse\n  extractor: \"eval_workbench.implementations.my_agent.extractors.my_extractor\"\n</code></pre> <p>For Neon sources, use the dotted path to the function:</p> <pre><code>source:\n  type: neon\n  extractor: \"eval_workbench.implementations.my_agent.extractors.extract_from_row\"\n</code></pre>"},{"location":"deep-dives/architecture/#quick-reference","title":"Quick Reference","text":"Core Pipeline Monitoring &amp; Orchestration FilePurpose <code>shared/monitoring/monitor.py</code>Online monitoring orchestrator <code>shared/monitoring/sources.py</code>Data sources <code>shared/monitoring/scored_items.py</code>Dedup store <code>shared/monitoring/scheduler.py</code>Local scheduler Integrations External Systems &amp; Config FilePurpose <code>shared/langfuse/trace.py</code>Trace wrappers <code>shared/langfuse/prompt.py</code>Prompts + caching <code>shared/langfuse/webhook.py</code>Cache invalidation + Slack <code>shared/database/neon.py</code>Neon/Postgres helper FilePurpose <code>implementations/athena/config/monitoring.yaml</code>Athena monitor config <code>.github/workflows/monitoring.yml</code>Scheduled run automation"},{"location":"deep-dives/database/","title":"Database Integration","text":"<p> Neon/PostgreSQL with connection pooling, DataFrame I/O, and concurrent task execution. The shared database module provides synchronous and asynchronous interfaces for all persistence needs. </p>"},{"location":"deep-dives/database/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     NeonSettings                                \u2502\n\u2502  Configuration from environment variables                       \u2502\n\u2502  \u2022 DATABASE_URL, DB_POOL_*, DB_STATEMENT_TIMEOUT_MS            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                                   \u2502\n            \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  NeonConnection       \u2502         \u2502  AsyncNeonConnection      \u2502\n\u2502  (Synchronous)        \u2502         \u2502  (Asynchronous)           \u2502\n\u2502                       \u2502         \u2502                           \u2502\n\u2502  \u2022 fetch_all()        \u2502         \u2502  \u2022 await fetch_all()      \u2502\n\u2502  \u2022 fetch_one()        \u2502         \u2502  \u2022 await fetch_one()      \u2502\n\u2502  \u2022 execute_commit()   \u2502         \u2502  \u2022 await execute_commit() \u2502\n\u2502  \u2022 fetch_dataframe()  \u2502         \u2502  \u2022 await fetch_dataframe()\u2502\n\u2502  \u2022 upload_dataframe() \u2502         \u2502  \u2022 await upload_dataframe()\u2502\n\u2502  \u2022 create_table()     \u2502         \u2502  \u2022 await create_table()   \u2502\n\u2502  \u2022 check_health()     \u2502         \u2502  \u2022 await check_health()   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                                   \u2502\n            \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   ConnectionPool      \u2502         \u2502   AsyncConnectionPool     \u2502\n\u2502   (psycopg3)          \u2502         \u2502   (psycopg3)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     QueueExecutor                               \u2502\n\u2502  Concurrent task execution with asyncio.Queue                   \u2502\n\u2502  \u2022 Worker pool pattern                                          \u2502\n\u2502  \u2022 Supports sync and async functions                            \u2502\n\u2502  \u2022 Configurable backpressure                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-dives/database/#configuration","title":"Configuration","text":""},{"location":"deep-dives/database/#neonsettings","title":"NeonSettings","text":"<pre><code>class NeonSettings(RepoSettingsBase):\n    database_url: str | None              # PostgreSQL connection URL\n    db_pool_min_size: int                 # Minimum pool connections\n    db_pool_max_size: int                 # Maximum pool connections\n    db_connect_timeout_seconds: int       # Connection timeout\n    db_statement_timeout_ms: int          # Query timeout (0 = disabled)\n    db_use_startup_statement_timeout: bool\n    db_application_name: str | None       # For PostgreSQL monitoring\n    db_upload_chunk_size: int             # Rows per batch for uploads\n</code></pre>"},{"location":"deep-dives/database/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>DATABASE_URL</code> str Required Postgres connection URL <code>DB_POOL_MIN_SIZE</code> int <code>0</code> Minimum connections in pool <code>DB_POOL_MAX_SIZE</code> int <code>20</code> Maximum connections in pool <code>DB_CONNECT_TIMEOUT_SECONDS</code> int <code>10</code> Connection timeout <code>DB_STATEMENT_TIMEOUT_MS</code> int <code>60000</code> Statement timeout (0 = disabled) <code>DB_USE_STARTUP_STATEMENT_TIMEOUT</code> bool <code>False</code> Use startup options (unsupported by Neon pooler) <code>DB_APPLICATION_NAME</code> str None PostgreSQL application_name <code>DB_UPLOAD_CHUNK_SIZE</code> int <code>1000</code> Rows per batch for DataFrame uploads"},{"location":"deep-dives/database/#settings-access","title":"Settings Access","text":"<pre><code>from eval_workbench.shared.database.neon import get_neon_settings, reset_neon_settings_cache\n\n# Get cached settings\nsettings = get_neon_settings()\n\n# Clear cache when environment changes\nreset_neon_settings_cache()\n</code></pre>"},{"location":"deep-dives/database/#connection-managers","title":"Connection Managers","text":"NeonConnection Synchronous \u2014 scripts, data analysis, web apps <pre><code>from eval_workbench.shared.database.neon import NeonConnection\n\n# Using environment variable\ndb = NeonConnection()\n\n# Using explicit connection string\ndb = NeonConnection(\n    connection_string=\"postgresql://user:pass@host/db\"\n)\n</code></pre>  Context manager (recommended):  <pre><code>with NeonConnection() as db:\n    users = db.fetch_all(\"SELECT * FROM users\")\n    # Pool automatically closed on exit\n</code></pre> AsyncNeonConnection Asynchronous \u2014 FastAPI, high-concurrency, AI agents <pre><code>from eval_workbench.shared.database.neon import AsyncNeonConnection\n\ndb = AsyncNeonConnection()  # Pool opens lazily\n</code></pre>  Async context manager:  <pre><code>async with AsyncNeonConnection() as db:\n    users = await db.fetch_all(\"SELECT * FROM users\")\n    # Pool automatically closed on exit\n</code></pre>  Concurrent queries:  <pre><code>async with AsyncNeonConnection() as db:\n    users, orders, products = await asyncio.gather(\n        db.fetch_all(\"SELECT * FROM users\"),\n        db.fetch_all(\"SELECT * FROM orders\"),\n        db.fetch_all(\"SELECT * FROM products\"),\n    )\n</code></pre>"},{"location":"deep-dives/database/#query-methods","title":"Query Methods","text":"<p>All methods are available on both <code>NeonConnection</code> (sync) and <code>AsyncNeonConnection</code> (async with <code>await</code>).</p>"},{"location":"deep-dives/database/#fetch_all-fetch_one","title":"fetch_all / fetch_one","text":"<pre><code># Positional parameters\nresults = db.fetch_all(\n    \"SELECT * FROM users WHERE age &gt; %s\", (18,)\n)\n\n# Named parameters\nresults = db.fetch_all(\n    \"SELECT * FROM users WHERE email = %(email)s\",\n    {\"email\": \"user@example.com\"}\n)\n\n# Single row\nuser = db.fetch_one(\"SELECT * FROM users WHERE id = %s\", (1,))\nif user:\n    print(user['name'])\n</code></pre>"},{"location":"deep-dives/database/#execute_commit","title":"execute_commit","text":"<pre><code>db.execute_commit(\n    \"INSERT INTO users (name, email) VALUES (%s, %s)\",\n    (\"Alice\", \"alice@example.com\")\n)\n</code></pre>"},{"location":"deep-dives/database/#dataframe-io","title":"DataFrame I/O","text":"<pre><code># Query to DataFrame\ndf = db.fetch_dataframe(\n    \"SELECT id, name, score FROM results WHERE score &gt; %s\", (75,)\n)\n\n# Upload DataFrame (batched by db_upload_chunk_size)\ndb.upload_dataframe(df, 'measurements')\n</code></pre>"},{"location":"deep-dives/database/#table-management","title":"Table Management","text":"<pre><code>db.create_table(\n    table_name='users',\n    columns=[\n        ('id', 'SERIAL PRIMARY KEY'),\n        ('name', 'VARCHAR(255) NOT NULL'),\n        ('email', 'VARCHAR(255) UNIQUE'),\n        ('created_at', 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'),\n    ]\n)\n</code></pre> <p>SQL Type Validation</p> <p>Only allows: letters, digits, underscore, space, parentheses, comma, period. Prevents SQL injection attacks.</p>"},{"location":"deep-dives/database/#health-check","title":"Health Check","text":"<pre><code>if db.check_health():\n    print(\"Database is healthy\")\n</code></pre>"},{"location":"deep-dives/database/#queueexecutor","title":"QueueExecutor","text":"Worker pool pattern using `asyncio.Queue` for concurrent task execution. Supports both sync and async functions with configurable backpressure.  <pre><code>from eval_workbench.shared.database.neon import QueueExecutor\n\nasync def process_item(item_id: int):\n    await asyncio.sleep(1)\n    return f\"Processed {item_id}\"\n\nasync def main():\n    async with QueueExecutor(num_workers=10) as executor:\n        futures = [\n            executor.submit(process_item, i)\n            for i in range(100)\n        ]\n        results = await asyncio.gather(*futures)\n        return results\n</code></pre> <pre><code># With backpressure control \u2014 blocks when queue is full\nasync with QueueExecutor(num_workers=5, maxsize=100) as executor:\n    future = executor.submit(slow_function)\n</code></pre>"},{"location":"deep-dives/database/#usage-patterns","title":"Usage Patterns","text":"1 <p>Simple Queries</p> <p>Direct <code>fetch_one</code> / <code>fetch_all</code> calls with <code>NeonConnection()</code>. Best for scripts and one-off data exploration.</p> 2 <p>DataFrame Workflow</p> <p>Query to DataFrame, transform in pandas, upload back. Great for ETL pipelines and evaluation result processing.</p> 3 <p>Async + FastAPI</p> <p><code>AsyncNeonConnection</code> with lifecycle events. Pool opens at startup, closes on shutdown. Concurrent queries with <code>asyncio.gather</code>.</p> 4 <p>Batch Processing</p> <p><code>QueueExecutor</code> for parallel processing with backpressure. Combined with <code>upload_dataframe</code> for large-scale data operations.</p>"},{"location":"deep-dives/database/#evaluationuploader","title":"EvaluationUploader","text":"<p>Source: <code>src/eval_workbench/shared/database/evaluation_upload.py</code></p> <p>High-level upload interface for evaluation data. Handles column subsetting, JSONB normalization, conflict resolution, and batched inserts.</p>"},{"location":"deep-dives/database/#column-definitions","title":"Column Definitions","text":"<p><code>EVALUATION_DATASET_COLUMNS</code> (29 columns) \u2014 <code>dataset_id</code>, <code>query</code>, <code>expected_output</code>, <code>actual_output</code>, <code>additional_input</code>, <code>acceptance_criteria</code>, <code>dataset_metadata</code>, <code>user_tags</code>, <code>conversation</code>, <code>tools_called</code>, <code>expected_tools</code>, <code>judgment</code>, <code>critique</code>, <code>trace</code>, <code>additional_output</code>, <code>source_type</code>, <code>environment</code>, <code>source_name</code>, <code>source_component</code>, <code>created_at</code>, <code>retrieved_content</code>, <code>document_text</code>, <code>actual_reference</code>, <code>expected_reference</code>, <code>latency</code>, <code>trace_id</code>, <code>observation_id</code>, <code>has_errors</code></p> <p><code>EVALUATION_RESULTS_COLUMNS</code> (23 columns) \u2014 <code>run_id</code>, <code>dataset_id</code>, <code>metric_name</code>, <code>metric_score</code>, <code>passed</code>, <code>explanation</code>, <code>metric_type</code>, <code>metric_category</code>, <code>threshold</code>, <code>signals</code>, <code>metric_id</code>, <code>parent</code>, <code>weight</code>, <code>evaluation_name</code>, <code>eval_mode</code>, <code>cost_estimate</code>, <code>model_name</code>, <code>llm_provider</code>, <code>version</code>, <code>timestamp</code>, <code>source</code>, <code>metric_metadata</code>, <code>evaluation_metadata</code></p>"},{"location":"deep-dives/database/#jsonb-normalization","title":"JSONB Normalization","text":"<p>Columns marked as JSONB are automatically handled:</p> <ul> <li><code>dict</code> and <code>list</code> values are serialized to JSON strings</li> <li>JSON string values are validated for correctness</li> <li><code>NaN</code>, <code>Inf</code>, and <code>NaT</code> values are sanitized to <code>None</code></li> <li>Non-string dict keys raise <code>EvaluationUploadError</code></li> </ul>"},{"location":"deep-dives/database/#conflict-resolution","title":"Conflict Resolution","text":"Mode Behavior <code>error</code> Raise on conflict (default) <code>do_nothing</code> Skip conflicting rows (<code>ON CONFLICT DO NOTHING</code>) <code>upsert</code> Update existing rows (<code>ON CONFLICT DO UPDATE</code>)"},{"location":"deep-dives/database/#evaluationuploader-class","title":"EvaluationUploader Class","text":"<pre><code>from eval_workbench.shared.database.evaluation_upload import EvaluationUploader\nfrom eval_workbench.shared.database.neon import NeonConnection\n\nwith NeonConnection() as db:\n    uploader = EvaluationUploader(\n        db=db,\n        on_conflict=\"do_nothing\",   # error | do_nothing | upsert\n        chunk_size=1000,\n        include_missing_columns=False,\n        dataset_id_source=\"dataset_id\",  # dataset_id | id | metric_id\n    )\n\n    # Upload dataset items\n    uploaded_df = uploader.upload_dataset(dataset_df)\n\n    # Upload evaluation results\n    uploaded_df = uploader.upload_results(results_df)\n\n    # Generic upload (specify table)\n    uploaded_df = uploader.upload(df, table=\"evaluation_dataset\")\n</code></pre>"},{"location":"deep-dives/database/#standalone-functions","title":"Standalone Functions","text":"<pre><code>from eval_workbench.shared.database.evaluation_upload import (\n    subset_evaluation_dataset_df_for_upload,\n    subset_evaluation_results_df_for_upload,\n    upload_evaluation_dataset_df,\n    upload_evaluation_results_df,\n)\n\n# Subset DataFrame to only valid columns (no upload)\nclean_df = subset_evaluation_dataset_df_for_upload(df, include_missing_columns=True)\n\n# Full upload pipeline\nwith NeonConnection() as db:\n    upload_evaluation_dataset_df(db, dataset_df, on_conflict=\"do_nothing\")\n    upload_evaluation_results_df(db, results_df, on_conflict=\"upsert\")\n</code></pre>"},{"location":"deep-dives/database/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = db.fetch_all(query, params)\nexcept psycopg.Error as e:\n    logger.error(f\"Database query failed: {e}\")\n    raise\n</code></pre> Exception Description <code>psycopg.Error</code> Base exception for all psycopg errors <code>psycopg.DatabaseError</code> Database-specific errors <code>psycopg.IntegrityError</code> Constraint violations <code>psycopg.OperationalError</code> Connection/operational issues <code>ValueError</code> Invalid configuration or parameters"},{"location":"deep-dives/database/#connection-pool-management","title":"Connection Pool Management","text":"SynchronousAsynchronous <pre><code>self.pool = ConnectionPool(\n    conninfo=self.dsn,\n    min_size=0,          # Pre-allocated connections\n    max_size=20,         # Maximum concurrent connections\n    kwargs={\n        \"row_factory\": dict_row,\n        \"connect_timeout\": 10,\n    },\n)\n</code></pre> <pre><code>self.pool = AsyncConnectionPool(\n    conninfo=self.dsn,\n    min_size=0,\n    max_size=20,\n    open=False,          # Lazy initialization\n)\n</code></pre> <p>Statement Timeout</p> <p>Applied per-connection when not using startup options: <code>SET statement_timeout = 60000;</code> (60 seconds).</p>"},{"location":"deep-dives/database/#dependencies","title":"Dependencies","text":"Package Version Purpose <code>psycopg[binary]</code> &gt;=3.0 PostgreSQL adapter <code>psycopg-pool</code> &gt;=3.0 Connection pooling <code>pandas</code> &gt;=1.0 DataFrame support <code>pydantic</code> &gt;=2.0 Settings validation <code>pydantic-settings</code> &gt;=2.0 Environment loading"},{"location":"deep-dives/database/#exports","title":"Exports","text":"<pre><code>from eval_workbench.shared.database.neon import (\n    # Configuration\n    NeonSettings,\n    get_neon_settings,\n    reset_neon_settings_cache,\n\n    # Database Managers\n    NeonConnection,\n    AsyncNeonConnection,\n\n    # Task Execution\n    QueueExecutor,\n)\n</code></pre>"},{"location":"deep-dives/schema/","title":"Evaluation Schema","text":"<p> A 3-stage data model for evaluation persistence. Raw input data flows into <code>evaluation_dataset</code>, metric scores flow into <code>evaluation_results</code>, and the <code>evaluation_view</code> joins both for unified analysis. </p>"},{"location":"deep-dives/schema/#data-model","title":"Data Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  evaluation_dataset \u2502\u2500\u2500\u2500\u2500&lt;\u2502  evaluation_results \u2502\n\u2502  (Input/Ground Truth)\u2502 1:N \u2502  (Metric Scores)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                         \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u25bc\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502   evaluation_view   \u2502\n           \u2502   (Joined Analysis) \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> 1 <p>Dataset</p> <p>Each evaluation item (prompt, response, context) gets a unique <code>dataset_id</code>. This is the ground truth input.</p> 2 <p>Results</p> <p>Each metric evaluation produces a row keyed by <code>(run_id, dataset_id, metric_name)</code>. A single dataset item can have many metric scores across runs.</p> 3 <p>View</p> <p>Joins results back to dataset context \u2014 enabling queries like \"show me all failed metrics with their original prompts.\"</p>"},{"location":"deep-dives/schema/#evaluation_dataset","title":"evaluation_dataset","text":"Stores the input data for evaluation: queries, responses, context, and ground truth. **Primary Key:** `dataset_id`"},{"location":"deep-dives/schema/#core-fields","title":"Core Fields","text":"Column Type Description <code>dataset_id</code> TEXT Unique identifier for the evaluation record <code>query</code> TEXT The user's input/prompt <code>expected_output</code> TEXT Reference/expected output (ground truth) <code>actual_output</code> TEXT System's generated response <code>created_at</code> TIMESTAMPTZ Record creation timestamp"},{"location":"deep-dives/schema/#context-metadata","title":"Context &amp; Metadata","text":"Column Type Description <code>additional_input</code> JSONB Extra context inputs (business logic, flags, data) <code>acceptance_criteria</code> JSONB User-defined definitions of acceptable responses <code>dataset_metadata</code> JSONB Additional metadata for the input data source <code>user_tags</code> JSONB Custom tags applied to the record <code>conversation</code> JSONB Multi-turn conversation structure"},{"location":"deep-dives/schema/#source-tracking","title":"Source Tracking","text":"Column Type Description <code>source_type</code> TEXT Source channel (e.g., <code>'slack'</code>, <code>'online'</code>, <code>'langfuse'</code>) <code>environment</code> TEXT Execution environment (e.g., <code>'production'</code>, <code>'staging'</code>) <code>source_name</code> TEXT Name of the agent/system being evaluated <code>source_component</code> TEXT Specific component of the agent"},{"location":"deep-dives/schema/#tool-retrieval","title":"Tool &amp; Retrieval","text":"Column Type Description <code>tools_called</code> JSONB Tools actually invoked by the agent <code>expected_tools</code> JSONB Tools that should have been called <code>retrieved_content</code> JSONB RAG context chunks retrieved"},{"location":"deep-dives/schema/#evaluation-signals","title":"Evaluation Signals","text":"Column Type Description <code>judgment</code> JSONB Binary/categorical evaluation decision <code>critique</code> JSONB Detailed reasoning for the judgment <code>trace</code> JSONB Full execution trace information <code>additional_output</code> JSONB Extra outputs (debug info, side effects)"},{"location":"deep-dives/schema/#document-reference","title":"Document &amp; Reference","text":"Column Type Description <code>document_text</code> TEXT Full text of processed document <code>actual_reference</code> JSONB Ranked list of retrieved documents <code>expected_reference</code> JSONB Ground truth reference ranking"},{"location":"deep-dives/schema/#observability","title":"Observability","text":"Column Type Description <code>latency</code> DOUBLE PRECISION Response time in seconds <code>trace_id</code> TEXT Link to distributed trace (Datadog, Langfuse, etc.) <code>observation_id</code> TEXT ID of specific observation/span evaluated <code>has_errors</code> BOOLEAN Whether tool errors occurred"},{"location":"deep-dives/schema/#evaluation_results","title":"evaluation_results","text":"Stores metric scores and metadata from evaluation runs. **Primary Key:** `(run_id, dataset_id, metric_name)` (composite). **Foreign Key:** `dataset_id` \u2192 `evaluation_dataset.dataset_id`"},{"location":"deep-dives/schema/#score-fields","title":"Score Fields","text":"Column Type Description <code>run_id</code> TEXT Unique ID for the evaluation run <code>dataset_id</code> TEXT Reference to the dataset item (FK) <code>metric_name</code> TEXT Name of the metric evaluated <code>metric_score</code> DOUBLE PRECISION Computed score value <code>passed</code> BOOLEAN Whether score met threshold <code>explanation</code> TEXT Justification for the score <code>threshold</code> DOUBLE PRECISION Pass/fail cutoff value"},{"location":"deep-dives/schema/#metric-metadata","title":"Metric Metadata","text":"Column Type Description <code>metric_type</code> TEXT Type of node (<code>'metric'</code>, <code>'component'</code>) <code>metric_category</code> TEXT Category (<code>'score'</code>, <code>'analysis'</code>, <code>'classification'</code>) <code>metric_id</code> TEXT Unique identifier for this metric evaluation <code>signals</code> JSONB Granular breakdown/intermediate signals"},{"location":"deep-dives/schema/#hierarchy","title":"Hierarchy","text":"Column Type Description <code>parent</code> TEXT Parent component name (hierarchical evals) <code>weight</code> DOUBLE PRECISION Weight relative to siblings <code>source</code> TEXT Source of metric (evaluator class)"},{"location":"deep-dives/schema/#run-context","title":"Run Context","text":"Column Type Description <code>evaluation_name</code> TEXT Name of experiment/test campaign <code>eval_mode</code> TEXT Evaluation mode <code>model_name</code> TEXT LLM used for evaluation <code>llm_provider</code> TEXT Provider of the LLM <code>timestamp</code> TIMESTAMPTZ When evaluation occurred"},{"location":"deep-dives/schema/#cost-versioning","title":"Cost &amp; Versioning","text":"Column Type Description <code>cost_estimate</code> DOUBLE PRECISION Estimated cost of running this metric <code>metric_metadata</code> JSONB Structured metadata (token usage, etc.) <code>evaluation_metadata</code> JSONB Run-level metadata <code>version</code> TEXT Version of metric logic"},{"location":"deep-dives/schema/#evaluation_view","title":"evaluation_view","text":"<p> A database view joining <code>evaluation_results</code> with <code>evaluation_dataset</code> on <code>dataset_id</code> \u2014 providing a unified table for analysis. </p> <pre><code>CREATE VIEW evaluation_view AS\nSELECT *\nFROM evaluation_results r\nLEFT JOIN evaluation_dataset d ON r.dataset_id = d.dataset_id;\n</code></pre> <p>Use <code>evaluation_view</code> when you need both metric scores and input context in the same query.</p> From Dataset 28 columns \u2014 Context <p>Original input, expected output, source tracking, tool usage, retrieval context, and observability fields for each evaluated item.</p> From Results 23 columns \u2014 Metrics <p>Scores, thresholds, explanations, hierarchy, run metadata, cost estimates, and versioning for each metric evaluation.</p> <p>Column Primary Key</p> <p>Both tables have <code>dataset_id</code>. In the view, only one is kept (from <code>evaluation_results</code>). Use explicit column references if needed.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide covers environment setup, configuration, and common workflows for Eval Workbench.</p>"},{"location":"getting-started/#install-editable","title":"Install (editable)","text":"<p>Install the repo in editable mode so <code>shared</code> and <code>implementations.&lt;name&gt;</code> imports work without <code>sys.path</code> hacks:</p> <pre><code>pip install -e .\n</code></pre> <p>This is required for notebooks under <code>src/implementations/&lt;name&gt;/notebooks/</code>.</p>"},{"location":"getting-started/#environment-variables","title":"Environment Variables","text":"<p>Environment variables are loaded with Pydantic Settings in two layers, with later files overriding earlier ones:</p> <ol> <li>Repo root <code>.env</code> provides global defaults.</li> <li><code>implementations/&lt;name&gt;/.env</code> provides per-implementation overrides.</li> </ol> <p>Use <code>.env.example</code> files as the canonical templates for what each layer supports. When you add a new setting, define it on a settings class that inherits <code>shared.settings.RepoSettingsBase</code> and sets <code>model_config</code> via <code>build_settings_config(from_path=Path(__file__))</code>, then document the value in the relevant <code>.env.example</code>.</p>"},{"location":"getting-started/#langfuse-settings","title":"Langfuse Settings","text":"<p>Prompt management uses these Langfuse settings:</p> <pre><code>LANGFUSE_PUBLIC_KEY=\"\"\nLANGFUSE_SECRET_KEY=\"\"\nLANGFUSE_HOST=\"\"\nLANGFUSE_DEFAULT_LABEL=\"production\"\nLANGFUSE_DEFAULT_CACHE_TTL_SECONDS=60\n</code></pre>"},{"location":"getting-started/#database-settings","title":"Database Settings","text":"<p>The Neon/Postgres helper reads these variables (all optional unless your app needs a DB connection):</p> <pre><code>DATABASE_URL=\"\"\nDB_POOL_MIN_SIZE=0\nDB_POOL_MAX_SIZE=20\nDB_CONNECT_TIMEOUT_SECONDS=10\nDB_STATEMENT_TIMEOUT_MS=60000\nDB_USE_STARTUP_STATEMENT_TIMEOUT=false\nDB_APPLICATION_NAME=\"\"\nDB_UPLOAD_CHUNK_SIZE=1000\n</code></pre>"},{"location":"getting-started/#prompt-patterns","title":"Prompt Patterns","text":"<p>Prompt extraction uses an explicit strategy passed to <code>Trace</code> or <code>TraceCollection</code>. For Athena:</p> <pre><code>from eval_workbench.implementations.athena.langfuse.prompt_patterns import (\n    ChatPromptPatterns,\n    WorkflowPromptPatterns,\n)\nfrom eval_workbench.shared.langfuse.trace import TraceCollection\n\nchat_traces = TraceCollection(data, prompt_patterns=ChatPromptPatterns)\nrecommendation_traces = TraceCollection(data, prompt_patterns=WorkflowPromptPatterns)\n</code></pre>"},{"location":"getting-started/#langfuse-webhooks","title":"Langfuse Webhooks","text":"<p>Use a webhook to invalidate prompt cache on updates:</p> <pre><code>export LANGFUSE_PUBLIC_KEY=\"...\"\nexport LANGFUSE_SECRET_KEY=\"...\"\nexport LANGFUSE_WEBHOOK_SECRET=\"whsec_...\"\nexport LANGFUSE_WEBHOOK_NOTIFY_URL=\"https://your-app/notify\"\nexport LANGFUSE_SLACK_CHANNEL_ID=\"C0123456789\"\nexport LANGFUSE_SLACK_REQUEST_TIMEOUT_SECONDS=\"10\"\nexport LANGFUSE_SLACK_RETRY_MAX_ATTEMPTS=\"3\"\nexport LANGFUSE_SLACK_RETRY_BACKOFF_SECONDS=\"0.5\"\nexport LANGFUSE_SLACK_RETRY_MAX_BACKOFF_SECONDS=\"4.0\"\nexport SLACK_ATHENA_TOKEN=\"xoxb-...\"\nexport SLACK_AIMEE_TOKEN=\"xoxb-...\"\nexport SLACK_CANARY_TOKEN=\"xoxb-...\"\nexport SLACK_PROMETHEUS_TOKEN=\"xoxb-...\"\nexport SLACK_QUILL_TOKEN=\"xoxb-...\"\n\nuvicorn shared.langfuse.webhook:app --host 0.0.0.0 --port 5001\n</code></pre> <p>Configure Langfuse to POST to <code>https://your-host/webhooks/langfuse</code> for <code>prompt.created</code>, <code>prompt.updated</code>, and <code>prompt.deleted</code> events.</p> <p>Test: update a prompt in Langfuse and confirm a message posts to the Slack channel. Slack posting happens in a background task and includes retry/backoff with rate-limit handling.</p>"},{"location":"getting-started/#pre-commit","title":"Pre-commit","text":"<p>Formatting is managed via pre-commit hooks.</p> <pre><code># Install to run after every commit\npre-commit install\n\n# Run on all files\npre-commit run --all-files\n</code></pre>"},{"location":"guides/","title":"Guides","text":"<p>Guides for Langfuse integrations, monitoring, and Slack tooling.</p>"},{"location":"guides/#monitoring","title":"Monitoring","text":"<ul> <li>Monitoring Configuration (YAML)</li> </ul>"},{"location":"guides/athena-langfuse/","title":"Langfuse Prompt Management (Athena)","text":"<p>The Athena Langfuse integration provides a join system that bridges Neon database records with Langfuse distributed traces, enabling rich analysis of AI workflows through combined case and trace data.</p>"},{"location":"guides/athena-langfuse/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AthenaNeonLangfuseJoiner   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                          \u2502\n    \u25bc                                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 NeonDatabaseMgr  \u2502              \u2502 LangfuseTraceLoader  \u2502\n\u2502  fetch_dataframe \u2502              \u2502   fetch_traces()     \u2502\n\u2502  fetch_cases()   \u2502              \u2502  fetch_trace_by_id() \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                   \u2502\n         \u25bc                                   \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 cases   \u2502                    \u2502 TraceCollection  \u2502\n    \u2502DataFrame\u2502                    \u2502  List[Trace]     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                \u2502\n         \u2502         join_cases_with_traces()\n         \u2502                 \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502 Joined DataFrame     \u2502\n           \u2502 - case columns       \u2502\n           \u2502 + langfuse_trace col \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/athena-langfuse/#trace-access-smartaccess","title":"Trace Access (SmartAccess)","text":"<pre><code>Trace.recommendation\n   \u2514\u2500 TraceStep\n      \u251c\u2500 .generation (GENERATION observation)\n      \u251c\u2500 .context (SPAN observation)\n      \u2514\u2500 .variables (extracted from patterns)\n         \u251c\u2500 caseAssessment\n         \u251c\u2500 contextData\n         \u251c\u2500 underwritingFlags\n         \u2514\u2500 swallowDebugData\n</code></pre>"},{"location":"guides/athena-langfuse/#joinsettings-configuration","title":"JoinSettings Configuration","text":"<p>Frozen dataclass for configuring the join operation.</p> <pre><code>@dataclass(frozen=True)\nclass JoinSettings:\n    case_table: str = \"athena_cases\"\n    case_columns: tuple[str, ...] = (\n        \"id\",\n        \"workflow_id\",\n        \"quote_locator\",\n        \"slack_thread_ts\",\n        \"slack_channel_id\",\n        \"langfuse_trace_id\",\n    )\n    trace_name: str = \"athena\"\n    trace_tags: tuple[str, ...] = (\"production\",)\n</code></pre> Attribute Type Default Purpose <code>case_table</code> <code>str</code> <code>\"athena_cases\"</code> Target table in Neon database <code>case_columns</code> <code>tuple[str, ...]</code> See above Columns to fetch from case_table <code>trace_name</code> <code>str</code> <code>\"athena\"</code> Langfuse trace name filter <code>trace_tags</code> <code>tuple[str, ...]</code> <code>(\"production\",)</code> Langfuse trace tags to filter by <p>Usage: <pre><code># Default settings\nsettings = JoinSettings()\n\n# Custom settings\nsettings = JoinSettings(\n    case_table=\"custom_cases\",\n    case_columns=(\"id\", \"workflow_id\", \"status\"),\n    trace_name=\"custom_workflow\",\n    trace_tags=(\"staging\", \"v2\"),\n)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#athenaneonlangfusejoiner","title":"AthenaNeonLangfuseJoiner","text":"<p>Main helper class to coordinate fetching and joining case data from Neon with trace data from Langfuse.</p>"},{"location":"guides/athena-langfuse/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    neon_db: NeonConnection,\n    trace_loader: LangfuseTraceLoader,\n    *,\n    settings: JoinSettings | None = None,\n    prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n) -&gt; None\n</code></pre> Parameter Type Default Description <code>neon_db</code> <code>NeonConnection</code> Required Database connection manager <code>trace_loader</code> <code>LangfuseTraceLoader</code> Required Langfuse trace fetching client <code>settings</code> <code>JoinSettings \\| None</code> <code>JoinSettings()</code> Configuration for table/column/trace names <code>prompt_patterns</code> <code>PromptPatternsBase</code> <code>WorkflowPromptPatterns</code> Pattern registry for prompt extraction"},{"location":"guides/athena-langfuse/#methods","title":"Methods","text":""},{"location":"guides/athena-langfuse/#fetch_cases","title":"fetch_cases()","text":"<p>Fetch case records from Neon database.</p> <pre><code>def fetch_cases(\n    self,\n    *,\n    limit: Optional[int] = None,\n    where: Optional[str] = None,\n    columns: Optional[Sequence[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Example: <pre><code>cases = joiner.fetch_cases(\n    limit=50,\n    where=\"status = 'active'\",\n    columns=(\"id\", \"workflow_id\", \"quote_locator\"),\n)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#fetch_traces","title":"fetch_traces()","text":"<p>Fetch traces from Langfuse by name and tags.</p> <pre><code>def fetch_traces(\n    self,\n    *,\n    limit: int = 200,\n    name: Optional[str] = None,\n    tags: Optional[Sequence[str]] = None,\n    fetch_full_traces: bool = True,\n    show_progress: bool = False,\n) -&gt; TraceCollection\n</code></pre> <p>Example: <pre><code>traces = joiner.fetch_traces(\n    limit=100,\n    name=\"custom_workflow\",\n    tags=[\"staging\"],\n    fetch_full_traces=True,\n)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#fetch_traces_by_ids","title":"fetch_traces_by_ids()","text":"<p>Fetch specific traces by their IDs.</p> <pre><code>def fetch_traces_by_ids(\n    self,\n    trace_ids: Sequence[str],\n    *,\n    fetch_full_traces: bool = True,\n    show_progress: bool = False,\n    trace_fetcher: Callable[[str], object] | None = None,\n) -&gt; TraceCollection\n</code></pre> <p>Example: <pre><code>trace_ids = [\"trace-001\", \"trace-002\", \"trace-003\"]\ntraces = joiner.fetch_traces_by_ids(trace_ids, fetch_full_traces=True)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#join_cases_with_traces","title":"join_cases_with_traces()","text":"<p>Join case records with corresponding trace objects via trace ID.</p> <pre><code>def join_cases_with_traces(\n    self,\n    cases: pd.DataFrame,\n    traces: TraceCollection,\n    *,\n    trace_id_column: str = \"langfuse_trace_id\",\n    trace_output_column: str = \"langfuse_trace\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Example: <pre><code>joined = joiner.join_cases_with_traces(\n    cases,\n    traces,\n    trace_id_column=\"langfuse_trace_id\",\n    trace_output_column=\"trace_data\",\n)\n# Result: cases DataFrame with new 'trace_data' column containing Trace objects\n</code></pre></p>"},{"location":"guides/athena-langfuse/#prompt-extraction-patterns","title":"Prompt Extraction Patterns","text":"<p>Pattern classes define regex patterns for extracting structured data from LLM prompts. They inherit from <code>PromptPatternsBase</code>.</p>"},{"location":"guides/athena-langfuse/#promptpatternsbase","title":"PromptPatternsBase","text":"<p>Base registry for regex extraction patterns.</p> <pre><code>class PromptPatternsBase:\n    @classmethod\n    def get_for(cls, step_name: str) -&gt; Dict[str, str]:\n        \"\"\"\n        Dynamically looks for methods named _patterns_{step_name}().\n        Returns dict of {field_name: regex_pattern}.\n        \"\"\"\n</code></pre>"},{"location":"guides/athena-langfuse/#workflowpromptpatterns","title":"WorkflowPromptPatterns","text":"<p>Extract structured fields from \"recommendation\" workflow prompts.</p> <pre><code>class WorkflowPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_recommendation() -&gt; Dict[str, str]:\n        \"\"\"Returns patterns for recommendation step.\"\"\"\n</code></pre> <p>Extracted Fields:</p> Field Header Delimiter Description <code>caseAssessment</code> \"CASE ASSESSMENT...\" \u2192 \"FULL CONTEXT DATA\" Previous case analysis <code>contextData</code> \"FULL CONTEXT DATA\" \u2192 \"UNDERWRITING FLAGS...\" Complete context info <code>underwritingFlags</code> \"UNDERWRITING FLAGS...\" \u2192 \"SWALLOW DEBUG DATA...\" Active underwriting rules <code>swallowDebugData</code> \"SWALLOW DEBUG DATA...\" \u2192 \"PREVIOUS RECOMMENDATIONS...\" or EOF Debug trace data"},{"location":"guides/athena-langfuse/#chatpromptpatterns","title":"ChatPromptPatterns","text":"<p>Extract structured fields from chat-based prompts.</p> <pre><code>class ChatPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_chat() -&gt; Dict[str, str]:\n        \"\"\"Returns patterns for chat step.\"\"\"\n</code></pre> <p>Extracted Fields:</p> Field Delimiter Pattern Description <code>conversationHistory</code> <code>**Conversation History:**</code> \u2192 <code>**Quote Context:**</code> Chat message history <code>quoteContext</code> <code>**Quote Context:**</code> \u2192 <code>**Current User Message:**</code> Quote/reference data <code>currentUserMessage</code> <code>**Current User Message:**</code> \u2192 <code>**Context:**</code> Latest user input <code>context</code> <code>**Context:**</code> \u2192 <code>**Instructions:**</code> System/background context <code>instructions</code> <code>**Instructions:**</code> \u2192 EOF System instructions"},{"location":"guides/athena-langfuse/#tracecollection-and-smartaccess-utilities","title":"TraceCollection and SmartAccess Utilities","text":""},{"location":"guides/athena-langfuse/#tracecollection","title":"TraceCollection","text":"<p>Wraps a list of trace data items, providing collection-level operations.</p> <pre><code>class TraceCollection:\n    def __init__(\n        self,\n        data: List[Any],\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    ): ...\n\n    def __getitem__(self, index: int) -&gt; Trace: ...\n    def __iter__(self) -&gt; Iterator[Trace]: ...\n    def __len__(self) -&gt; int: ...\n    def filter_by(self, **kwargs) -&gt; TraceCollection: ...\n</code></pre> <p>Example: <pre><code>traces = TraceCollection(trace_data_list, prompt_patterns=WorkflowPromptPatterns)\n\n# Access individual traces\nfirst_trace = traces[0]\n\n# Iterate\nfor trace in traces:\n    print(trace.id)\n\n# Filter\nproduction_traces = traces.filter_by(name=\"athena\")\n\n# Get count\ntotal = len(traces)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#smartaccess","title":"SmartAccess","text":"<p>Base class enabling intelligent dot-notation access with fuzzy matching.</p> <p>Features: - Dot notation access: <code>trace.recommendation</code> - Bracket access: <code>trace['recommendation']</code> - Case-insensitive matching: <code>trace.product_type</code> matches <code>productType</code> - Recursive wrapping: nested objects maintain SmartAccess</p> <p>Fuzzy Matching: <pre><code>def _normalize_key(key: str) -&gt; str:\n    \"\"\"'product_type' matches 'productType'\"\"\"\n    return key.lower().replace(\"_\", \"\")\n</code></pre></p>"},{"location":"guides/athena-langfuse/#trace","title":"Trace","text":"<p>Main wrapper providing access to steps and trace-level attributes.</p> <pre><code>class Trace(SmartAccess):\n    def __init__(\n        self,\n        trace_data: Any,\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    ): ...\n</code></pre> <p>Access Patterns:</p> Access Returns Example <code>trace.{step_name}</code> <code>TraceStep</code> <code>trace.recommendation</code> <code>trace.{trace_attr}</code> Any <code>trace.id</code>, <code>trace.latency</code>"},{"location":"guides/athena-langfuse/#tracestep","title":"TraceStep","text":"<p>Represents a specific named step (e.g., 'recommendation').</p> <pre><code>class TraceStep(SmartAccess):\n    name: str                              # Step name\n    observations: List[ObservationsView]   # Observations in step\n    prompt_patterns: PromptPatternsBase    # Pattern registry\n</code></pre> <p>Properties:</p> Access Returns Description <code>step.variables</code> <code>Dict[str, str]</code> Lazily extracted prompt variables <code>step.generation</code> <code>ObservationsView</code> First GENERATION observation <code>step.context</code> <code>ObservationsView</code> SPAN observation (context) <code>step.span</code> <code>ObservationsView</code> SPAN observation"},{"location":"guides/athena-langfuse/#integration-with-sharedlangfusetracepy","title":"Integration with shared/langfuse/trace.py","text":"<p>The Athena implementation builds on the shared Langfuse trace module:</p> <ol> <li>PromptPatternsBase Inheritance - <code>WorkflowPromptPatterns</code> and    <code>ChatPromptPatterns</code> extend base</li> <li>TraceCollection Usage - <code>fetch_traces()</code> returns <code>TraceCollection</code></li> <li>Trace Smart Access - All Trace objects use SmartAccess for dot notation</li> <li>Pattern Resolution - Helper converts class to instance for all wrapped    traces</li> </ol>"},{"location":"guides/athena-langfuse/#code-examples","title":"Code Examples","text":""},{"location":"guides/athena-langfuse/#basic-usage-fetch-and-join","title":"Basic Usage: Fetch and Join","text":"<pre><code>from eval_workbench.implementations.athena.langfuse.join import AthenaNeonLangfuseJoiner, JoinSettings\nfrom eval_workbench.implementations.athena.langfuse.prompt_patterns import WorkflowPromptPatterns\nfrom eval_workbench.shared.database.neon import NeonConnection\nfrom axion.tracing import LangfuseTraceLoader\n\n# Initialize components\nneon_db = NeonConnection()\ntrace_loader = LangfuseTraceLoader(api_key=\"YOUR_API_KEY\")\n\n# Create joiner\njoiner = AthenaNeonLangfuseJoiner(\n    neon_db,\n    trace_loader,\n    prompt_patterns=WorkflowPromptPatterns,\n)\n\n# Fetch cases and traces\ncases = joiner.fetch_cases(limit=100, where=\"status = 'completed'\")\ntrace_ids = cases['langfuse_trace_id'].dropna().unique()\ntraces = joiner.fetch_traces_by_ids(trace_ids, fetch_full_traces=True)\n\n# Join\njoined = joiner.join_cases_with_traces(cases, traces)\nprint(f\"Joined {len(joined)} cases with traces\")\n</code></pre>"},{"location":"guides/athena-langfuse/#analyzing-traces","title":"Analyzing Traces","text":"<pre><code># Access joined data\nfor idx, row in joined.iterrows():\n    case_id = row['id']\n    trace = row['langfuse_trace']\n\n    if trace is None:\n        print(f\"Case {case_id}: No trace found\")\n        continue\n\n    # Access trace metadata\n    print(f\"Case {case_id}:\")\n    print(f\"  Trace ID: {trace.id}\")\n    print(f\"  Latency: {trace.latency}ms\")\n\n    # Access recommendation step\n    rec_step = trace.recommendation\n\n    # Extract variables from prompt (lazy evaluation)\n    variables = rec_step.variables\n    print(f\"  Case Assessment: {variables.get('caseAssessment', 'N/A')[:100]}...\")\n    print(f\"  Context Data: {variables.get('contextData', 'N/A')[:100]}...\")\n\n    # Access generation observation\n    gen = rec_step.generation\n    print(f\"  Generation model: {gen.model}\")\n</code></pre>"},{"location":"guides/athena-langfuse/#custom-settings","title":"Custom Settings","text":"<pre><code># Configure for staging environment\nsettings = JoinSettings(\n    case_table=\"athena_cases_staging\",\n    case_columns=(\"id\", \"workflow_id\", \"langfuse_trace_id\", \"status\"),\n    trace_name=\"athena-staging\",\n    trace_tags=(\"staging\", \"v2\"),\n)\n\njoiner = AthenaNeonLangfuseJoiner(\n    neon_db,\n    trace_loader,\n    settings=settings,\n    prompt_patterns=WorkflowPromptPatterns,\n)\n</code></pre>"},{"location":"guides/athena-langfuse/#custom-prompt-patterns","title":"Custom Prompt Patterns","text":"<pre><code>from eval_workbench.shared.langfuse.trace import PromptPatternsBase, create_extraction_pattern\n\nclass CustomPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_my_step() -&gt; Dict[str, str]:\n        return {\n            \"fieldA\": create_extraction_pattern(\n                \"FIELD A HEADER\",\n                \"FIELD B HEADER\"\n            ),\n            \"fieldB\": create_extraction_pattern(\n                \"FIELD B HEADER\",\n                \"(?:$)\"  # End of text\n            ),\n        }\n\n# Use custom patterns\njoiner = AthenaNeonLangfuseJoiner(\n    neon_db,\n    trace_loader,\n    prompt_patterns=CustomPromptPatterns,\n)\n</code></pre>"},{"location":"guides/athena-langfuse/#working-with-tracecollection","title":"Working with TraceCollection","text":"<pre><code># Fetch traces\ntraces = joiner.fetch_traces(limit=200)\n\n# Filter traces\nrecommendation_traces = traces.filter_by(name=\"recommendation\")\n\n# Iterate and analyze\nfor trace in traces:\n    # Access via dot notation (case insensitive)\n    trace_id = trace.id\n\n    # Access steps\n    if hasattr(trace, 'recommendation'):\n        rec = trace.recommendation\n        print(f\"Trace {trace_id}: has recommendation step\")\n\n        # Lazy variable extraction\n        vars = rec.variables\n        if 'caseAssessment' in vars:\n            print(f\"  Assessment: {vars['caseAssessment'][:50]}...\")\n</code></pre>"},{"location":"guides/athena-langfuse/#exports","title":"Exports","text":"<pre><code>from eval_workbench.implementations.athena.langfuse.join import (\n    JoinSettings,\n    AthenaNeonLangfuseJoiner,\n)\n\nfrom eval_workbench.implementations.athena.langfuse.prompt_patterns import (\n    WorkflowPromptPatterns,\n    ChatPromptPatterns,\n)\n\nfrom eval_workbench.shared.langfuse.trace import (\n    PromptPatternsBase,\n    SmartAccess,\n    SmartDict,\n    SmartObject,\n    Trace,\n    TraceStep,\n    TraceCollection,\n    create_extraction_pattern,\n)\n</code></pre>"},{"location":"guides/configuration/","title":"Configuration","text":"<p> Two complementary systems: a YAML config module with dot notation and env interpolation, and a Pydantic settings module with .env cascade. Together they handle all configuration needs from YAML files to environment variables. </p>"},{"location":"guides/configuration/#config-module","title":"Config Module","text":"<p>Source: <code>src/eval_workbench/shared/config.py</code></p> <p>The config module provides a global dictionary-based configuration store with dot-notation access, environment variable interpolation, and context manager support.</p>"},{"location":"guides/configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from eval_workbench.shared.config import load, load_config\n\n# Load YAML into global config (permanent)\nload(\"path/to/config.yaml\")\n\n# Load as context manager (temporary \u2014 reverts on exit)\nwith load(\"path/to/config.yaml\"):\n    # config is active here\n    pass\n# config reverted to previous state\n\n# Load dict directly\nload({\"source\": {\"type\": \"slack\", \"limit\": 50}})\n\n# Load YAML without modifying global config\ncfg = load_config(\"path/to/config.yaml\")\ncfg = load_config(\"path/to/config.yaml\", overrides={\"source.limit\": 100})\n</code></pre>"},{"location":"guides/configuration/#reading-values","title":"Reading Values","text":"<pre><code>from eval_workbench.shared.config import get\n\n# Dot notation access\nsource_type = get(\"source.type\")\nlimit = get(\"source.limit\", default=50)\n\n# Raise on missing key\nvalue = get(\"required.key\", error=True)  # raises KeyError if missing\n\n# Read from a specific config dict (not global)\nvalue = get(\"source.type\", cfg=my_config)\n</code></pre>"},{"location":"guides/configuration/#setting-values","title":"Setting Values","text":"<pre><code>from eval_workbench.shared.config import set\n\n# Temporary override via context manager\nwith set({\"source.limit\": 100, \"sampling.n\": 5}):\n    # overridden values active here\n    pass\n# reverted\n</code></pre>"},{"location":"guides/configuration/#environment-variable-interpolation","title":"Environment Variable Interpolation","text":"<p>YAML values containing <code>${VAR}</code> or <code>${VAR:-default}</code> are automatically resolved:</p> <pre><code># In YAML\nenvironment: \"${ENVIRONMENT:-production}\"\ndatabase:\n  connection_string: \"${DATABASE_URL}\"\n  pool_size: \"${DB_POOL_SIZE:-20}\"\n</code></pre> <pre><code># After loading\nget(\"environment\")                   # \"production\" (if ENVIRONMENT unset)\nget(\"database.connection_string\")    # value of DATABASE_URL env var\nget(\"database.pool_size\")            # \"20\" (if DB_POOL_SIZE unset)\n</code></pre>"},{"location":"guides/configuration/#settings-module","title":"Settings Module","text":"<p>Source: <code>src/eval_workbench/shared/settings.py</code></p> <p>The settings module provides Pydantic-based settings with automatic .env file resolution and implementation-aware configuration.</p>"},{"location":"guides/configuration/#reposettingsbase","title":"RepoSettingsBase","text":"<p>Base class for all settings models. Pre-configured for UTF-8 encoding and <code>extra='ignore'</code>.</p> <pre><code>from eval_workbench.shared.settings import RepoSettingsBase, build_settings_config\n\nclass MySettings(RepoSettingsBase):\n    model_config = build_settings_config()\n\n    api_key: str\n    timeout: int = 30\n</code></pre>"},{"location":"guides/configuration/#env-cascade","title":".env Cascade","text":"<p>Settings are loaded from .env files in a specific order:</p> <pre><code>1. Repo root .env           \u2192  /path/to/eval-workbench/.env\n2. Implementation .env      \u2192  /path/to/eval-workbench/src/.../implementations/{name}/.env\n</code></pre> <p>Later files override earlier ones. The cascade is resolved automatically.</p>"},{"location":"guides/configuration/#key-functions","title":"Key Functions","text":"<pre><code>from eval_workbench.shared.settings import (\n    find_repo_root,\n    infer_implementation_name,\n    resolve_env_files,\n    build_settings_config,\n)\n\n# Find repository root (searches for .git directory)\nroot = find_repo_root()\n\n# Infer implementation from current path\nname = infer_implementation_name()  # e.g., \"athena\"\n\n# Resolve .env file paths\nenv_files = resolve_env_files()\n# [\"/.../eval-workbench/.env\", \"/.../implementations/athena/.env\"]\n\n# Build Pydantic settings config\nconfig = build_settings_config(\n    from_path=None,              # Start search path (default: cwd)\n    implementation_name=None,    # Override implementation name\n    env_prefix=None,             # Environment variable prefix\n)\n</code></pre>"},{"location":"guides/configuration/#usage-pattern","title":"Usage Pattern","text":"<pre><code>from pydantic_settings import BaseSettings\nfrom eval_workbench.shared.settings import build_settings_config\n\nclass NeonSettings(BaseSettings):\n    model_config = build_settings_config(env_prefix=\"DB_\")\n\n    database_url: str | None = None\n    pool_min_size: int = 0\n    pool_max_size: int = 20\n    statement_timeout_ms: int = 60000\n</code></pre> <p>With a <code>.env</code> file:</p> <pre><code>DB_DATABASE_URL=postgresql://user:pass@host/db\nDB_POOL_MAX_SIZE=10\n</code></pre>"},{"location":"guides/configuration/#when-to-use-which","title":"When to Use Which","text":"Need Module Example YAML monitoring configs <code>config</code> <code>load(\"monitoring_slack.yaml\")</code> Runtime config overrides <code>config</code> <code>with set({\"source.limit\": 5}): ...</code> Dot-notation nested access <code>config</code> <code>get(\"source.type\")</code> Environment-based settings <code>settings</code> <code>NeonSettings()</code> Validated, typed settings <code>settings</code> Pydantic model with defaults .env file resolution <code>settings</code> <code>build_settings_config()</code> Config with env interpolation <code>config</code> <code>${DATABASE_URL}</code> in YAML"},{"location":"guides/memory-dashboard-spec/","title":"Memory Dashboard Tab \u2014 Frontend Spec","text":""},{"location":"guides/memory-dashboard-spec/#overview","title":"Overview","text":"<p>Dashboard tab for visualizing what the Athena agent has learned about underwriting rules. Two data sources:</p> <ul> <li>Neon (Postgres) \u2014 flat <code>rule_extractions</code> table with every extracted rule, raw source text, and ingestion status</li> <li>Zep (Graph) \u2014 knowledge graph with semantic relationships (RiskFactor -&gt; Rule -&gt; Outcome, Mitigant -&gt; Rule)</li> </ul> <p>No existing frontend or API layer. Backend is Python (FastAPI available as dependency). All data access functions exist and are documented below with exact return shapes.</p>"},{"location":"guides/memory-dashboard-spec/#page-layout","title":"Page Layout","text":"<p>The Memory tab is a single page with a summary strip at the top, an optional conflict warning banner, and sub-tabs for the main content area.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Memory                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502 12 Rules \u2502 \u2502 11 Risk  \u2502 \u2502 15 Mit-  \u2502 \u2502 3 Hard   \u2502       \u2502\n\u2502  \u2502          \u2502 \u2502 Factors  \u2502 \u2502 igants   \u2502 \u2502 Stops    \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                                             \u2502\n\u2502  \u250c\u2500 small bar charts (inline, beside cards) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  rules by action: \u2588\u2588 decline \u2588\u2588 refer \u2588 approve       \u2502  \u2502\n\u2502  \u2502  rules by product: \u2588\u2588 ALL \u2588\u2588 Property \u2588 BOP \u2588 LRO     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                             \u2502\n\u2502  \u26a0 2 Conflicting Rules Detected (click to view)            \u2502\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 [Rules]  [Decision Quality]  [Hard Stops]  [Batches] \u2502   \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502\n\u2502  \u2502                                                      \u2502   \u2502\n\u2502  \u2502  (sub-tab content area)                              \u2502   \u2502\n\u2502  \u2502                                                      \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/memory-dashboard-spec/#component-breakdown","title":"Component breakdown","text":"Component Position Always visible Source Summary cards Top strip Yes <code>graph.summary()</code> + Neon status counts Action/product bar charts Top strip, beside cards Yes <code>graph.summary()</code> Conflict warning banner Below cards, above tabs Only if conflicts exist <code>graph.conflicting_rules()</code> across known risk factors Sub-tabs Main content area Yes (one active) See below"},{"location":"guides/memory-dashboard-spec/#sub-tabs","title":"Sub-tabs","text":"Tab Label Content 1 Rules Filterable rules table with expandable row detail (includes inline decision path) 2 Decision Quality Aligned vs divergent split view + soft thresholds 3 Hard Stops Unmitigated decline rules list 4 Batches Pipeline activity / batch history"},{"location":"guides/memory-dashboard-spec/#summary-cards-always-visible","title":"Summary Cards (always visible)","text":"<p>Source: Zep \u2014 <code>graph.summary()</code> + Neon status counts</p> <p>Zep return shape: <pre><code>{\n  \"total_nodes\": 42,\n  \"total_edges\": 58,\n  \"nodes_by_type\": {\n    \"RiskFactor\": 11,\n    \"Rule\": 11,\n    \"Outcome\": 7,\n    \"Mitigant\": 15,\n    \"Source\": 9\n  },\n  \"edges_by_relation\": {\n    \"TRIGGERS\": 11,\n    \"RESULTS_IN\": 11,\n    \"OVERRIDES\": 18,\n    \"DERIVED_FROM\": 9\n  },\n  \"rules_by_action\": {\n    \"decline\": 5,\n    \"refer\": 4,\n    \"approve_with_conditions\": 1,\n    \"flag_for_review\": 1\n  },\n  \"rules_by_product\": {\n    \"ALL\": 6,\n    \"BOP\": 2,\n    \"Property\": 3,\n    \"LRO\": 1\n  }\n}\n</code></pre></p> <p>Neon status counts (direct SQL): <pre><code>SELECT ingestion_status, COUNT(*) as count\nFROM rule_extractions WHERE agent_name = 'athena'\nGROUP BY ingestion_status\n</code></pre></p> <p>Cards to show: - Rules \u2014 <code>nodes_by_type.Rule</code> count - Risk Factors \u2014 <code>nodes_by_type.RiskFactor</code> count - Mitigants \u2014 <code>nodes_by_type.Mitigant</code> count - Hard Stops \u2014 count from unmitigated declines (or pre-computed)</p> <p>Bar charts (small, inline): - Rules by action \u2014 horizontal bars, color-coded: red=decline, yellow=refer, blue=approve_with_conditions, gray=flag_for_review - Rules by product \u2014 horizontal bars, neutral colors</p>"},{"location":"guides/memory-dashboard-spec/#conflict-warning-banner","title":"Conflict Warning Banner","text":"<p>Source: Zep \u2014 <code>graph.conflicting_rules(risk_factor)</code> run across known risk factors</p> <p>Behavior: - On page load, check for conflicts across all risk factors from <code>graph.list_all_rules()</code> - If any conflicts found, show amber banner: \"\u26a0 {N} Conflicting Rules Detected\" - Click banner \u2192 opens a modal or expandable section showing:</p> <p>Return shape: <pre><code>[\n  {\n    \"risk_factor\": \"Employee Count\",\n    \"product_type\": \"BOP\",\n    \"actions\": [\"decline\", \"refer\"],\n    \"rules\": [\n      {\"rule_name\": \"Rule A\", \"action\": \"decline\", \"properties\": {}},\n      {\"rule_name\": \"Rule B\", \"action\": \"refer\", \"properties\": {}}\n    ]\n  }\n]\n</code></pre></p> <p>Conflict detail layout: - Each conflict is a card showing: risk factor, product type, then the conflicting rules side-by-side with their actions highlighted in their badge colors</p>"},{"location":"guides/memory-dashboard-spec/#sub-tab-1-rules","title":"Sub-tab 1: Rules","text":"<p>Source: Neon \u2014 <code>fetch_all_extractions(db, agent_name='athena')</code></p> <p>Dataset can also be read from CSV: <code>/Users/mattevanoff/Downloads/rule_extractions_v1.csv</code></p> <p>Return shape \u2014 <code>list[dict]</code>, each dict: <pre><code>{\n  \"id\": \"uuid-string\",\n  \"created_at\": \"2026-02-06T12:00:00Z\",\n  \"batch_id\": \"uuid-string\",\n  \"agent_name\": \"athena\",\n  \"raw_text\": \"Properties within 1 mile of coast require...\",\n  \"raw_text_hash\": \"sha256-hex\",\n  \"risk_factor\": \"Coastal Exposure\",\n  \"risk_category\": \"location\",\n  \"rule_name\": \"Coastal Wind Referral\",\n  \"product_type\": \"Property\",\n  \"action\": \"refer\",\n  \"outcome_description\": \"Properties within 1 mile of coast require wind/hail review...\",\n  \"mitigants\": [\"Hurricane shutters installed\", \"Miami-Dade rated roof\"],\n  \"source\": \"Catastrophe Guidelines 2024\",\n  \"source_type\": \"manual\",\n  \"confidence\": \"high\",\n  \"threshold\": {\"field\": \"distance_to_coast\", \"operator\": \"lt\", \"value\": 1, \"unit\": \"miles\"},\n  \"threshold_type\": \"hard\",\n  \"historical_exceptions\": null,\n  \"decision_quality\": null,\n  \"compound_trigger\": null,\n  \"data_fields\": null,\n  \"ingestion_status\": \"ingested\",\n  \"ingestion_error\": null,\n  \"ingested_at\": \"2026-02-06T12:00:05Z\"\n}\n</code></pre></p>"},{"location":"guides/memory-dashboard-spec/#table-columns","title":"Table columns","text":"Column Source field Display Rule Name <code>rule_name</code> Text, clickable to expand row Risk Factor <code>risk_factor</code> Text Action <code>action</code> Badge: red=<code>decline</code>, yellow=<code>refer</code>, blue=<code>approve_with_conditions</code>, gray=<code>flag_for_review</code> Product <code>product_type</code> Text, <code>ALL</code> shown as a distinct style Threshold <code>threshold_type</code> Badge: <code>hard</code> (solid) vs <code>soft</code> (outline) \u2014 soft means flexibility exists Status <code>ingestion_status</code> Dot: green=<code>ingested</code>, gray=<code>pending</code>, red=<code>failed</code> Confidence <code>confidence</code> Text or badge Created <code>created_at</code> Relative time (\"2 hours ago\")"},{"location":"guides/memory-dashboard-spec/#filters-all-optional-combinable","title":"Filters (all optional, combinable)","text":"<ul> <li>Action: multi-select (<code>decline</code>, <code>refer</code>, <code>approve_with_conditions</code>, <code>flag_for_review</code>)</li> <li>Product: multi-select (<code>ALL</code>, <code>BOP</code>, <code>Property</code>, <code>LRO</code>)</li> <li>Status: multi-select (<code>ingested</code>, <code>pending</code>, <code>failed</code>)</li> <li>Threshold type: <code>hard</code> / <code>soft</code></li> <li>Risk category: text/select</li> <li>Batch: select dropdown</li> <li>Date range</li> </ul>"},{"location":"guides/memory-dashboard-spec/#expanded-row-detail-click-a-row-to-expand","title":"Expanded row detail (click a row to expand)","text":"<p>Two sections: Rule Detail and Decision Path.</p> <p>Rule Detail section \u2014 fields not shown in the table:</p> <ul> <li>Outcome Description \u2014 full text</li> <li>Mitigants \u2014 bulleted list from the JSONB array</li> <li>Threshold \u2014 render the JSON object as human-readable: <code>distance_to_coast &lt; 1 miles</code> (field + operator + value + unit)</li> <li>Historical Exceptions \u2014 italic text block, shows what UWs have actually done vs the manual</li> <li>Source \u2014 <code>source</code> + <code>source_type</code> badge (manual / compliance / production)</li> <li>Decision Quality \u2014 <code>aligned</code> vs <code>divergent</code> badge (if present)</li> <li>Compound Trigger \u2014 the boolean expression as code text (if present)</li> <li>Data Fields \u2014 code-formatted list of field paths (if present)</li> <li>Raw Text \u2014 collapsible/scrollable area showing the original source text</li> <li>Ingestion Error \u2014 red text block (only if <code>ingestion_status = 'failed'</code>)</li> <li>IDs \u2014 <code>id</code>, <code>batch_id</code> in small muted text</li> </ul> <p>Decision Path section \u2014 inline graph trace for this rule:</p> <p>Source: Zep \u2014 <code>graph.trace(rule.risk_factor, product_type=rule.product_type)</code></p> <p>Render as a horizontal flow:</p> <pre><code>[Coastal Exposure] --TRIGGERS--&gt; [Coastal Wind Referral] --RESULTS_IN--&gt; [Refer]\n                                         ^\n                            [Hurricane shutters installed] --OVERRIDES--\n                            [Miami-Dade rated roof] --OVERRIDES--\n</code></pre> <ul> <li>Nodes as rounded boxes, edges as labeled arrows</li> <li>Risk Factor node in blue, Rule node in orange, Outcome node in green, Mitigant nodes in purple</li> <li>Keep it compact \u2014 this is inside an expanded table row, not a full page</li> </ul>"},{"location":"guides/memory-dashboard-spec/#sub-tab-2-decision-quality","title":"Sub-tab 2: Decision Quality","text":"<p>Source: Neon \u2014 direct SQL</p> <pre><code>SELECT rule_name, risk_factor, action, product_type, threshold_type,\n       historical_exceptions, decision_quality, outcome_description,\n       threshold, mitigants\nFROM rule_extractions\nWHERE decision_quality IS NOT NULL\nORDER BY decision_quality, created_at DESC\n</code></pre>"},{"location":"guides/memory-dashboard-spec/#layout-two-column-split","title":"Layout \u2014 two-column split","text":"<p>Left column: \"Aligned\" (<code>decision_quality = 'aligned'</code>) - Green section header - Rules where the agent's learned behavior matches the manual - Card list, each card shows:   - Rule name   - Action badge   - Risk factor   - Brief outcome description (truncated to 2 lines)</p> <p>Right column: \"Divergent\" (<code>decision_quality = 'divergent'</code>) - Orange/amber section header - Rules where real UW decisions diverge from the manual \u2014 these are the key insights - Card list, each card shows:   - Rule name + action badge   - <code>outcome_description</code> \u2014 what the manual says   - <code>historical_exceptions</code> \u2014 highlighted/emphasized \u2014 what UWs actually did   - <code>threshold</code> rendered as \"manual says X, but UWs approved up to Y\"   - <code>mitigants</code> \u2014 what conditions enabled the divergent decisions</p>"},{"location":"guides/memory-dashboard-spec/#soft-thresholds-section-below-the-split","title":"Soft Thresholds section (below the split)","text":"<p>Source: Neon \u2014 direct SQL</p> <pre><code>SELECT rule_name, risk_factor, action, product_type,\n       threshold, threshold_type, historical_exceptions,\n       outcome_description, mitigants\nFROM rule_extractions\nWHERE threshold_type = 'soft'\nORDER BY created_at DESC\n</code></pre> <p>Purpose: Show rules where a hard number has learned flexibility. Rendered as a card list below the aligned/divergent split.</p> <p>Each card: - Rule name - Manual threshold: <code>employee_count &gt; 20 employees</code> (rendered from <code>threshold</code> JSON) - What actually happens: <code>historical_exceptions</code> text - Mitigants that unlock flexibility: bulleted <code>mitigants</code> list</p>"},{"location":"guides/memory-dashboard-spec/#sub-tab-3-hard-stops","title":"Sub-tab 3: Hard Stops","text":"<p>Source: Zep \u2014 <code>graph.unmitigated_declines()</code></p> <p>Return shape \u2014 <code>list[dict]</code>: <pre><code>[\n  {\n    \"rule_name\": \"Cannabis Exclusion\",\n    \"risk_factor\": \"Cannabis Operations\",\n    \"product_type\": \"ALL\"\n  }\n]\n</code></pre></p> <p>Purpose: Rules that are absolute hard stops \u2014 no mitigants, no flexibility, no overrides. The agent should never try to work around these.</p> <p>Layout: - Red-themed section header: \"Hard Stops \u2014 No Overrides Available\" - Count badge: \"{N} unmitigated decline rules\" - Card or list for each rule:   - Rule name (bold)   - Risk factor   - Product type badge   - Red \"DECLINE\" badge - Optionally: link each rule to its expanded detail in the Rules tab</p>"},{"location":"guides/memory-dashboard-spec/#sub-tab-4-batches","title":"Sub-tab 4: Batches","text":"<p>Source: Neon \u2014 direct SQL (aggregation query)</p> <pre><code>SELECT\n    batch_id,\n    COUNT(*) as rule_count,\n    MIN(created_at) as started_at,\n    SUM(CASE WHEN ingestion_status = 'ingested' THEN 1 ELSE 0 END) as ingested,\n    SUM(CASE WHEN ingestion_status = 'pending' THEN 1 ELSE 0 END) as pending,\n    SUM(CASE WHEN ingestion_status = 'failed' THEN 1 ELSE 0 END) as failed\nFROM rule_extractions\nWHERE agent_name = 'athena'\nGROUP BY batch_id\nORDER BY MIN(created_at) DESC\n</code></pre> <p>Purpose: Pipeline run history \u2014 when the agent extracted rules, how many, success rate.</p> <p>Layout: - List of batches, most recent first - Each batch row:   - Batch ID (truncated UUID, e.g. <code>abc12345...</code>)   - Timestamp (relative: \"2 hours ago\")   - Rule count   - Stacked progress bar: green (ingested) / gray (pending) / red (failed)   - Percentage: \"92% ingested\" - Click a batch \u2192 navigates to the Rules sub-tab filtered by that <code>batch_id</code></p>"},{"location":"guides/memory-dashboard-spec/#backend-functions-reference","title":"Backend Functions Reference","text":"<p>All functions are importable from Python. The frontend needs API endpoints wrapping these.</p> <pre><code># Neon (Postgres) \u2014 rule_extractions table\nfrom eval_workbench.shared.database.neon import NeonConnection\nfrom eval_workbench.shared.memory.persistence import (\n    fetch_all_extractions,   # (db, agent_name='athena', limit=500) -&gt; list[dict]\n    fetch_pending,           # (db, agent_name='athena', batch_id=None) -&gt; list[dict]\n    save_extractions,        # (db, rules, batch_id, agent_name, raw_text) -&gt; list[str]\n    mark_ingested,           # (db, rule_id) -&gt; None\n    mark_failed,             # (db, rule_id, error) -&gt; None\n)\n\n# Zep (Graph) \u2014 knowledge graph\nfrom eval_workbench.implementations.athena.memory.analytics import AthenaGraphAnalytics\n\n# graph = AthenaGraphAnalytics(store=store)\n# graph.summary()                                          -&gt; dict\n# graph.query(input, product_type=None, limit=10)          -&gt; list[dict]\n# graph.trace(input, product_type=None, limit=25)          -&gt; list[dict]\n# graph.export(limit=500)                                  -&gt; list[dict]\n# graph.get_rule(rule_name, limit=25)                      -&gt; dict | None\n# graph.list_all_rules(limit=500)                          -&gt; list[dict]\n# graph.rules_by_action(action, limit=500)                 -&gt; list[dict]\n# graph.rules_by_product(product_type, limit=500)          -&gt; list[dict]\n# graph.uncovered_risks(limit=500)                         -&gt; list[str]\n# graph.mitigants_for_rule(rule_name, limit=25)            -&gt; list[str]\n# graph.rules_mitigated_by(mitigant, limit=25)             -&gt; list[str]\n# graph.unmitigated_declines(limit=500)                    -&gt; list[dict]\n# graph.conflicting_rules(risk_factor, limit=50)           -&gt; list[dict]\n# graph.overlapping_thresholds(field, limit=100)           -&gt; list[dict]\n# graph.evaluate(risk_factor, product_type, context, limit=25) -&gt; list[dict]\n</code></pre>"},{"location":"guides/memory-dashboard-spec/#connection-setup","title":"Connection setup","text":"<pre><code># Neon\ndb = NeonConnection()  # reads DATABASE_URL from env\n# use as context manager: with NeonConnection() as db: ...\n\n# Zep\nfrom eval_workbench.implementations.athena.memory import ATHENA_ONTOLOGY\nfrom eval_workbench.shared.memory import ZepGraphStore\n\nstore = ZepGraphStore(agent_name='athena', ontology=ATHENA_ONTOLOGY)\ngraph = AthenaGraphAnalytics(store=store)\n</code></pre>"},{"location":"guides/memory-dashboard-spec/#direct-sql-for-views-not-covered-by-existing-functions","title":"Direct SQL for views not covered by existing functions","text":"<pre><code># Pipeline activity (Batches sub-tab)\ndb.fetch_all(\"\"\"\n    SELECT batch_id, COUNT(*) as rule_count, MIN(created_at) as started_at,\n           SUM(CASE WHEN ingestion_status = 'ingested' THEN 1 ELSE 0 END) as ingested,\n           SUM(CASE WHEN ingestion_status = 'pending' THEN 1 ELSE 0 END) as pending,\n           SUM(CASE WHEN ingestion_status = 'failed' THEN 1 ELSE 0 END) as failed\n    FROM rule_extractions WHERE agent_name = %s\n    GROUP BY batch_id ORDER BY MIN(created_at) DESC\n\"\"\", ('athena',))\n\n# Decision quality split (Decision Quality sub-tab)\ndb.fetch_all(\"\"\"\n    SELECT rule_name, risk_factor, action, product_type, threshold_type,\n           historical_exceptions, decision_quality, outcome_description,\n           threshold, mitigants\n    FROM rule_extractions WHERE decision_quality IS NOT NULL\n    ORDER BY decision_quality, created_at DESC\n\"\"\")\n\n# Soft thresholds (Decision Quality sub-tab, lower section)\ndb.fetch_all(\"\"\"\n    SELECT rule_name, risk_factor, action, product_type,\n           threshold, threshold_type, historical_exceptions,\n           outcome_description, mitigants\n    FROM rule_extractions WHERE threshold_type = 'soft'\n    ORDER BY created_at DESC\n\"\"\")\n\n# Status counts (summary cards)\ndb.fetch_all(\"\"\"\n    SELECT ingestion_status, COUNT(*) as count\n    FROM rule_extractions WHERE agent_name = %s\n    GROUP BY ingestion_status\n\"\"\", ('athena',))\n</code></pre>"},{"location":"guides/memory-dashboard-spec/#suggested-api-endpoints","title":"Suggested API Endpoints","text":"Endpoint Method Backend call Used by <code>/api/rules</code> GET <code>fetch_all_extractions(db)</code> Rules sub-tab <code>/api/rules?status=pending</code> GET <code>fetch_pending(db)</code> Rules sub-tab filtered <code>/api/rules?batch_id={id}</code> GET <code>fetch_all_extractions</code> + filter Rules sub-tab filtered from Batches <code>/api/rules/quality</code> GET Direct SQL (decision_quality) Decision Quality sub-tab <code>/api/rules/soft-thresholds</code> GET Direct SQL (threshold_type=soft) Decision Quality sub-tab <code>/api/batches</code> GET Direct SQL (GROUP BY batch_id) Batches sub-tab <code>/api/graph/summary</code> GET <code>graph.summary()</code> Summary cards <code>/api/graph/trace?q={risk}&amp;product={type}</code> GET <code>graph.trace(q, product_type)</code> Expanded row decision path <code>/api/graph/rules</code> GET <code>graph.list_all_rules()</code> Summary charts <code>/api/graph/rule/{name}</code> GET <code>graph.get_rule(name)</code> Expanded row detail <code>/api/graph/unmitigated</code> GET <code>graph.unmitigated_declines()</code> Hard Stops sub-tab <code>/api/graph/conflicts?q={risk}</code> GET <code>graph.conflicting_rules(q)</code> Conflict warning banner <code>/api/status-counts</code> GET Direct SQL (GROUP BY status) Summary cards"},{"location":"guides/memory-dashboard-spec/#environment-variables-required","title":"Environment Variables Required","text":"<pre><code>MEMORY_DATABASE_URL=postgresql://...   # Neon Postgres connection\nZEP_API_KEY=...                 # Zep Cloud API key\n</code></pre>"},{"location":"guides/memory-dashboard-spec/#build-priority","title":"Build Priority","text":"<ol> <li>Summary cards + bar charts \u2014 top strip, always visible, quick win</li> <li>Rules sub-tab \u2014 main content, filterable table with expandable rows</li> <li>Decision Quality sub-tab \u2014 aligned vs divergent split + soft thresholds</li> <li>Hard Stops sub-tab \u2014 small list, low effort</li> <li>Batches sub-tab \u2014 pipeline history</li> <li>Expanded row decision path \u2014 inline graph trace (most complex visual)</li> <li>Conflict warning banner \u2014 check on page load, show if any exist</li> </ol>"},{"location":"guides/memory/","title":"Knowledge Graph Memory","text":"<p> Persistent knowledge graph memory for AI agents. Currently used by Athena (underwriting) to store risk factors, rules, decision logic, and their relationships \u2014 then retrieve them semantically at runtime. Built on Zep Cloud Graphiti for graph storage and semantic search. </p>"},{"location":"guides/memory/#how-it-works-end-to-end","title":"How It Works (End-to-End)","text":"<p>The memory system has four stages. Raw knowledge goes in one end; structured, searchable graph edges come out the other.</p> 1 <p>Raw Text</p> <p>Manuals, SME notes, training feedback, production learnings \u2014 unstructured text containing underwriting knowledge.</p> 2 <p>Extraction</p> <p><code>RuleExtractor</code> (OpenAI) \u2014 LLM parses text into structured rule dicts with risk factors, thresholds, and actions.</p> 3 <p>Ingestion</p> <p><code>ZepGraphStore</code> (Zep Cloud) \u2014 Edges stored in knowledge graph per agent with entity deduplication.</p> 4 <p>Retrieval</p> <p><code>AthenaGraphAnalytics</code> \u2014 Semantic search returns relevant rules + graph paths for runtime decisions.</p>"},{"location":"guides/memory/#stage-1-raw-text-input","title":"Stage 1: Raw Text Input","text":"<p>The input is unstructured text \u2014 anything that contains underwriting knowledge:</p> <ul> <li>Underwriting manual excerpts \u2014 formal rules and limits</li> <li>Training session notes \u2014 SME-taught heuristics and guidelines</li> <li>SME interview transcripts \u2014 tribal knowledge and known flexibility</li> <li>Production decision logs \u2014 the richest source: real outcomes, decision quality, learned patterns</li> <li>Compliance bulletins \u2014 hard regulatory constraints</li> </ul> <p>Production case reviews are especially valuable because they reveal multiple rules per text block, compound triggers, and cases where real decisions diverged from stated rules (making a \"hard\" rule effectively soft):</p> <pre><code>raw_texts = [\n    # Manual excerpt \u2014 gives you the stated rule\n    \"\"\"Gas stations are automatically referred for senior underwriter review due to\n    fuel storage liability. This applies to all LRO policies. However, if a fire\n    suppression system is installed and the station is located more than 200ft from\n    residential areas, the referral can be waived at the underwriter's discretion.\n    Source: Underwriting Manual v3.2\"\"\",\n\n    # Production case review \u2014 gives you compound triggers, decision quality, data fields\n    \"\"\"Athena correctly recommended decline for a lessor's risk warehouse with\n    catastrophic recent loss ($600K representing 23.5% of building exposure),\n    pricing 37% below minimum threshold, and prior carrier non-renewal. The\n    combination of recent major loss + below-minimum pricing + prior carrier\n    rejection is an automatic decline regardless of other positive factors.\"\"\",\n\n    # Divergent production decision \u2014 reveals that a \"hard\" rule is actually soft\n    \"\"\"The property rate of 0.19% is below the 0.25% minimum threshold, which\n    would normally trigger a referral. However, the underwriter approved because\n    this was a BASIC cause of loss policy with wind/hail exclusion. The lower\n    rate was appropriate for the limited coverage scope. Athena should learn\n    that low property rates are often justified for BASIC CoL policies.\"\"\",\n\n    # Data quality pattern \u2014 a non-traditional rule\n    \"\"\"The building exposure from Magic Dust was $2.3M vs customer input of $1.1M \u2014\n    a 109% discrepancy. The address format \"404-600 Main St\" indicates this may be\n    a multi-location risk quoted as a single location. Flag for verification.\"\"\",\n]\n</code></pre> <p>You can pass a single string, a list of strings, or a dict of strings. The pipeline handles all three.</p>"},{"location":"guides/memory/#stage-2-llm-extraction","title":"Stage 2: LLM Extraction","text":"<p>The <code>RuleExtractor</code> sends raw text to an LLM (default: <code>gpt-4o</code>) with a system prompt engineered to extract structured underwriting rules. The LLM returns JSON:</p> <pre><code>from eval_workbench.implementations.athena.memory import RuleExtractor\n\nextractor = RuleExtractor(model='gpt-4o')\nrules = extractor.extract_batch([raw_text])\n</code></pre> <p>Each extracted rule is a dict. A single text block (especially production case reviews) often yields multiple rules. The extractor distinguishes between hard deterministic rules, soft guidelines, compound triggers, data quality patterns, and more.</p> <p>Hard rule (no threshold flexibility):</p> <pre><code>{\n    \"risk_factor\": \"Gas Station\",\n    \"risk_category\": \"occupancy\",\n    \"rule_name\": \"Gas Station Auto-Refer\",\n    \"product_type\": \"LRO\",\n    \"action\": \"refer\",\n    \"outcome_description\": \"Gas stations must be referred to senior underwriter due to fuel storage liability.\",\n    \"mitigants\": [\"Fire suppression system installed\", \"Located &gt;200ft from residential\"],\n    \"source\": \"Underwriting Manual v3.2\",\n    \"source_type\": \"manual\",\n    \"confidence\": \"high\",\n    \"threshold\": null,\n    \"threshold_type\": null,\n    \"historical_exceptions\": null,\n    \"decision_quality\": null,\n    \"compound_trigger\": null,\n    \"data_fields\": []\n}\n</code></pre> <p>Compound trigger from a production case (multiple conditions must combine):</p> <pre><code>{\n    \"risk_factor\": \"Adverse Selection Trifecta\",\n    \"risk_category\": \"eligibility\",\n    \"rule_name\": \"Adverse Selection Auto-Decline\",\n    \"product_type\": \"ALL\",\n    \"action\": \"decline\",\n    \"outcome_description\": \"Combination of recent major loss (&gt;20% of building exposure) + below-minimum pricing + prior carrier non-renewal is an automatic decline regardless of other positive factors.\",\n    \"mitigants\": [],\n    \"source\": \"Production Decision Log - warehouse decline case\",\n    \"source_type\": \"production\",\n    \"confidence\": \"high\",\n    \"threshold\": null,\n    \"threshold_type\": \"hard\",\n    \"historical_exceptions\": null,\n    \"decision_quality\": \"aligned\",\n    \"compound_trigger\": \"recent_major_loss AND below_minimum_pricing AND prior_carrier_rejection\",\n    \"data_fields\": [\"auxData.rateData.input.total_loss_payment\", \"auxData.swallowProject.property_rate\"]\n}\n</code></pre> <p>Soft threshold with divergent decision (LLM learned the real rule is softer than the manual):</p> <pre><code>{\n    \"risk_factor\": \"Property Rate Below Minimum\",\n    \"risk_category\": \"pricing\",\n    \"rule_name\": \"Low Property Rate Referral\",\n    \"product_type\": \"Property\",\n    \"action\": \"refer\",\n    \"outcome_description\": \"Property rate below 0.25% triggers referral. However, rates below threshold are often legitimate for BASIC cause of loss policies with wind/hail exclusions.\",\n    \"mitigants\": [\"BASIC cause of loss (limited coverage scope)\", \"Wind/hail exclusion in place\"],\n    \"source\": \"Production Decision Log\",\n    \"source_type\": \"production\",\n    \"confidence\": \"high\",\n    \"threshold\": {\"field\": \"property_rate\", \"operator\": \"lt\", \"value\": 0.0025, \"unit\": \"rate\"},\n    \"threshold_type\": \"soft\",\n    \"historical_exceptions\": \"Approved at 0.19% for newer construction with BASIC cause of loss and wind/hail exclusion\",\n    \"decision_quality\": \"divergent\",\n    \"compound_trigger\": null,\n    \"data_fields\": [\"auxData.rateData.input.property_rate\", \"underwritingFlags.property_rate_low_Refer\"]\n}\n</code></pre> <p>Data quality pattern (non-traditional risk factor):</p> <pre><code>{\n    \"risk_factor\": \"Building Exposure Discrepancy\",\n    \"risk_category\": \"data_quality\",\n    \"rule_name\": \"Magic Dust Exposure Mismatch Flag\",\n    \"product_type\": \"ALL\",\n    \"action\": \"flag_for_review\",\n    \"outcome_description\": \"When Magic Dust building exposure differs from customer input by &gt;20%, flag for verification. Address ranges (e.g. 404-600) may indicate multi-location risk.\",\n    \"mitigants\": [\"Recent appraisal on file\", \"Physical inspection completed\"],\n    \"source\": \"Production Decision Log\",\n    \"source_type\": \"production\",\n    \"confidence\": \"high\",\n    \"threshold\": {\"field\": \"building_exposure_discrepancy_pct\", \"operator\": \"gt\", \"value\": 20, \"unit\": \"percent\"},\n    \"threshold_type\": \"soft\",\n    \"historical_exceptions\": \"Cases approved after visual verification confirmed Magic Dust data was incorrect\",\n    \"decision_quality\": \"divergent\",\n    \"compound_trigger\": null,\n    \"data_fields\": [\"auxData.rateData.input.building_exposure\", \"magicDustByElement.building_area\"]\n}\n</code></pre> <p>The key extraction fields:</p> Field Purpose <code>risk_category</code> Category: <code>occupancy</code>, <code>construction</code>, <code>location</code>, <code>operations</code>, <code>eligibility</code>, <code>pricing</code>, <code>data_quality</code>, <code>geographic_appetite</code>, <code>catastrophe</code>, <code>classification</code>, <code>process</code> <code>threshold</code> Structured numeric boundary: <code>{field, operator, value, unit}</code> <code>threshold_type</code> <code>\"hard\"</code> (absolute, never exceeded) or <code>\"soft\"</code> (known flexibility) <code>historical_exceptions</code> Plain-text description of real-world deviations from the rule <code>decision_quality</code> From production cases: <code>\"aligned\"</code> (AI matched UW), <code>\"divergent\"</code> (disagreed), <code>\"partial\"</code> <code>compound_trigger</code> Multi-condition rule: <code>\"condition_a AND condition_b AND condition_c\"</code> <code>data_fields</code> System field paths referenced (e.g. <code>auxData.rateData.input.property_rate</code>) <code>source_type</code> Where the knowledge came from: <code>manual</code>, <code>training</code>, <code>sme</code>, <code>production</code>, <code>compliance</code> <code>action</code> Prescribed action: <code>refer</code>, <code>decline</code>, <code>approve_with_conditions</code>, <code>exclude</code>, <code>verify</code>, <code>flag_for_review</code> <p>The extraction prompt teaches the LLM to:</p> <ul> <li>Extract multiple rules per text block (production case reviews often contain 3-5+)</li> <li>Look for numeric thresholds embedded in prose (\"$75/sq ft\", \"0.25% rate\", \"$1.5M frame limit\")</li> <li>Distinguish hard vs soft thresholds from language signals (\"always\" vs \"generally\")</li> <li>Capture compound triggers when multiple conditions must combine</li> <li>Recognize data quality patterns (exposure discrepancies, address ranges, zero-value fields)</li> <li>Recognize geographic appetite boundaries (Tier 1 counties, state exclusions)</li> <li>Recognize pricing rules (rate minimums, TIV/sqft thresholds)</li> <li>When a production case shows a divergent decision, extract both the original rule AND the learned exception</li> </ul>"},{"location":"guides/memory/#stage-3-graph-ingestion","title":"Stage 3: Graph Ingestion","text":"<p>Each extracted rule is converted into a set of graph edges that follow the Athena ontology, then ingested into Zep's knowledge graph.</p> <p>The <code>_rule_to_ingest_payload()</code> method on the pipeline builds these edges. For hard rules:</p> <pre><code>Gas Station \u2500\u2500TRIGGERS\u2500\u2500\u25b6 Gas Station Auto-Refer \u2500\u2500RESULTS_IN\u2500\u2500\u25b6 Refer\n                                    \u25b2\n                                    \u2502\n            Fire suppression system \u2500\u2500OVERRIDES\n            Located &gt;200ft          \u2500\u2500OVERRIDES\n                                    \u2502\n                                    \u25bc\n            Gas Station Auto-Refer \u2500\u2500DERIVED_FROM\u2500\u2500\u25b6 Underwriting Manual v3.2\n</code></pre> <p>For production-sourced rules, all contextual metadata \u2014 thresholds, historical exceptions, decision quality, compound triggers, and data field references \u2014 are embedded directly in the edge properties and outcome description. This is intentional \u2014 Zep does semantic search over fact text, so the nuance needs to be in the facts, not just in the graph topology.</p> <p>Soft threshold example:</p> <pre><code>Employee Count \u2500\u2500TRIGGERS\u2500\u2500\u25b6 Swallow API Employee Limit \u2500\u2500RESULTS_IN\u2500\u2500\u25b6 Decline\n                  \u2502                       \u25b2\n                  \u2502 threshold: gt 20      \u2502\n                  \u2502 threshold_type: soft  \u2502\n                  \u2502 historical: \"up to    Strong loss history \u2500\u2500OVERRIDES\n                  \u2502   23 approved\"        Well-established biz \u2500\u2500OVERRIDES\n                  \u2502                       \u2502\n                  \u2502                       \u25bc\n                  \u2502             Swallow API Employee Limit \u2500\u2500DERIVED_FROM\u2500\u2500\u25b6 Production Decisions\n                  \u2502\n                  \u2514\u2500\u25b6 Outcome includes: \"Limit is 20, but UWs approved up to 23...\"\n</code></pre> <p>Compound trigger example:</p> <pre><code>Adverse Selection Trifecta \u2500\u2500TRIGGERS\u2500\u2500\u25b6 Adverse Selection Auto-Decline \u2500\u2500RESULTS_IN\u2500\u2500\u25b6 Decline\n                  \u2502\n                  \u2502 threshold_type: hard\n                  \u2502 compound_trigger: \"recent_major_loss AND below_minimum_pricing\n                  \u2502                    AND prior_carrier_rejection\"\n                  \u2502 decision_quality: aligned\n                  \u2502 data_fields: [auxData.rateData.input.total_loss_payment, ...]\n                  \u2502\n                  \u2514\u2500\u25b6 Outcome includes: \"Combination of major loss + low pricing +\n                      prior rejection = automatic decline regardless of other positives\"\n</code></pre> <p>When someone later searches \"employee count 22\", Zep's semantic search surfaces the fact with the full context \u2014 including that 22 falls in the soft zone between the stated limit (20) and the historical maximum (23). When someone searches \"prior loss and coverage gap\", the compound trigger rule surfaces with the full multi-condition context.</p> <p>In code:</p> <pre><code>from eval_workbench.shared.memory import ZepGraphStore\nfrom eval_workbench.implementations.athena.memory import AthenaRulePipeline, ATHENA_ONTOLOGY\n\nstore = ZepGraphStore(agent_name='athena', ontology=ATHENA_ONTOLOGY)\npipeline = AthenaRulePipeline(store=store)\n\n# Full pipeline: raw text \u2192 LLM extraction \u2192 graph ingestion\nresult = pipeline.run(raw_text, batch_size=5)\nprint(f'Processed: {result.items_processed}, Ingested: {result.items_ingested}')\n</code></pre> <p>The <code>ZepGraphStore</code> serializes each edge set to JSON and calls <code>client.graph.add()</code>. Each agent gets its own Zep user (e.g. <code>athena_global_rules</code>) for multi-tenancy isolation.</p>"},{"location":"guides/memory/#stage-4-runtime-retrieval","title":"Stage 4: Runtime Retrieval","text":"<p>At query time, <code>AthenaGraphAnalytics</code> provides three retrieval modes:</p>"},{"location":"guides/memory/#quick-search-query","title":"Quick search \u2014 <code>query()</code>","text":"<p>Returns matching edges filtered by product type. Use this for fast lookups in the underwriting workflow.</p> <pre><code>from eval_workbench.implementations.athena.memory import AthenaGraphAnalytics\n\nanalytics = AthenaGraphAnalytics(store=store)\nresults = analytics.query('Gas Station', product_type='LRO')\n\nfor r in results:\n    print(f\"{r['source']} --{r['relation']}--&gt; {r['target']}\")\n</code></pre> <p>There's also a domain alias:</p> <pre><code>results = analytics.check_risk_appetite('Gas Station', product_type='LRO')\n</code></pre>"},{"location":"guides/memory/#decision-path-trace-trace","title":"Decision path trace \u2014 <code>trace()</code>","text":"<p>Returns the full decision tree: RiskFactor \u2192 Rule \u2192 Outcome, with any applicable mitigants. Use this for explainability and audit trails.</p> <pre><code>paths = analytics.trace('Gas Station', product_type='LRO')\n\nfor path in paths:\n    print(f\"Risk:      {path['risk_factor']}\")\n    print(f\"Rule:      {path['rule']}\")\n    print(f\"Outcomes:  {path['outcomes']}\")\n    print(f\"Mitigants: {path['mitigants']}\")\n</code></pre>"},{"location":"guides/memory/#export-export","title":"Export \u2014 <code>export()</code>","text":"<p>Dumps the entire graph as structured dicts. Use this for audits, backups, or feeding into other systems.</p> <pre><code>all_rules = analytics.export()\nanalytics.export_to_json('rules_backup.json')\n</code></pre>"},{"location":"guides/memory/#visualization","title":"Visualization","text":"<p>Renders the graph as a networkx/matplotlib plot. Requires the <code>memory</code> optional dependencies.</p> <pre><code>analytics.visualize(title='Athena Underwriting Rules')\n</code></pre>"},{"location":"guides/memory/#hard-vs-soft-rules","title":"Hard vs Soft Rules","text":"<p>Key concept: Not all rules are equal. The memory system preserves the distinction between absolute limits and flexible guidelines, enabling nuanced AI recommendations.</p> <p>Not all underwriting rules are binary. The system distinguishes between:</p> <p>Hard rules \u2014 absolute limits with no flexibility. The rule is the rule.</p> <ul> <li>Cannabis operations \u2192 always decline</li> <li>Coastal distance &lt; 1 mile \u2192 always refer (no exceptions without wind mitigation)</li> <li>These have <code>threshold_type: \"hard\"</code> (or <code>null</code> if there's no numeric boundary)</li> </ul> <p>Soft rules \u2014 guidelines with known flexibility where UW discretion applies.</p> <ul> <li>Employee count &gt; 20 \u2192 manual says decline, but UWs have approved up to 23</li> <li>Frame TIV &gt; $1M \u2192 manual says refer, but seniors have approved $1.25M with sprinklers</li> <li>Vacant &gt; 60 days \u2192 manual says decline, but approved with active renovation permits</li> <li>These have <code>threshold_type: \"soft\"</code> and a populated <code>historical_exceptions</code> field</li> </ul> <p>Why this matters for retrieval: When Athena searches for \"employee count 22\", Zep returns the fact with the full context: the stated limit (20), the threshold type (soft), and the historical precedent (approved up to 23). Athena can then make a nuanced recommendation instead of a blanket decline \u2014 e.g., \"This exceeds the stated limit of 20 but falls within the historical approval range of up to 23. Consider approving if loss history is clean.\"</p> <p>How it gets into the graph: The richness comes from two places:</p> <ol> <li> <p>The extraction prompt teaches the LLM to look for signals of flexibility in    the source text (\"generally\", \"typically\", \"has been approved\", etc.) and to    capture deviations in <code>historical_exceptions</code>.</p> </li> <li> <p>The pipeline embeds threshold metadata and historical context directly into    the edge properties and outcome description, so the full nuance is stored as    searchable fact text in Zep.</p> </li> </ol> <p>Beyond hard vs soft \u2014 other rule patterns the system captures:</p> <ul> <li> <p>Compound triggers \u2014 rules that only fire when multiple conditions combine   (e.g. \"recent major loss AND below-minimum pricing AND prior carrier rejection\").   The <code>compound_trigger</code> field captures the multi-condition logic.</p> </li> <li> <p>Data quality rules \u2014 non-traditional risk factors like exposure discrepancies,   address range anomalies, or zero-value fields. Use <code>risk_category: \"data_quality\"</code>.</p> </li> <li> <p>Geographic appetite \u2014 hard boundaries based on state, county tier, or coastal   distance. These are typically absolute (\"California out of appetite\"), with   <code>risk_category: \"geographic_appetite\"</code>.</p> </li> <li> <p>Pricing thresholds \u2014 rate minimums, TIV/sqft floors, premium bounds. Often soft   in practice because the stated minimum doesn't account for coverage scope variations.   Use <code>risk_category: \"pricing\"</code>.</p> </li> <li> <p>Process rules \u2014 behavioral patterns like \"refer for data correction, don't decline\"   when a blocking issue is correctable. Use <code>risk_category: \"process\"</code>.</p> </li> </ul> <p>Source variety matters: Soft rules emerge when you ingest from multiple source types. A manual alone only gives you the hard limit. The flexibility appears when you also ingest:</p> <ul> <li>Production decision logs (\"we approved 23 employees last quarter\")</li> <li>SME interviews (\"the 20 limit is a guideline, not a hard stop\")</li> <li>Training notes (\"for well-established businesses, we have room on employee count\")</li> </ul> <p>Set <code>source_type</code> accordingly so the graph captures the provenance.</p>"},{"location":"guides/memory/#ontology","title":"Ontology","text":"<p>Key concept: The ontology defines the vocabulary of the knowledge graph \u2014 what types of nodes and edges exist and what they mean. Each implementation defines its own ontology via Pydantic frozen models that guide LLM extraction.</p>"},{"location":"guides/memory/#athena-ontology","title":"Athena Ontology","text":"<p>5 node types:</p> Node Type Description RiskFactor An observable characteristic that affects underwriting (e.g. Gas Station, Employee Count) Rule An underwriting rule or guideline \u2014 can be hard (absolute) or soft (flexible). Has <code>threshold_type</code> property. Outcome The result of applying a rule (Refer, Decline, Approve with Conditions). Description includes threshold context. Mitigant A condition that can override or reduce a rule's impact Source Origin of the knowledge (manual, training, SME, production, compliance) <p>4 edge types (relations):</p> Relation Source \u2192 Target Key Properties TRIGGERS RiskFactor \u2192 Rule <code>threshold</code>, <code>threshold_type</code>, <code>historical_exceptions</code>, <code>confidence</code>, <code>decision_quality</code>, <code>compound_trigger</code>, <code>data_fields</code> RESULTS_IN Rule \u2192 Outcome Outcome description enriched with threshold + historical context OVERRIDES Mitigant \u2192 Rule A mitigant can override a rule DERIVED_FROM Rule \u2192 Source Source <code>type</code>: manual, training, sme, production, compliance"},{"location":"guides/memory/#ontology-registry","title":"Ontology Registry","text":"<p>Ontologies are registered at import time, following the same pattern as metric registration:</p> <pre><code>from eval_workbench.shared.memory import ontology_registry\n\n# Importing the athena memory module triggers registration\nfrom eval_workbench.implementations.athena.memory import ATHENA_ONTOLOGY\n\nontology_registry.list_ontologies()  # ['athena_underwriting']\nontology_registry.get('athena_underwriting').to_prompt_context()  # Markdown for LLM prompts\n</code></pre>"},{"location":"guides/memory/#creating-a-new-ontology","title":"Creating a New Ontology","text":"<p>To add a knowledge graph for a new agent, define an <code>OntologyDefinition</code> and register it:</p> <pre><code>from eval_workbench.shared.memory import (\n    OntologyDefinition,\n    NodeTypeDefinition,\n    EdgeTypeDefinition,\n    ontology_registry,\n)\n\nMY_ONTOLOGY = OntologyDefinition(\n    name='my_agent_domain',\n    version='1.0.0',\n    description='Knowledge graph for ...',\n    node_types=[\n        NodeTypeDefinition(label='Concept', description='...', required_properties=['name']),\n    ],\n    edge_types=[\n        EdgeTypeDefinition(relation='RELATES_TO', source_label='Concept', target_label='Concept', description='...'),\n    ],\n)\n\nontology_registry.register(MY_ONTOLOGY)\n</code></pre>"},{"location":"guides/memory/#multi-tenancy","title":"Multi-Tenancy","text":"<p>Each agent gets its own isolated graph via a Zep user. The user ID is generated from a template in settings:</p> <pre><code>user_id_template: \"{agent_name}_global_rules\"\n\nathena  \u2192 athena_global_rules\nabby  \u2192 abby_global_rules\n</code></pre> <p>This means agents cannot accidentally read or overwrite each other's knowledge. A future MCP server can expose cross-agent search by querying multiple stores.</p>"},{"location":"guides/memory/#configuration","title":"Configuration","text":""},{"location":"guides/memory/#yaml-config","title":"YAML Config","text":"<pre><code># implementations/athena/config/memory.yaml\nmemory:\n  zep:\n    api_key: \"${ZEP_API_KEY}\"\n    admin_email: \"athena@system.local\"\n    user_id_template: \"{agent_name}_global_rules\"\n  extractor:\n    model: \"gpt-4o\"\n  pipeline:\n    batch_size: 5\n</code></pre>"},{"location":"guides/memory/#environment-variables","title":"Environment Variables","text":"Variable Description Required <code>ZEP_API_KEY</code> Zep Cloud API key Yes <code>ZEP_BASE_URL</code> Zep Cloud base URL (self-hosted only) No <p>These can also be set in AthenaSettings:</p> <pre><code>from eval_workbench.implementations.athena.settings import AthenaSettings\n\nsettings = AthenaSettings()\nsettings.zep_api_key   # from ZEP_API_KEY or .env\nsettings.zep_base_url  # from ZEP_BASE_URL or .env\n</code></pre>"},{"location":"guides/memory/#installation","title":"Installation","text":"<p>The memory module uses optional dependencies to keep the core package lightweight:</p> <pre><code>pip install eval-workbench[memory]\n</code></pre> <p>This installs <code>zep-cloud</code>, <code>networkx</code>, and <code>matplotlib</code>.</p>"},{"location":"guides/memory/#architecture","title":"Architecture","text":""},{"location":"guides/memory/#file-layout","title":"File Layout","text":"<pre><code>src/eval_workbench/\n  shared/memory/                      # Shared abstractions\n    __init__.py                       # Public API re-exports\n    ontology.py                       # OntologyDefinition + OntologyRegistry\n    store.py                          # BaseGraphStore ABC + data models\n    pipeline.py                       # BasePipeline ABC\n    analytics.py                      # BaseGraphAnalytics ABC\n    settings.py                       # ZepSettings\n    zep/\n      store.py                        # ZepGraphStore (Zep Cloud backend)\n\n  implementations/athena/memory/      # Athena-specific\n    __init__.py                       # Ontology registration\n    ontology.py                       # ATHENA_ONTOLOGY constant\n    extractors.py                     # RuleExtractor (OpenAI)\n    pipeline.py                       # AthenaRulePipeline\n    analytics.py                      # AthenaGraphAnalytics\n</code></pre>"},{"location":"guides/memory/#shared-vs-implementation","title":"Shared vs Implementation","text":"<p>The split follows the same pattern as metrics and monitoring:</p> <ul> <li> <p><code>shared/memory/</code> defines the interfaces: <code>BaseGraphStore</code>, <code>BasePipeline</code>,   <code>BaseGraphAnalytics</code>, <code>OntologyDefinition</code>. These are stable abstractions that any   agent can build on.</p> </li> <li> <p><code>implementations/athena/memory/</code> provides the Athena-specific glue: the   underwriting ontology, the rule extractor prompt, the pipeline that builds ontology-correct   edges, and the analytics layer with domain methods like <code>check_risk_appetite()</code>.</p> </li> </ul> <p>To add memory for a new agent, you implement the same pattern \u2014 define an ontology, write an extractor, subclass the pipeline and analytics ABCs.</p>"},{"location":"guides/memory/#data-flow-diagram","title":"Data Flow Diagram","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502         AthenaRulePipeline             \u2502\n                    \u2502                                       \u2502\n  Raw text \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  1. RuleExtractor.extract_batch()     \u2502\n                    \u2502     (OpenAI gpt-4o)                   \u2502\n                    \u2502          \u2502                             \u2502\n                    \u2502          \u25bc                             \u2502\n                    \u2502  2. _rule_to_ingest_payload()          \u2502\n                    \u2502     (builds TRIGGERS, RESULTS_IN,      \u2502\n                    \u2502      OVERRIDES, DERIVED_FROM edges)    \u2502\n                    \u2502          \u2502                             \u2502\n                    \u2502          \u25bc                             \u2502\n                    \u2502  3. ZepGraphStore.ingest()             \u2502\n                    \u2502     (JSON \u2192 Zep Cloud API)            \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u2502  stored in Zep per-agent user\n                                       \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502         AthenaGraphAnalytics           \u2502\n                    \u2502                                       \u2502\n  \"Gas Station\" \u2500\u2500\u2500\u25b6\u2502  .query()          \u2192 matching edges   \u2502\n  \"Gas Station\" \u2500\u2500\u2500\u25b6\u2502  .trace()          \u2192 full paths       \u2502\n  \"*\"           \u2500\u2500\u2500\u25b6\u2502  .export()         \u2192 all knowledge    \u2502\n  \"Gas Station\" \u2500\u2500\u2500\u25b6\u2502  .check_risk_appetite() \u2192 alias       \u2502\n                    \u2502                                       \u2502\n                    \u2502  All go through ZepGraphStore.search() \u2502\n                    \u2502  \u2192 semantic search over the graph      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/memory/#mcp-server-readiness","title":"MCP Server Readiness","text":"<p>The interfaces are designed so an MCP server can wrap them directly. All return types are Pydantic models that serialize to JSON natively.</p> MCP Tool Maps To Type <code>memory_search</code> <code>store.search(query)</code> Query <code>memory_check_risk</code> <code>analytics.check_risk_appetite(...)</code> Query <code>memory_trace_path</code> <code>analytics.trace(...)</code> Query <code>memory_export_rules</code> <code>analytics.export()</code> Query <code>memory_list_ontologies</code> <code>ontology_registry.list_ontologies()</code> Query <code>memory_get_ontology</code> <code>ontology_registry.get(name).to_prompt_context()</code> Query <code>memory_ingest_rule</code> <code>store.ingest(payload)</code> Mutation <code>memory_add_mitigant</code> <code>store.add_mitigant(rule, mitigant)</code> Mutation <code>memory_run_pipeline</code> <code>pipeline.run(raw_data)</code> Mutation <code>memory_clear_graph</code> <code>store.clear()</code> Mutation <p>The ontology registry enables dynamic tool discovery per agent \u2014 an MCP server can list available ontologies and expose agent-specific tools.</p>"},{"location":"guides/memory/#quick-start","title":"Quick Start","text":""},{"location":"guides/memory/#full-pipeline-raw-text-graph-search","title":"Full pipeline (raw text \u2192 graph \u2192 search)","text":"<pre><code>from eval_workbench.shared.memory import ZepGraphStore\nfrom eval_workbench.implementations.athena.memory import (\n    ATHENA_ONTOLOGY,\n    AthenaRulePipeline,\n    AthenaGraphAnalytics,\n)\n\n# 1. Build the store\nstore = ZepGraphStore(agent_name='athena', ontology=ATHENA_ONTOLOGY)\n\n# 2. Ingest production learnings through the pipeline\npipeline = AthenaRulePipeline(store=store)\nresult = pipeline.run([\n    \"\"\"Athena correctly recommended decline for a lessor's risk warehouse with\n    catastrophic recent loss ($600K representing 23.5% of building exposure),\n    pricing 37% below minimum threshold, and prior carrier non-renewal. The\n    combination of recent major loss + below-minimum pricing + prior carrier\n    rejection is an automatic decline regardless of other positive factors.\"\"\",\n\n    \"\"\"The Swallow API employee count limit is 20 per the manual, but historical\n    underwriters have approved up to 23 employees for well-established businesses\n    with strong loss history. This is a soft threshold, not a hard stop.\"\"\",\n\n    \"\"\"California (CA) is currently out of appetite because the company is not yet\n    approved to operate in that state. Geographic restrictions based on state\n    approval should trigger early declination.\"\"\",\n])\nprint(f'Processed: {result.items_processed}, Ingested: {result.items_ingested}')\n</code></pre> <p>Sample output (what the LLM extractor produces from the above):</p> <pre><code>Processed: 7, Ingested: 7\n</code></pre> <p>The extractor finds ~7 rules across the 3 texts. For example, the warehouse text produces a compound trigger rule and a pricing rule. Here's what the extracted dicts look like and the edges they generate:</p> <pre><code># Rule 1 (from the warehouse learning):\n# {\n#     \"risk_factor\": \"Adverse Selection Trifecta\",\n#     \"risk_category\": \"eligibility\",\n#     \"rule_name\": \"Adverse Selection Auto-Decline\",\n#     \"action\": \"decline\",\n#     \"threshold_type\": \"hard\",\n#     \"compound_trigger\": \"recent_major_loss AND below_minimum_pricing AND prior_carrier_rejection\",\n#     \"decision_quality\": \"aligned\",\n#     \"data_fields\": [\"auxData.rateData.input.total_loss_payment\", ...]\n# }\n#\n# \u2192 Adverse Selection Trifecta --TRIGGERS--&gt; Adverse Selection Auto-Decline\n# \u2192 Adverse Selection Auto-Decline --RESULTS_IN--&gt; Decline\n# \u2192 Adverse Selection Auto-Decline --DERIVED_FROM--&gt; Production Decision Log\n\n# Rule 2 (from the employee count learning):\n# {\n#     \"risk_factor\": \"Employee Count\",\n#     \"rule_name\": \"Swallow API Employee Limit\",\n#     \"action\": \"decline\",\n#     \"threshold\": {\"field\": \"employee_count\", \"operator\": \"gt\", \"value\": 20},\n#     \"threshold_type\": \"soft\",\n#     \"historical_exceptions\": \"UWs have approved up to 23 employees\"\n# }\n#\n# \u2192 Employee Count --TRIGGERS--&gt; Swallow API Employee Limit\n#     (threshold_type: soft, historical: \"up to 23 approved\")\n# \u2192 Swallow API Employee Limit --RESULTS_IN--&gt; Decline\n#     (description includes \"...but UWs have approved up to 23...\")\n# \u2192 Strong loss history --OVERRIDES--&gt; Swallow API Employee Limit\n\n# Rule 3 (from the California learning):\n# {\n#     \"risk_factor\": \"California State\",\n#     \"risk_category\": \"geographic_appetite\",\n#     \"rule_name\": \"California Out of Appetite\",\n#     \"action\": \"decline\",\n#     \"threshold_type\": \"hard\"\n# }\n</code></pre> <p>Then at query time:</p> <pre><code># 3. Search\nanalytics = AthenaGraphAnalytics(store=store)\n\n# Hard rule \u2014 clear answer\nresults = analytics.check_risk_appetite('California', product_type='BOP')\n# \u2192 \"California State --TRIGGERS--&gt; California Out of Appetite\"\n#   threshold_type: hard, no exceptions\n\n# Soft threshold \u2014 nuanced answer with historical context\nresults = analytics.check_risk_appetite('employee count 22', product_type='BOP')\n# \u2192 \"Employee Count --TRIGGERS--&gt; Swallow API Employee Limit\"\n#   threshold_type: soft\n#   historical: \"UWs have approved up to 23 employees\"\n#   outcome: \"Limit is 20 per manual, but UWs approved up to 23...\"\n\n# Compound trigger \u2014 surfaces the multi-condition rule\nresults = analytics.check_risk_appetite('prior loss and coverage gap', product_type='ALL')\n# \u2192 \"Adverse Selection Trifecta --TRIGGERS--&gt; Adverse Selection Auto-Decline\"\n#   compound: \"recent_major_loss AND below_minimum_pricing AND prior_carrier_rejection\"\n</code></pre>"},{"location":"guides/memory/#from-yaml-config","title":"From YAML config","text":"<pre><code>pipeline = AthenaRulePipeline.from_yaml(\n    'src/eval_workbench/implementations/athena/config/memory.yaml'\n)\nresult = pipeline.run(raw_texts)\n</code></pre>"},{"location":"guides/memory/#pre-extracted-rules-skip-llm","title":"Pre-extracted rules (skip LLM)","text":"<p>If you already have structured rules (e.g. from a spreadsheet or manual curation), you can skip the extractor and ingest directly:</p> <pre><code>store = ZepGraphStore(agent_name='athena', ontology=ATHENA_ONTOLOGY)\npipeline = AthenaRulePipeline(store=store)\n\nrule = {\n    'risk_factor': 'Gas Station',\n    'rule_name': 'Gas Station Auto-Refer',\n    'product_type': 'LRO',\n    'action': 'refer',\n    'outcome_description': 'Referred due to fuel storage liability.',\n    'mitigants': ['Fire suppression system installed'],\n    'source': 'Underwriting Manual v3.2',\n    'confidence': 'high',\n}\n\npayload = AthenaRulePipeline._rule_to_ingest_payload(rule)\nstore.ingest(payload)\n</code></pre>"},{"location":"guides/memory/#adding-a-mitigant-to-an-existing-rule","title":"Adding a mitigant to an existing rule","text":"<pre><code>store.add_mitigant('Gas Station Auto-Refer', 'Annual fire inspection on file')\n</code></pre>"},{"location":"guides/memory/#demo-script","title":"Demo script","text":"<p>A runnable demo with sample rules is available at:</p> <pre><code>python private_scripts/run_memory_graph.py          # full demo\npython private_scripts/run_memory_graph.py --clear   # clear + re-ingest\npython private_scripts/run_memory_graph.py --export rules.json\n</code></pre> <p>Requires <code>ZEP_API_KEY</code> to be set.</p>"},{"location":"guides/memory/#agent-entry-points","title":"Agent Entry Points","text":"<p>This section describes how an AI agent (like Athena) actually uses the memory system at runtime. There are two integration paths: direct Python API and MCP tools.</p>"},{"location":"guides/memory/#path-1-direct-python-api-current","title":"Path 1: Direct Python API (current)","text":"<p>The agent's workflow code imports and calls the analytics layer directly. This is the simplest path and works for agents running in the same Python process.</p> <p>At agent initialization:</p> <pre><code>from eval_workbench.shared.memory import ZepGraphStore\nfrom eval_workbench.implementations.athena.memory import (\n    ATHENA_ONTOLOGY,\n    AthenaGraphAnalytics,\n)\n\n# Build once, reuse across the agent's lifetime\nstore = ZepGraphStore(agent_name='athena', ontology=ATHENA_ONTOLOGY)\nanalytics = AthenaGraphAnalytics(store=store)\n</code></pre> <p>During underwriting evaluation (the main entry point for the agent):</p> <pre><code>def evaluate_submission(submission: dict) -&gt; dict:\n    \"\"\"Where the agent calls into memory during its decision-making loop.\"\"\"\n\n    risk_factor = submission['occupancy_description']  # e.g. \"Gas Station\"\n    product_type = submission['product_type']           # e.g. \"LRO\"\n\n    # Primary entry point: check if this risk is in the knowledge graph\n    rules = analytics.check_risk_appetite(risk_factor, product_type=product_type)\n\n    for rule in rules:\n        action = rule.get('action')          # \"refer\", \"decline\", etc.\n        threshold_type = rule.get('threshold_type')  # \"hard\" or \"soft\"\n        historical = rule.get('historical_exceptions')\n\n        if action == 'decline' and threshold_type == 'hard':\n            return {'decision': 'decline', 'reason': rule['outcome']}\n        elif threshold_type == 'soft' and historical:\n            # Nuanced decision \u2014 the agent can weigh flexibility\n            ...\n\n    # For explainability / audit trail\n    paths = analytics.trace(risk_factor, product_type=product_type)\n    return {'decision': '...', 'reasoning_chain': paths}\n</code></pre> <p>For knowledge ingestion (called by an admin workflow, CI/CD, or a learning pipeline \u2014 not during real-time evaluation):</p> <pre><code>from eval_workbench.implementations.athena.memory import AthenaRulePipeline\n\npipeline = AthenaRulePipeline(store=store)\n\n# Ingest raw production learnings (LLM extraction + graph storage)\nresult = pipeline.run(raw_learnings_text, batch_size=5)\n</code></pre> <p>The key insight: retrieval is on the hot path, ingestion is not. The agent queries the graph during every submission evaluation, but ingestion happens offline when new learnings are collected.</p>"},{"location":"guides/memory/#path-2-mcp-tools-future","title":"Path 2: MCP Tools (future)","text":"<p>When the memory system is wrapped in an MCP server, the agent's LLM calls tools instead of Python functions. The tool signatures map directly to the Python API:</p> <pre><code>Agent LLM                          MCP Server                    Memory System\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\"Check if Gas Station              memory_check_risk             analytics.check_risk_appetite(\n is in appetite for LRO\"  \u2500\u2500\u2500\u2500\u2500\u2500\u25b6  {risk_factor: \"Gas Station\",    \"Gas Station\",\n                                    product_type: \"LRO\"}          product_type=\"LRO\"\n                                          \u2502                      )\n                                          \u25bc                         \u2502\n                                   Returns JSON with               \u25bc\n                                   matching rules, thresholds,   GraphSearchResult\n                                   historical exceptions         \u2192 serialized to JSON\n</code></pre> <p>The MCP tool names and their Python API equivalents:</p> Agent asks... MCP Tool Python Entry Point \"Is this risk in appetite?\" <code>memory_check_risk</code> <code>analytics.check_risk_appetite(risk, product_type=...)</code> \"What rules apply here?\" <code>memory_search</code> <code>store.search(query, limit=...)</code> \"Show the full decision path\" <code>memory_trace_path</code> <code>analytics.trace(risk, product_type=...)</code> \"What does our ontology cover?\" <code>memory_get_ontology</code> <code>ontology_registry.get(name).to_prompt_context()</code> \"Record this new learning\" <code>memory_run_pipeline</code> <code>pipeline.run(raw_data)</code> <p>In this path, the LLM decides when to query memory based on its system prompt. A typical system prompt injection:</p> <pre><code># Inject the ontology into the agent's system prompt so it knows what to search for\nontology_context = ATHENA_ONTOLOGY.to_prompt_context()\nsystem_prompt += f\"\"\"\nYou have access to an underwriting knowledge graph. Use the memory_check_risk tool\nto look up any risk factor before making a recommendation. The graph contains:\n\n{ontology_context}\n\nAlways check memory before recommending decline or referral.\n\"\"\"\n</code></pre>"},{"location":"guides/memory/#which-path-to-choose","title":"Which path to choose","text":"Consideration Direct Python API MCP Tools Setup complexity Lower \u2014 just import and call Higher \u2014 requires MCP server Agent autonomy Agent code decides when to query LLM decides when to query Multi-language Python only Any language that speaks MCP Latency Lower \u2014 no HTTP hop Slightly higher \u2014 MCP protocol overhead Best for Deterministic agent workflows LLM-driven autonomous agents <p>Most agents will start with the direct Python API and migrate to MCP tools when they need the LLM to autonomously decide when to query memory.</p>"},{"location":"guides/memory/#where-to-look-cheat-sheet","title":"\"Where to look\" cheat-sheet","text":"What Where Shared ABCs + data models <code>src/eval_workbench/shared/memory/</code> Zep backend implementation <code>src/eval_workbench/shared/memory/zep/store.py</code> Athena ontology definition <code>src/eval_workbench/implementations/athena/memory/ontology.py</code> LLM rule extractor <code>src/eval_workbench/implementations/athena/memory/extractors.py</code> Athena ingestion pipeline <code>src/eval_workbench/implementations/athena/memory/pipeline.py</code> Athena analytics / search <code>src/eval_workbench/implementations/athena/memory/analytics.py</code> YAML config <code>src/eval_workbench/implementations/athena/config/memory.yaml</code> Zep settings <code>src/eval_workbench/shared/memory/settings.py</code> Runnable demo <code>private_scripts/run_memory_graph.py</code>"},{"location":"guides/monitoring-config/","title":"Monitoring Configuration (YAML)","text":"<p>This guide explains the <code>OnlineMonitor</code> YAML configuration format. Start from <code>src/shared/monitoring/monitoring.yaml.example</code> and customize it for your implementation.</p>"},{"location":"guides/monitoring-config/#1-top-level-fields","title":"1) Top-Level Fields","text":"<pre><code>version: \"1.0\"\nname: \"my_monitor\"\n</code></pre> <ul> <li><code>version</code>: config schema version</li> <li><code>name</code>: monitor name (used for dedup + experiment naming)</li> </ul>"},{"location":"guides/monitoring-config/#optional-scored-items-store","title":"Optional scored items store","text":"<pre><code>scored_store:\n  # type: none | csv | db\n  type: csv\n  # file_path: \"data/scored_items.csv\"\n  # connection_string: \"postgresql://user:pass@host:5432/dbname\"\n</code></pre> <ul> <li><code>type</code>: <code>csv</code> uses file-based dedup, <code>db</code> uses <code>evaluation_dataset</code>, <code>none</code> disables</li> <li><code>file_path</code>: only for <code>csv</code></li> <li><code>connection_string</code>: only for <code>db</code></li> </ul>"},{"location":"guides/monitoring-config/#optional-schedule-for-monitoringscheduler","title":"Optional schedule (for MonitoringScheduler)","text":"<pre><code>schedule:\n  # Use ONE of these\n  interval_minutes: 10\n  # cron: \"*/10 * * * *\"\n</code></pre> <ul> <li><code>interval_minutes</code>: run every N minutes</li> <li><code>cron</code>: cron expression for aligned schedules</li> </ul>"},{"location":"guides/monitoring-config/#2-source-configuration","title":"2) Source Configuration","text":""},{"location":"guides/monitoring-config/#langfuse-source","title":"Langfuse source","text":"<pre><code>source:\n  type: langfuse\n  name: \"my_agent\"\n  component: \"my_component\"\n  extractor: \"implementations.my_agent.extractors.extract_trace\"\n  # prompt_patterns: \"implementations.my_agent.langfuse.prompt_patterns.MyPromptPatterns\"\n  limit: 100\n  days_back: 7\n  tags: [\"production\"]\n  timeout: 60\n  fetch_full_traces: true\n  show_progress: true\n</code></pre> <p>Key fields:</p> <ul> <li><code>extractor</code>: python path to a <code>(Trace) -&gt; DatasetItem</code> function</li> <li><code>limit</code>: max traces to fetch</li> <li><code>days_back</code> / <code>hours_back</code> / <code>minutes_back</code>: time window (days_back wins if both set; hours_back wins over minutes_back)</li> <li><code>tags</code>: filter by Langfuse tags</li> <li><code>fetch_full_traces</code>: include observations/scores (slower but richer)</li> </ul>"},{"location":"guides/monitoring-config/#slack-source-type-slack","title":"Slack source (<code>type: slack</code>)","text":"<pre><code>source:\n  type: slack\n  name: \"my_slack_channels\"\n  channel_ids:\n    - \"C0XXXXXXXXX\"\n  limit: 50\n  scrape_threads: true\n  bot_name: \"MyBot\"\n  bot_names: [\"MyBot\", \"MyBotV2\"]\n  workspace_domain: \"myworkspace\"\n  drop_if_first_is_user: true\n  drop_if_all_ai: true\n  max_concurrent: 2\n  # filter_sender: \"U1234567890\"\n\n  # Time windows (mutually exclusive with oldest_ts/latest_ts)\n  # window_days: 7\n  # window_hours: 24\n  # window_minutes: 60\n  # oldest_ts: 1700000000.0\n  # latest_ts: 1700100000.0\n\n  # Filtering\n  exclude_senders: [\"Prometheus\"]\n  drop_message_regexes:\n    - \"^Street View\"\n    - \"google\\\\.com/maps\"\n  strip_citation_block: true\n\n  # Display name mapping\n  member_id_to_display_name:\n    U09JKDU4RE2: \"Athena assistant\"\n  human_mention_token: \"@human\"\n</code></pre>"},{"location":"guides/monitoring-config/#slack-source-field-reference","title":"Slack source field reference","text":"Field Type Default Description <code>name</code> str Required Source identifier <code>channel_ids</code> list[str] Required Slack channel IDs to scrape <code>limit</code> int <code>10</code> Max messages per channel <code>scrape_threads</code> bool <code>true</code> Include thread replies <code>filter_sender</code> str \u2014 Only include messages from this sender <code>bot_name</code> str <code>\"Athena\"</code> Name identifying AI messages <code>bot_names</code> list[str] \u2014 Multiple names identifying AI messages <code>workspace_domain</code> str <code>\"mgtinsurance\"</code> Slack workspace subdomain <code>drop_if_first_is_user</code> bool <code>false</code> Drop if first message is from user <code>drop_if_all_ai</code> bool <code>false</code> Drop if all messages are AI <code>max_concurrent</code> int <code>2</code> Max concurrent channel scrapes <code>oldest_ts</code> float \u2014 Inclusive lower timestamp bound <code>latest_ts</code> float \u2014 Inclusive upper timestamp bound <code>window_days</code> float \u2014 Relative lookback in days <code>window_hours</code> float \u2014 Relative lookback in hours <code>window_minutes</code> float \u2014 Relative lookback in minutes <code>exclude_senders</code> list[str] \u2014 Sender names to omit from conversations <code>drop_message_regexes</code> list[str] \u2014 Regex patterns \u2014 matching messages are dropped <code>strip_citation_block</code> bool <code>false</code> Remove trailing citation blocks like <code>[1] ...</code> <code>member_id_to_display_name</code> dict \u2014 Slack user ID \u2192 display name mapping <code>human_mention_token</code> str <code>\"@human\"</code> Token for identifying human mentions <p>Time window precedence</p> <p><code>oldest_ts</code>/<code>latest_ts</code> take priority over relative windows. Among relative windows: <code>window_days</code> &gt; <code>window_hours</code> &gt; <code>window_minutes</code>.</p>"},{"location":"guides/monitoring-config/#slack-neon-join-source-type-slack_neon_join","title":"Slack-Neon Join source (<code>type: slack_neon_join</code>)","text":"<p>Fetches Slack threads and Neon database rows, then joins them on configurable columns. Useful when conversation context needs to be enriched with database metadata (e.g., case IDs, trace IDs).</p> <pre><code>source:\n  type: slack_neon_join\n  name: \"athena\"\n  channel_ids:\n    - \"C09MAP9HR9D\"\n    - \"C09JE5SSP43\"\n  limit: 100\n  scrape_threads: true\n  bot_names: [\"Athena\"]\n  workspace_domain: \"mgtinsurance\"\n  drop_if_first_is_user: true\n  drop_if_all_ai: true\n  max_concurrent: 2\n  exclude_senders: [\"Prometheus\"]\n  drop_message_regexes:\n    - \"^Street View\"\n    - \"google\\\\.com/maps\"\n  strip_citation_block: true\n  member_id_to_display_name:\n    U09JKDU4RE2: \"Athena assistant\"\n  human_mention_token: \"@human\"\n  use_slack_thread_dataset_id: true\n  window_minutes: 10000\n\n  # Neon join configuration\n  neon_query: |\n    SELECT\n      slack_thread_ts,\n      slack_channel_id,\n      quote_locator,\n      langfuse_trace_id AS trace_id,\n      created_at\n    FROM athena_cases\n  slack_join_columns: [\"channel_id\", \"thread_ts\"]\n  neon_join_columns: [\"slack_channel_id\", \"slack_thread_ts\"]\n  neon_time_column: \"created_at\"\n  buffer_minutes: 30\n  connection_string: \"${DATABASE_URL}\"\n</code></pre>"},{"location":"guides/monitoring-config/#join-specific-fields","title":"Join-specific fields","text":"<p>All Slack source fields above are also supported. These additional fields control the Neon join:</p> Field Type Default Description <code>neon_query</code> str Required SQL query to fetch Neon rows <code>slack_join_columns</code> list[str] Required Column names in Slack dataset for join keys <code>neon_join_columns</code> list[str] Required Column names in Neon results for join keys (must match length) <code>dataset_id_column</code> str \u2014 Use this column from merged data as item ID <code>use_slack_thread_dataset_id</code> bool <code>false</code> Construct ID as <code>slack-{channel_id}-{thread_ts}</code> <code>neon_time_column</code> str <code>\"created_at\"</code> Timestamp column for time bounds <code>buffer_minutes</code> float <code>0.0</code> Time buffer (minutes) to expand Neon query bounds <code>neon_connection_string</code> str <code>DATABASE_URL</code> env Database URL for Neon <code>neon_chunk_size</code> int <code>1000</code> Rows per batch when streaming <code>neon_timeout_seconds</code> float \u2014 Query timeout override"},{"location":"guides/monitoring-config/#neon-source-type-neon","title":"Neon source (<code>type: neon</code>)","text":"<p>Direct PostgreSQL queries with a custom extractor function. Use when your data source is a database table rather than Slack or Langfuse.</p> <pre><code>source:\n  type: neon\n  name: \"athena_cases\"\n  query: |\n    SELECT\n      id,\n      quote_locator,\n      recommendation_entries,\n      langfuse_trace_id,\n      created_at\n    FROM athena_cases\n    WHERE created_at &gt;= NOW() - INTERVAL '1 hour'\n    ORDER BY created_at DESC\n  extractor: \"eval_workbench.implementations.athena.extractors.extract_recommendation_from_row\"\n  limit: 100\n</code></pre>"},{"location":"guides/monitoring-config/#neon-source-field-reference","title":"Neon source field reference","text":"Field Type Default Description <code>name</code> str Required Source identifier <code>query</code> str Required SQL query to execute <code>extractor</code> str Required Dotted Python path to <code>(row_dict) -&gt; DatasetItem</code> function <code>connection_string</code> str <code>DATABASE_URL</code> env Database URL <code>params</code> dict/tuple \u2014 Query parameters for parameterized SQL <code>limit</code> int \u2014 Append <code>LIMIT</code> clause if query lacks one <code>has_field</code> str \u2014 Skip rows where this field is empty/falsy <code>chunk_size</code> int <code>1000</code> Rows per batch when streaming <code>timeout_seconds</code> float \u2014 Per-query statement timeout override"},{"location":"guides/monitoring-config/#3-sampling-configuration","title":"3) Sampling Configuration","text":"<pre><code>sampling:\n  strategy: random\n  n: 10\n  # seed: 42\n</code></pre> <p>Strategies:</p> <ul> <li><code>all</code> (default)</li> <li><code>random</code></li> <li><code>most_recent</code></li> <li><code>oldest</code></li> </ul> <p>Best practice: set <code>source.limit</code> much larger than <code>sampling.n</code>.</p>"},{"location":"guides/monitoring-config/#4-metrics-configuration","title":"4) Metrics Configuration","text":"<pre><code>metrics_config:\n  MyMetric:\n    class: \"my_metric_class\"\n    llm_provider: \"openai\"\n    model_name: \"gpt-4o\"\n</code></pre> <p>Each metric key is a label in results; <code>class</code> must match a registered metric.</p>"},{"location":"guides/monitoring-config/#5-publishing-configuration-optional","title":"5) Publishing Configuration (Optional)","text":"<pre><code>publishing:\n  push_to_db: false\n  push_to_langfuse: false\n  trace_experiment: false\n  # metric_names:\n  #   - \"MyMetric\"\n\n  database:\n    on_conflict: do_nothing\n    chunk_size: 1000\n\n  experiment:\n    enabled: true\n    dataset_name: \"development\"\n    run_name: \"online-${ENVIRONMENT:-preview}\"\n    link_to_traces: true\n    # run_metadata: {}\n    # tags: [\"monitoring\", \"athena\"]\n    # flush: true\n    # score_on_runtime_traces: false\n    # metrics:  # legacy: prefer publishing.metric_names\n    #   - \"MyMetric\"\n</code></pre>"},{"location":"guides/monitoring-config/#usage-examples","title":"Usage Examples","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, ScoredItemsStore\n\n# Basic usage\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\")\nresults = monitor.run()\n\n# With dedup store\nstore = ScoredItemsStore(\"data/scored_items.csv\")\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\", scored_store=store)\nresults = monitor.run(deduplicate=True)\n\n# With overrides\nmonitor = OnlineMonitor.from_yaml(\n    \"config/monitoring.yaml\",\n    overrides={\n        \"source.limit\": 50,\n        \"sampling.strategy\": \"most_recent\",\n        \"sampling.n\": 5,\n    },\n)\n</code></pre>"},{"location":"guides/monitoring/","title":"Online Monitoring","text":"<p>Online monitoring module with pluggable data sources and deduplication. Supports multiple data sources (Langfuse, Slack, etc.) and tracks scored items to avoid re-processing. Can be scheduled via APScheduler (local/dev) or GitHub Actions cron (production).</p>"},{"location":"guides/monitoring/#scored-items-store","title":"Scored Items Store","text":"<p>The <code>ScoredItemsStore</code> abstract base class provides deduplication to avoid re-processing items that have already been evaluated. Two implementations are available:</p> <ul> <li>CSVScoredItemsStore: File-based storage using CSV (pandas for reads, raw CSV for appends)</li> <li>DBScoredItemsStore: Database-backed storage using the <code>evaluation_dataset</code> table</li> </ul>"},{"location":"guides/monitoring/#csv-backed-deduplication","title":"CSV-backed deduplication","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, CSVScoredItemsStore\n\nstore = CSVScoredItemsStore(\"data/scored_items.csv\")\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\", scored_store=store)\nresults = monitor.run(publish=True)\n</code></pre> <p>You can also configure a scored store directly in YAML:</p> <pre><code>scored_store:\n  type: csv\n  file_path: \"data/scored_items.csv\"\n</code></pre>"},{"location":"guides/monitoring/#database-backed-deduplication","title":"Database-backed deduplication","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, DBScoredItemsStore\n\n# Uses DATABASE_URL environment variable\nstore = DBScoredItemsStore()\n\n# Or with explicit connection string\nstore = DBScoredItemsStore(connection_string=\"postgresql://...\")\n\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\", scored_store=store)\nresults = monitor.run(publish=True)\n</code></pre> <p>YAML version:</p> <pre><code>scored_store:\n  type: db\n  connection_string: \"postgresql://...\"\n</code></pre>"},{"location":"guides/monitoring/#scheduling-via-yaml","title":"Scheduling via YAML","text":"<p>If you use <code>MonitoringScheduler</code>, you can define the interval or cron in YAML:</p> <pre><code>schedule:\n  interval_minutes: 10\n  # cron: \"*/10 * * * *\"\n</code></pre>"},{"location":"guides/monitoring/#no-deduplication","title":"No deduplication","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor\n\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\")  # scored_store=None\nresults = monitor.run(deduplicate=False, publish=True)\n</code></pre>"},{"location":"guides/monitoring/#example-programmatic-setup","title":"Example - Programmatic setup","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, LangfuseDataSource, DBScoredItemsStore\n\nsource = LangfuseDataSource(\n    name=\"athena\",\n    extractor=extract_fn,\n    limit=100,\n    hours_back=2,\n)\nstore = DBScoredItemsStore()\nmonitor = OnlineMonitor(\n    name=\"athena_monitor\",\n    source=source,\n    metrics_config={\"Metric\": {\"class\": \"metric_class\"}},\n    scored_store=store,\n)\nresults = monitor.run()\n</code></pre>"},{"location":"guides/monitoring/#example-scheduled-monitoring","title":"Example - Scheduled monitoring","text":"<pre><code>from eval_workbench.shared.monitoring import MonitoringScheduler, DBScoredItemsStore\n\nstore = DBScoredItemsStore()\nscheduler = MonitoringScheduler(scored_store=store)\n\nscheduler.add_monitor(\"config/monitoring.yaml\", interval_minutes=60)\nscheduler.start()\n</code></pre>"},{"location":"guides/monitoring/#example-config-access","title":"Example - Config access","text":"<pre><code>from shared import config\n\nconfig.load(\"config/monitoring.yaml\")\nlimit = config.get(\"trace_loader.limit\")\n\n# Temporary config overrides\nwith config.set({\"trace_loader.limit\": 10}):\n    pass\n</code></pre>"},{"location":"guides/scripts/","title":"Scripts Reference","text":"<p> Runnable entrypoints for monitoring, table creation, and KPI population. All scripts live in <code>scripts/</code> and can be run directly with Python. </p> 1 <p>monitoring_entrypoint.py</p> <p>Run a single monitoring pass from a YAML config. Designed for GitHub Actions cron.</p> 2 <p>create_evaluation_tables.py</p> <p>Create <code>evaluation_dataset</code>, <code>evaluation_results</code> tables, and a joined view.</p> 3 <p>create_athena_kpi_objects.py</p> <p>Create the <code>agent_kpi_logs</code> EAV table for KPI observations.</p> 4 <p>create_rule_extractions_table.py</p> <p>Create the <code>rule_extractions</code> table for memory pipeline output.</p> 5 <p>populate_athena_kpis.py</p> <p>Populate <code>agent_kpi_logs</code> from <code>athena_cases</code> and <code>evaluation_results</code>.</p>"},{"location":"guides/scripts/#monitoring_entrypointpy","title":"monitoring_entrypoint.py","text":"<p>Run a single <code>OnlineMonitor</code> pass from a YAML config file and exit. Designed for GitHub Actions cron runs (run-once per schedule).</p> <pre><code>python scripts/monitoring/monitoring_entrypoint.py [config_file]\n</code></pre> <p>Arguments:</p> Argument Default Description <code>config_file</code> <code>monitoring_langfuse.yaml</code> Config file path or name. Searches <code>implementations/athena/config/</code> if not found directly. <code>.yaml</code> extension optional. <p>Environment Variables:</p> Variable Default Description <code>DEDUPLICATE</code> <code>true</code> Enable/disable deduplication <code>ENVIRONMENT</code> \u2014 Production environment designation <code>OPENAI_API_KEY</code> \u2014 OpenAI API key (for LLM metrics) <code>ANTHROPIC_API_KEY</code> \u2014 Anthropic API key (for LLM metrics) <code>DATABASE_URL</code> \u2014 PostgreSQL connection string <code>LANGFUSE_*</code> \u2014 Langfuse credentials <p>Examples:</p> <pre><code># Default config\npython scripts/monitoring/monitoring_entrypoint.py\n\n# Specific config (with or without extension)\npython scripts/monitoring/monitoring_entrypoint.py monitoring_slack\npython scripts/monitoring/monitoring_entrypoint.py monitoring_slack.yaml\n\n# Full path\npython scripts/monitoring/monitoring_entrypoint.py src/eval_workbench/implementations/athena/config/monitoring_neon.yaml\n\n# Disable deduplication\nDEDUPLICATE=false python scripts/monitoring/monitoring_entrypoint.py monitoring_slack\n</code></pre>"},{"location":"guides/scripts/#create_evaluation_tablespy","title":"create_evaluation_tables.py","text":"<p>Creates the core evaluation persistence tables and a joined view.</p> <pre><code>python scripts/create_evaluation_tables.py\n</code></pre> <p>Tables created:</p> <p><code>evaluation_dataset</code> \u2014 Stores dataset items for evaluation.</p> Column Type Description <code>dataset_id</code> TEXT PK Unique identifier <code>query</code> TEXT Input query <code>expected_output</code> TEXT Ground truth <code>actual_output</code> TEXT Model output <code>conversation</code> JSONB Full conversation <code>additional_input</code> JSONB Extra context <code>source_type</code> TEXT Data source type <code>environment</code> TEXT Production/staging <code>trace_id</code> TEXT Langfuse trace ID <code>created_at</code> TIMESTAMPTZ Row creation time <p><code>evaluation_results</code> \u2014 Stores metric evaluation results.</p> Column Type Description <code>run_id</code> TEXT Evaluation run identifier <code>dataset_id</code> TEXT FK Foreign key to evaluation_dataset <code>metric_name</code> TEXT Name of metric <code>metric_score</code> DOUBLE Numeric score <code>passed</code> BOOLEAN Whether metric passed <code>explanation</code> TEXT Score explanation <code>signals</code> JSONB Detailed signal data <code>model_name</code> TEXT LLM model used <code>timestamp</code> TIMESTAMPTZ Evaluation time <p>Primary key: <code>(run_id, dataset_id, metric_name)</code></p> <p><code>evaluation_view</code> \u2014 Joined view combining both tables.</p> <p>Requires: <code>DATABASE_URL</code> environment variable.</p>"},{"location":"guides/scripts/#create_athena_kpi_objectspy","title":"create_athena_kpi_objects.py","text":"<p>Creates the <code>agent_kpi_logs</code> EAV (Entity-Attribute-Value) table for storing agent KPI observations.</p> <pre><code>python scripts/create_athena_kpi_objects.py\n</code></pre> <p>Table: <code>agent_kpi_logs</code></p> Column Type Description <code>id</code> TEXT PK UUID <code>created_at</code> TIMESTAMPTZ Observation timestamp <code>source_name</code> TEXT Source (e.g., <code>athena</code>) <code>kpi_name</code> TEXT KPI name (e.g., <code>stp_rate</code>) <code>kpi_category</code> TEXT Category (see below) <code>dataset_id</code> TEXT Reference to specific case <code>numeric_value</code> DOUBLE KPI numeric value <code>text_value</code> TEXT KPI text value <code>json_value</code> JSONB KPI JSON value <code>source_component</code> TEXT Component (e.g., <code>underwriter</code>) <code>source_step</code> TEXT Pipeline step <code>environment</code> TEXT Defaults to <code>production</code> <code>tags</code> JSONB Flexible tagging <code>metadata</code> JSONB Additional metadata <p>KPI Categories: <code>operational_efficiency</code>, <code>risk_accuracy</code>, <code>data_integrity</code>, <code>commercial_impact</code></p> <p>Indexes: Composite on <code>(source_name, kpi_name)</code>, <code>kpi_category</code>, <code>dataset_id</code>, <code>created_at</code>, and a time-series index on <code>(source_name, kpi_name, created_at DESC)</code>.</p> <p>Requires: <code>DATABASE_URL</code> environment variable.</p>"},{"location":"guides/scripts/#create_rule_extractions_tablepy","title":"create_rule_extractions_table.py","text":"<p>Creates the <code>rule_extractions</code> table for storing structured rules extracted by the memory pipeline.</p> <pre><code>python scripts/create_rule_extractions_table.py\n</code></pre> <p>Table: <code>rule_extractions</code></p> Column Type Description <code>id</code> TEXT PK Unique identifier <code>created_at</code> TIMESTAMPTZ Row creation time <code>batch_id</code> TEXT Batch identifier <code>agent_name</code> TEXT Agent (defaults to <code>athena</code>) <code>raw_text</code> TEXT Original text content <code>raw_text_hash</code> TEXT Hash for deduplication <code>risk_factor</code> TEXT Risk factor name <code>risk_category</code> TEXT Risk category <code>rule_name</code> TEXT Rule name <code>product_type</code> TEXT Product type <code>action</code> TEXT Rule action <code>outcome_description</code> TEXT Expected outcome <code>mitigants</code> JSONB Mitigating factors <code>threshold</code> JSONB Threshold definitions <code>data_fields</code> JSONB Required data fields <code>ingestion_status</code> TEXT pending / processed / error <p>Indexes: On <code>ingestion_status</code>, <code>agent_name</code>, <code>batch_id</code>, <code>rule_name</code>, and composite <code>(risk_factor, product_type)</code>.</p> <p>Requires: <code>DATABASE_URL</code> environment variable.</p>"},{"location":"guides/scripts/#populate_athena_kpispy","title":"populate_athena_kpis.py","text":"<p>Populates <code>agent_kpi_logs</code> by reading from <code>athena_cases</code> and <code>evaluation_results</code>, then inserting deduplicated KPI observations.</p> <pre><code>python scripts/populate_athena_kpis.py\n</code></pre> <pre><code># Scheduled-style backfill window\npython scripts/populate_athena_kpis.py --since-days 2\n</code></pre> <p>KPIs computed:</p> KPI Category Source Logic <code>stp_rate</code> operational_efficiency athena_cases 1.0 if no UW flags, else 0.0 <code>time_to_quote</code> operational_efficiency athena_cases <code>executionTimeMs / 1000</code> <code>referral_rate</code> operational_efficiency athena_cases 1.0 if UW flags present <code>bindable_quote_rate</code> commercial_impact athena_cases 1.0 if no UW flags <code>decision_variance</code> risk_accuracy evaluation_results 0.0 if outcome matches, else 1.0 <code>referral_accuracy</code> risk_accuracy evaluation_results metric_score from Refer Reason <code>faithfulness_score</code> data_integrity evaluation_results metric_score from UW Faithfulness <code>hallucination_count</code> data_integrity evaluation_results Count from signals JSONB <p>Behavior:</p> <ul> <li>Reads from source database, writes to target database</li> <li><code>incremental</code> mode (default): inserts only new <code>(dataset_id, kpi_name)</code> rows</li> <li><code>backfill</code> mode: overwrites existing <code>(dataset_id, kpi_name)</code> rows in the lookback window</li> <li>Safe to re-run (idempotent)</li> <li>Default lookback: 30 days</li> </ul> <p>Arguments:</p> Argument Default Description <code>--since-days</code> <code>30</code> Lookback window for source rows <code>--source-database-url</code> env fallback Source DB URL (falls back to <code>SOURCE_DATABASE_URL</code>, then <code>DATABASE_URL</code>) <code>--target-database-url</code> env fallback Target DB URL (falls back to <code>TARGET_DATABASE_URL</code>, then <code>DATABASE_URL</code>) <code>--write-mode</code> <code>incremental</code> <code>incremental</code> or <code>backfill</code> <p>Requires: <code>DATABASE_URL</code> environment variable (or separate source/target URLs).</p>"},{"location":"guides/scripts/#scheduled-automation-github-actions","title":"Scheduled automation (GitHub Actions)","text":"<p>Workflow file: <code>.github/workflows/populate_athena_kpis.yml</code></p> <ul> <li>Cron: daily at 00:00 UTC</li> <li>Scheduled runs are gated by repo variable <code>KPI_POPULATION_ENABLED=true</code></li> <li>Manual trigger available via Run workflow</li> <li>Manual trigger supports <code>write_mode</code> (<code>incremental</code> or <code>backfill</code>)</li> <li>Secrets:</li> <li><code>DATABASE_URL</code> for same source/target DB, or</li> <li><code>SOURCE_DATABASE_URL</code> + <code>TARGET_DATABASE_URL</code> for split DBs</li> </ul>"},{"location":"guides/shared-langfuse/","title":"Langfuse Integration (Shared)","text":"<p>The shared Langfuse module provides prompt management, trace handling, and webhook integration for LLM observability. It enables intelligent access to prompts and traces with smart dot-notation access patterns.</p>"},{"location":"guides/shared-langfuse/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   LangfusePromptManager                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 \u2022 create_or_update_prompt()  [\u2192 PromptClient]           \u2502   \u2502\n\u2502  \u2502 \u2022 get_prompt()               [\u2192 PromptClient + caching] \u2502   \u2502\n\u2502  \u2502 \u2022 get_compiled_prompt()      [\u2192 str|List with variables]\u2502   \u2502\n\u2502  \u2502 \u2022 get_template()             [\u2192 raw template]           \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u2022 mark_prompt_as_stale(name)                                    \u2502\n\u2502  \u2022 notify_prompt_change(name, payload)                           \u2502\n\u2502  \u2022 on_prompt_change(listener) [registers callback]               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2195\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Webhook Handler                              \u2502\n\u2502  POST /webhooks/langfuse                                        \u2502\n\u2502  \u2022 verify_signature()  [HMAC-SHA256]                            \u2502\n\u2502  \u2022 Handles: prompt.{created,updated,deleted}                    \u2502\n\u2502  \u2022 Marks stale \u2192 Notifies listeners \u2192 Posts Slack alerts        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2195\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Trace Access &amp; Analysis (trace.py)                 \u2502\n\u2502                                                                 \u2502\n\u2502  Trace(trace_data, prompt_patterns)                             \u2502\n\u2502   \u251c\u2500 trace.recommendation  \u2192 TraceStep                          \u2502\n\u2502   \u2502   \u251c\u2500 step.generation   \u2192 ObservationsView                   \u2502\n\u2502   \u2502   \u251c\u2500 step.context      \u2192 ObservationsView                   \u2502\n\u2502   \u2502   \u2514\u2500 step.variables    \u2192 Dict[extracted variables]          \u2502\n\u2502   \u251c\u2500 trace.id              \u2192 trace attribute                    \u2502\n\u2502   \u2514\u2500 trace.latency         \u2192 trace attribute                    \u2502\n\u2502                                                                 \u2502\n\u2502  SmartAccess Framework (dot-notation + fuzzy matching)          \u2502\n\u2502   \u251c\u2500 SmartDict (dict wrapper)                                   \u2502\n\u2502   \u251c\u2500 SmartObject (object wrapper)                               \u2502\n\u2502   \u2514\u2500 Recursive wrapping of nested structures                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/shared-langfuse/#configuration","title":"Configuration","text":""},{"location":"guides/shared-langfuse/#langfusesettings","title":"LangfuseSettings","text":"<pre><code>class LangfuseSettings(RepoSettingsBase):\n    # API Credentials\n    langfuse_public_key: str | None = None\n    langfuse_secret_key: str | None = None\n    langfuse_host: str | None = None\n\n    # Prompt Management\n    langfuse_default_label: str = \"production\"\n    langfuse_default_cache_ttl_seconds: int = 60\n\n    # Webhook Configuration\n    langfuse_webhook_secret: str | None = None\n    langfuse_webhook_notify_url: str | None = None\n\n    # Slack Alerts\n    langfuse_slack_channel_id: str | None = None\n    langfuse_slack_request_timeout_seconds: float = 10\n    langfuse_slack_retry_max_attempts: int = 3\n    langfuse_slack_retry_backoff_seconds: float = 0.5\n    langfuse_slack_retry_max_backoff_seconds: float = 4.0\n</code></pre> Setting Default Description <code>langfuse_default_label</code> <code>\"production\"</code> Default prompt label <code>langfuse_default_cache_ttl_seconds</code> <code>60</code> Local cache duration <code>langfuse_slack_request_timeout_seconds</code> <code>10</code> Slack API timeout <code>langfuse_slack_retry_max_attempts</code> <code>3</code> Retry count for Slack"},{"location":"guides/shared-langfuse/#langfusepromptmanager","title":"LangfusePromptManager","text":"<p>Central manager for all Langfuse prompt operations including CRUD, caching, invalidation, and change notifications.</p>"},{"location":"guides/shared-langfuse/#initialization","title":"Initialization","text":"<pre><code>from eval_workbench.shared.langfuse.prompt import LangfusePromptManager\n\nmanager = LangfusePromptManager(\n    public_key=\"pk-...\",      # Optional, uses env if not provided\n    secret_key=\"sk-...\",      # Optional, uses env if not provided\n    host=\"https://...\",       # Optional, uses env if not provided\n)\n</code></pre>"},{"location":"guides/shared-langfuse/#creating-and-updating-prompts","title":"Creating and Updating Prompts","text":"<pre><code>def create_or_update_prompt(\n    self,\n    name: str,\n    prompt_content: Union[str, List[Dict[str, str]]],\n    prompt_type: str = \"text\",           # \"text\" or \"chat\"\n    labels: Optional[Sequence[str]] = None,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; PromptClient\n</code></pre> <p>Example: <pre><code># Text prompt\nmanager.create_or_update_prompt(\n    name=\"recommendation\",\n    prompt_content=\"Analyze this case: {{ case_data }}\",\n    prompt_type=\"text\",\n    labels=[\"production\", \"v2\"],\n    config={\"model\": \"gpt-4\", \"temperature\": 0.7},\n)\n\n# Chat prompt\nmanager.create_or_update_prompt(\n    name=\"chat_assistant\",\n    prompt_content=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"{{ user_message }}\"},\n    ],\n    prompt_type=\"chat\",\n    labels=[\"production\"],\n)\n</code></pre></p>"},{"location":"guides/shared-langfuse/#fetching-prompts","title":"Fetching Prompts","text":"<pre><code>def get_prompt(\n    self,\n    name: str,\n    version: Optional[int] = None,\n    label: Optional[str] = None,\n    cache_ttl_seconds: Optional[int] = None,\n    retry_count: int = 0,\n    retry_backoff_seconds: float = 0.2,\n) -&gt; PromptClient\n</code></pre> <p>Behavior: - Defaults to \"production\" label if version/label omitted - Checks <code>_stale_prompts</code> and bypasses cache if flagged - Implements exponential backoff retry logic</p>"},{"location":"guides/shared-langfuse/#compiling-prompts-with-variables","title":"Compiling Prompts with Variables","text":"<pre><code>def get_compiled_prompt(\n    self,\n    name: str,\n    variables: Dict[str, Any],\n    *,\n    fallback: Optional[Union[str, List[Dict[str, Any]]]] = None,\n    strict: bool = True,\n    required_variables: Optional[Sequence[str]] = None,\n    strict_mode: Literal[\"template\", \"required\", \"none\"] = \"template\",\n    fallback_on_compile_error: bool = True,\n    render_with: Optional[Callable] = None,\n    **kwargs,\n) -&gt; Union[str, List[Dict]]\n</code></pre> <p>Strict Mode Options: - <code>\"template\"</code>: Check for unrendered placeholders after compilation - <code>\"required\"</code>: Verify required_variables are present before compilation - <code>\"none\"</code>: Skip all strict checks</p> <p>Example: <pre><code>compiled = manager.get_compiled_prompt(\n    \"recommendation\",\n    variables={\"case_id\": \"123\", \"assessment\": \"high-risk\"},\n    strict_mode=\"template\",\n    fallback=\"Default prompt text\",\n)\n</code></pre></p>"},{"location":"guides/shared-langfuse/#cache-invalidation","title":"Cache Invalidation","text":"<pre><code># Mark prompt for immediate refresh on next fetch\nmanager.mark_prompt_as_stale(\"recommendation\")\n\n# Promote specific version to labels\nmanager.promote_version(\"recommendation\", version=5, labels=[\"production\"])\n</code></pre>"},{"location":"guides/shared-langfuse/#change-notifications","title":"Change Notifications","text":"<pre><code># Register listener\ndef on_update(name: str, payload: Dict[str, Any] | None):\n    print(f\"Prompt {name} updated: {payload}\")\n\nmanager.on_prompt_change(on_update)\n\n# Manually trigger notifications\nmanager.notify_prompt_change(\"recommendation\", payload={\"version\": 6})\n</code></pre>"},{"location":"guides/shared-langfuse/#smartaccess-framework","title":"SmartAccess Framework","text":"<p>The SmartAccess system enables intelligent dot-notation access with fuzzy matching across trace data.</p>"},{"location":"guides/shared-langfuse/#smartaccess-base-class","title":"SmartAccess (Base Class)","text":"<pre><code>class SmartAccess:\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Dot-notation with 2-step fallback:\n        1. Exact match via _lookup(key)\n        2. Fuzzy match via _lookup_insensitive(key)\n        \"\"\"\n\n    def _wrap(self, val: Any) -&gt; Any:\n        \"\"\"Recursively wrap results:\n        - dict \u2192 SmartDict\n        - list \u2192 list of wrapped items\n        - object \u2192 SmartObject\n        \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#fuzzy-matching","title":"Fuzzy Matching","text":"<p>Keys are normalized for case/separator-insensitive matching:</p> <pre><code>def _normalize_key(key: str) -&gt; str:\n    return key.lower().replace(\"_\", \"\")\n\n# Examples:\n# \"product_type\" matches \"productType\"\n# \"CaseAssessment\" matches \"caseassessment\"\n</code></pre>"},{"location":"guides/shared-langfuse/#smartdict","title":"SmartDict","text":"<p>Dictionary wrapper with dot-notation and fuzzy key matching.</p> <pre><code>smart_dict = SmartDict({\"productType\": \"insurance\", \"caseId\": \"123\"})\nsmart_dict.product_type  # \"insurance\" (fuzzy match)\nsmart_dict.case_id       # \"123\" (fuzzy match)\nsmart_dict.to_dict()     # Returns raw dictionary\n</code></pre>"},{"location":"guides/shared-langfuse/#smartobject","title":"SmartObject","text":"<p>Generic object wrapper ensuring attributes return Smart wrappers.</p> <pre><code>smart_obj = SmartObject(some_object)\nsmart_obj.nested_attr.deep_value  # Recursive wrapping\n</code></pre>"},{"location":"guides/shared-langfuse/#trace-classes","title":"Trace Classes","text":""},{"location":"guides/shared-langfuse/#trace","title":"Trace","text":"<p>Main entry point for accessing trace data with smart dot-notation navigation.</p> <pre><code>class Trace(SmartAccess):\n    def __init__(\n        self,\n        trace_data: Any,\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    )\n</code></pre> <p>Access Patterns:</p> Access Returns Description <code>trace.{step_name}</code> <code>TraceStep</code> Named workflow step <code>trace.id</code> <code>str</code> Trace identifier <code>trace.latency</code> <code>int</code> Execution time <code>trace.name</code> <code>str</code> Trace name"},{"location":"guides/shared-langfuse/#tracestep","title":"TraceStep","text":"<p>Represents a named workflow step (e.g., \"recommendation\") with observations.</p> <pre><code>class TraceStep(SmartAccess):\n    name: str                              # Step name\n    observations: List[ObservationsView]   # Observations in step\n    prompt_patterns: PromptPatternsBase    # Pattern registry\n</code></pre> <p>Properties:</p> Access Returns Description <code>step.variables</code> <code>Dict[str, str]</code> Lazily extracted prompt variables <code>step.generation</code> <code>ObservationsView</code> GENERATION observation <code>step.context</code> <code>ObservationsView</code> SPAN observation (alias) <code>step.span</code> <code>ObservationsView</code> SPAN observation"},{"location":"guides/shared-langfuse/#tracecollection","title":"TraceCollection","text":"<p>Container for multiple traces with filtering and iteration.</p> <pre><code>class TraceCollection:\n    def __init__(\n        self,\n        data: List[Any],\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    )\n\n    def __getitem__(self, index: int) -&gt; Trace: ...\n    def __iter__(self) -&gt; Iterator[Trace]: ...\n    def __len__(self) -&gt; int: ...\n    def filter_by(self, **kwargs) -&gt; TraceCollection: ...\n</code></pre> <p>Example: <pre><code>traces = TraceCollection(trace_list, prompt_patterns=WorkflowPromptPatterns)\n\n# Filter\nfiltered = traces.filter_by(name=\"athena\", status=\"completed\")\n\n# Iterate\nfor trace in traces:\n    print(trace.id, trace.recommendation.variables)\n</code></pre></p>"},{"location":"guides/shared-langfuse/#promptpatternsbase","title":"PromptPatternsBase","text":"<p>Registry for regex extraction patterns. Enables custom pattern definitions per workflow.</p> <pre><code>class PromptPatternsBase:\n    @classmethod\n    def get_for(cls, step_name: str) -&gt; Dict[str, str]:\n        \"\"\"\n        Looks for method: _patterns_{step_name_lowercase}()\n        Returns: Dict[pattern_name: pattern_regex]\n        \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#creating-custom-patterns","title":"Creating Custom Patterns","text":"<pre><code>from eval_workbench.shared.langfuse.trace import PromptPatternsBase, create_extraction_pattern\n\nclass CustomPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_recommendation() -&gt; Dict[str, str]:\n        return {\n            \"caseAssessment\": create_extraction_pattern(\n                \"CASE ASSESSMENT\",\n                \"CONTEXT DATA\"\n            ),\n            \"contextData\": create_extraction_pattern(\n                \"CONTEXT DATA\",\n                \"(?:$)\"  # End of text\n            ),\n        }\n</code></pre>"},{"location":"guides/shared-langfuse/#helper-function","title":"Helper Function","text":"<pre><code>def create_extraction_pattern(start_text: str, end_pattern: str) -&gt; str:\n    \"\"\"\n    Creates regex: escaped(Start) \u2192 (Content) \u2192 End\n\n    Example:\n        create_extraction_pattern(\"CONTEXT:\", \"FLAGS:\")\n        \u2192 r\"CONTEXT:\\s*(.*?)\\s*(?:FLAGS:)\"\n    \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#webhook-handler","title":"Webhook Handler","text":"<p>The webhook handler processes Langfuse prompt update events.</p>"},{"location":"guides/shared-langfuse/#endpoint","title":"Endpoint","text":"<pre><code>POST /webhooks/langfuse\nHeader: X-Langfuse-Signature: t=&lt;timestamp&gt;,v1=&lt;signature&gt;\n</code></pre>"},{"location":"guides/shared-langfuse/#signature-verification","title":"Signature Verification","text":"<pre><code>def verify_signature(\n    payload: bytes,\n    signature_header: str | None,\n    secret: str\n) -&gt; None:\n    \"\"\"\n    Verifies HMAC-SHA256 signature.\n    Supports both timestamped and non-timestamped formats.\n    \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#event-handling","title":"Event Handling","text":"<p>Supported Events: - <code>prompt.created</code> - New prompt created - <code>prompt.updated</code> - Existing prompt modified - <code>prompt.deleted</code> - Prompt removed</p> <p>Processing Flow: 1. Verify HMAC signature 2. Parse JSON payload 3. Extract prompt name 4. Mark prompt as stale in manager 5. Notify registered listeners 6. Notify external services (if configured) 7. Post Slack alert (async, non-blocking)</p>"},{"location":"guides/shared-langfuse/#slack-alerts","title":"Slack Alerts","text":"<p>When configured, posts formatted alerts to Slack on prompt changes:</p> <pre><code># Requires environment variables:\n# LANGFUSE_SLACK_CHANNEL_ID - Target Slack channel\n# SLACK_ATHENA_TOKEN - Bot token for posting\n</code></pre>"},{"location":"guides/shared-langfuse/#utility-functions","title":"Utility Functions","text":""},{"location":"guides/shared-langfuse/#parse_chat_transcript","title":"parse_chat_transcript","text":"<p>Parses raw string chat transcript into structured messages.</p> <pre><code>from eval_workbench.shared.langfuse.utils import parse_chat_transcript\n\ntranscript = \"\"\"\nAthena: Hello, how can I help?\nUser: I have a question about my policy.\nAthena: I'd be happy to help with that.\n\"\"\"\n\nmessages = parse_chat_transcript(transcript, agent_name=\"Athena\")\n# [\n#     {\"role\": \"assistant\", \"content\": \"Hello, how can I help?\"},\n#     {\"role\": \"user\", \"content\": \"I have a question about my policy.\"},\n#     {\"role\": \"assistant\", \"content\": \"I'd be happy to help with that.\"},\n# ]\n</code></pre>"},{"location":"guides/shared-langfuse/#code-examples","title":"Code Examples","text":""},{"location":"guides/shared-langfuse/#prompt-management","title":"Prompt Management","text":"<pre><code>from eval_workbench.shared.langfuse.prompt import LangfusePromptManager\n\nmanager = LangfusePromptManager()\n\n# Fetch and compile with variables\ncompiled = manager.get_compiled_prompt(\n    \"recommendation\",\n    variables={\"case_id\": \"123\", \"assessment\": \"high-risk\"},\n    strict_mode=\"template\",\n    fallback=\"Default prompt text\",\n)\n\n# Register for updates\ndef on_update(name, payload):\n    print(f\"Prompt {name} updated: {payload}\")\n\nmanager.on_prompt_change(on_update)\n</code></pre>"},{"location":"guides/shared-langfuse/#trace-analysis","title":"Trace Analysis","text":"<pre><code>from eval_workbench.shared.langfuse.trace import Trace, TraceCollection\nfrom eval_workbench.implementations.athena.langfuse.prompt_patterns import WorkflowPromptPatterns\n\n# Single trace\ntrace = Trace(trace_data, prompt_patterns=WorkflowPromptPatterns)\nassessment = trace.recommendation.variables[\"caseAssessment\"]\ngeneration_output = trace.recommendation.generation.output\n\n# Multiple traces\ntraces = TraceCollection(trace_list, prompt_patterns=WorkflowPromptPatterns)\nfor trace in traces:\n    if trace.id in important_ids:\n        print(trace.recommendation.generation.output)\n</code></pre>"},{"location":"guides/shared-langfuse/#deep-trace-navigation","title":"Deep Trace Navigation","text":"<pre><code>trace = Trace(trace_data, prompt_patterns=WorkflowPromptPatterns)\n\n# Access step\nrec = trace.recommendation\n\n# Get generation observation\ngen = rec.generation\nprint(gen.input)   # Prompt input\nprint(gen.output)  # Model output\nprint(gen.model)   # Model used\n\n# Extract variables (lazy evaluation)\nvars = rec.variables\nprint(vars[\"caseAssessment\"])\nprint(vars[\"contextData\"])\n\n# Access trace metadata\nprint(trace.id)\nprint(trace.latency)\nprint(trace.name)\n</code></pre>"},{"location":"guides/shared-langfuse/#exports","title":"Exports","text":"<pre><code>from eval_workbench.shared.langfuse.prompt import (\n    LangfuseSettings,\n    get_langfuse_settings,\n    LangfusePromptManager,\n)\n\nfrom eval_workbench.shared.langfuse.trace import (\n    # Enums\n    ModelUsageUnit,\n    ObservationLevel,\n\n    # Dataclasses\n    Usage,\n\n    # SmartAccess Framework\n    SmartAccess,\n    SmartDict,\n    SmartObject,\n\n    # Trace Views\n    TraceView,\n    ObservationsView,\n\n    # Pattern Extraction\n    create_extraction_pattern,\n    PromptPatternsBase,\n\n    # Main Classes\n    TraceStep,\n    Trace,\n    TraceCollection,\n)\n\nfrom eval_workbench.shared.langfuse.webhook import (\n    verify_signature,\n    langfuse_webhook,\n)\n\nfrom eval_workbench.shared.langfuse.utils import (\n    parse_chat_transcript,\n)\n</code></pre>"},{"location":"guides/slack/","title":"Slack Integration","text":"<p>The Slack integration module provides a complete, production-ready interface for Slack interactions with async support, retry logic, message parsing, formatting validation, and subscription management.</p>"},{"location":"guides/slack/#architecture","title":"Architecture","text":"<pre><code>SlackConfig (Token Management)\n    \u2514\u2500\u2500 get_token(agent_id) -&gt; str\n\nSlackHttpClient (Low-level Async)\n    \u251c\u2500\u2500 request() -&gt; with retry logic\n    \u2514\u2500\u2500 _get_session()\n\nSlackService (High-level API)\n    \u251c\u2500\u2500 post_message()\n    \u251c\u2500\u2500 update_message()\n    \u251c\u2500\u2500 post_threaded_message()\n    \u251c\u2500\u2500 get_thread_replies()\n    \u251c\u2500\u2500 get_channel_history()\n    \u251c\u2500\u2500 open_modal()\n    \u251c\u2500\u2500 publish_app_home()\n    \u251c\u2500\u2500 download_file()\n    \u251c\u2500\u2500 post_to_subscribed_channels()\n    \u2514\u2500\u2500 post_to_all_threads()\n\nSlackScraper (Message Parsing)\n    \u251c\u2500\u2500 extract_text_from_blocks()\n    \u251c\u2500\u2500 extract_text_from_attachments()\n    \u251c\u2500\u2500 simplify_message()\n    \u251c\u2500\u2500 simplify_message_extended()\n    \u2514\u2500\u2500 extract_thread_metadata()\n\nSlackBlockBuilder (Message Formatting)\n    \u251c\u2500\u2500 format_prompt_change_alert()\n    \u251c\u2500\u2500 format_chat_response()\n    \u2514\u2500\u2500 create_error_message()\n</code></pre>"},{"location":"guides/slack/#slackconfig-token-management","title":"SlackConfig (Token Management)","text":"<p>Centralized management of Slack bot tokens for multiple agents.</p> <pre><code>class SlackConfig:\n    ATHENA_TOKEN = os.getenv(\"SLACK_ATHENA_TOKEN\")\n    AIMEE_TOKEN = os.getenv(\"SLACK_AIMEE_TOKEN\")\n    CANARY_TOKEN = os.getenv(\"SLACK_CANARY_TOKEN\")\n    PROMETHEUS_TOKEN = os.getenv(\"SLACK_PROMETHEUS_TOKEN\")\n    QUILL_TOKEN = os.getenv(\"SLACK_QUILL_TOKEN\")\n\n    @classmethod\n    def get_token(\n        cls,\n        agent_id: Optional[str] = None,\n        override_token: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"\n        Get token by priority:\n        1. override_token if provided\n        2. agent_map[agent_id] if agent_id provided\n        3. ATHENA_TOKEN as fallback\n        \"\"\"\n</code></pre> <p>Supported Agents: <code>\"athena\"</code>, <code>\"aimee\"</code>, <code>\"canary\"</code>, <code>\"quill\"</code>, <code>\"prometheus\"</code></p>"},{"location":"guides/slack/#slackhttpclient-async-client-with-retry","title":"SlackHttpClient (Async Client with Retry)","text":"<p>Low-level async HTTP client with exponential backoff retry strategy.</p> <pre><code>class SlackHttpClient:\n    def __init__(\n        self,\n        *,\n        timeout_seconds: float = 10,\n        max_attempts: int = 3,\n        backoff_seconds: float = 0.5,\n        max_backoff_seconds: float = 4.0,\n        jitter_seconds: float = 0.1,\n    ): ...\n\n    async def request(\n        self,\n        *,\n        method: str,\n        url: str,\n        token: str,\n        json_data: Any = None,\n        data: Any = None,\n        headers: Dict | None = None,\n        params: Dict | None = None,\n    ) -&gt; Dict[str, Any]: ...\n</code></pre> <p>Retry Strategy: 1. Detects rate limiting (HTTP 429 or \"ratelimited\" error) 2. Detects server errors (HTTP 5xx) 3. Exponential backoff: <code>delay = backoff_seconds * (2 ^ (attempt - 1))</code> 4. Adds random jitter to prevent thundering herd 5. Respects Slack's <code>Retry-After</code> header if provided</p>"},{"location":"guides/slack/#slackscraper-message-parsing","title":"SlackScraper (Message Parsing)","text":"<p>Extract and simplify raw Slack message data into readable formats.</p>"},{"location":"guides/slack/#extract_text_from_blocks","title":"extract_text_from_blocks()","text":"<pre><code>@staticmethod\ndef extract_text_from_blocks(blocks: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"\n    Extracts text from Slack Block Kit elements:\n    - section blocks: text.text\n    - rich_text blocks: rich_text_section elements\n    - context blocks: element text field\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#extract_text_from_attachments","title":"extract_text_from_attachments()","text":"<pre><code>@staticmethod\ndef extract_text_from_attachments(attachments: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"Handles legacy Slack attachments.\"\"\"\n</code></pre>"},{"location":"guides/slack/#simplify_message","title":"simplify_message()","text":"<pre><code>@staticmethod\ndef simplify_message(raw: Dict[str, Any]) -&gt; SimplifiedMessage:\n    \"\"\"\n    Returns SimplifiedMessage with:\n    - ts: Message timestamp\n    - sender: Human-readable sender name\n    - is_bot: Boolean flag\n    - content: Concatenated, deduplicated, normalized text\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#simplify_message_extended","title":"simplify_message_extended()","text":"<pre><code>@staticmethod\ndef simplify_message_extended(raw: Dict[str, Any]) -&gt; ExtendedSimplifiedMessage:\n    \"\"\"\n    Returns ExtendedSimplifiedMessage with additional fields:\n    - timestamp_utc: Parsed datetime\n    - user_id: Slack user ID\n    - reply_count: Number of thread replies\n    - message_url: Permalink to message\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#extract_thread_metadata","title":"extract_thread_metadata()","text":"<pre><code>@staticmethod\ndef extract_thread_metadata(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns:\n    - thread_created_at: Earliest message timestamp\n    - thread_last_activity_at: Latest message timestamp\n    - human_participants: List of non-bot user IDs\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#slackservice-readwrite-operations","title":"SlackService (Read/Write Operations)","text":"<p>High-level API for Slack interactions.</p> <pre><code>class SlackService:\n    def __init__(\n        self,\n        storage: SubscriptionStorage = None,  # Default: InMemorySubscriptionStorage\n        client: SlackHttpClient = None         # Default: shared global client\n    ): ...\n</code></pre>"},{"location":"guides/slack/#write-operations","title":"Write Operations","text":""},{"location":"guides/slack/#post_message","title":"post_message()","text":"<pre><code>async def post_message(\n    channel: str,\n    options: PostMessageOptions,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"\n    Posts a message to a channel.\n    - Validates token and message content\n    - Applies payload limits (text: 40000 chars, block text: 3000 chars)\n    - Posts to chat.postMessage endpoint\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#update_message","title":"update_message()","text":"<pre><code>async def update_message(\n    channel: str,\n    ts: str,\n    options: PostMessageOptions,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"Updates an existing message by timestamp.\"\"\"\n</code></pre>"},{"location":"guides/slack/#post_threaded_message","title":"post_threaded_message()","text":"<pre><code>async def post_threaded_message(\n    channel: str,\n    thread_ts: str,\n    options: PostMessageOptions,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"\n    Posts a reply in a thread.\n    - Validates thread exists\n    - Sets thread_ts in options\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#readscrape-operations","title":"Read/Scrape Operations","text":""},{"location":"guides/slack/#get_thread_replies","title":"get_thread_replies()","text":"<pre><code>async def get_thread_replies(\n    channel: str,\n    thread_ts: str,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"\n    Retrieves all replies in a thread with automatic pagination.\n    - Endpoint: conversations.replies\n    - Batch size: 200\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#get_channel_history","title":"get_channel_history()","text":"<pre><code>async def get_channel_history(\n    channel: str,\n    limit: int = 100,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"Fetches channel message history with pagination.\"\"\"\n</code></pre>"},{"location":"guides/slack/#interaction-operations","title":"Interaction Operations","text":""},{"location":"guides/slack/#open_modal","title":"open_modal()","text":"<pre><code>async def open_modal(\n    trigger_id: str,\n    view: Dict,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"Opens a modal view dialog.\"\"\"\n</code></pre>"},{"location":"guides/slack/#publish_app_home","title":"publish_app_home()","text":"<pre><code>async def publish_app_home(\n    agent_id: str,\n    user_id: str\n) -&gt; None:\n    \"\"\"Publishes the App Home tab view for a user.\"\"\"\n</code></pre>"},{"location":"guides/slack/#download_file","title":"download_file()","text":"<pre><code>async def download_file(\n    file: SlackFile,\n    agent_id: Optional[str] = None\n) -&gt; FileDownloadResponse:\n    \"\"\"\n    Downloads a file from Slack with redirect handling.\n    - Handles up to 5 redirects to preserve auth headers\n    - Validates content type\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#multi-channel-broadcasting","title":"Multi-Channel Broadcasting","text":""},{"location":"guides/slack/#post_to_subscribed_channels","title":"post_to_subscribed_channels()","text":"<pre><code>async def post_to_subscribed_channels(\n    agent_id: str,\n    event_type: str,\n    event_data: Dict[str, str],\n    message_options: PostMessageOptions\n) -&gt; MultiChannelPostResult:\n    \"\"\"Posts to all subscribed channels matching event type and filters.\"\"\"\n</code></pre>"},{"location":"guides/slack/#post_to_all_threads","title":"post_to_all_threads()","text":"<pre><code>async def post_to_all_threads(\n    threads: List[SlackThread],\n    message_options: PostMessageOptions,\n    agent_id: str\n) -&gt; None:\n    \"\"\"Broadcasts reply to multiple tracked threads.\"\"\"\n</code></pre>"},{"location":"guides/slack/#slackblockbuilder-block-kit-formatting","title":"SlackBlockBuilder (Block Kit Formatting)","text":"<p>Utility class for constructing Slack Block Kit formatted messages.</p>"},{"location":"guides/slack/#format_prompt_change_alert","title":"format_prompt_change_alert()","text":"<pre><code>@staticmethod\ndef format_prompt_change_alert(\n    prompt_name: str,\n    event_type: str,\n    prompt_version: Optional[Union[int, str]] = None,\n    prompt_id: Optional[str] = None,\n    prompt_url: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Creates notification for Langfuse prompt updates.\"\"\"\n</code></pre>"},{"location":"guides/slack/#format_chat_response","title":"format_chat_response()","text":"<pre><code>@staticmethod\ndef format_chat_response(\n    message: str,\n    citations: List[Dict] = None,\n    is_learning_worthy: bool = False,\n    is_feature_request: bool = False\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Creates AI chat response message with:\n    - Main message block\n    - Optional citations block\n    - Optional action buttons (Save as Learning, Create Feature Request)\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#create_error_message","title":"create_error_message()","text":"<pre><code>@staticmethod\ndef create_error_message(error: str) -&gt; Dict[str, Any]:\n    \"\"\"Creates error alert message with section block.\"\"\"\n</code></pre>"},{"location":"guides/slack/#typeddicts-and-dataclasses","title":"TypedDicts and Dataclasses","text":""},{"location":"guides/slack/#slackresponse","title":"SlackResponse","text":"<pre><code>class SlackResponse(TypedDict, total=False):\n    success: bool\n    ts: Optional[str]                        # Message timestamp\n    channel: Optional[str]                   # Channel ID\n    error: Optional[str]                     # Error message\n    status: Optional[int]                    # HTTP status code\n    messages: Optional[List[Dict[str, Any]]] # Array of message objects\n    user: Optional[Dict[str, Any]]\n</code></pre>"},{"location":"guides/slack/#simplifiedmessage","title":"SimplifiedMessage","text":"<pre><code>class SimplifiedMessage(TypedDict):\n    ts: str           # Message timestamp\n    sender: str       # Sender name/bot name\n    is_bot: bool      # Is from a bot\n    content: str      # Consolidated text content\n</code></pre>"},{"location":"guides/slack/#extendedsimplifiedmessage","title":"ExtendedSimplifiedMessage","text":"<pre><code>class ExtendedSimplifiedMessage(TypedDict):\n    ts: str\n    timestamp_utc: Optional[datetime]   # Parsed UTC datetime\n    sender: str\n    user_id: Optional[str]              # Slack user ID\n    is_bot: bool\n    content: str\n    reply_count: int                    # Number of thread replies\n    message_url: Optional[str]          # Message permalink\n</code></pre>"},{"location":"guides/slack/#postmessageoptions","title":"PostMessageOptions","text":"<pre><code>class PostMessageOptions(TypedDict, total=False):\n    text: Optional[str]\n    blocks: Optional[List[Dict[str, Any]]]\n    attachments: Optional[List[Dict[str, Any]]]\n    thread_ts: Optional[str]\n    reply_broadcast: Optional[bool]\n    unfurl_links: Optional[bool]\n    unfurl_media: Optional[bool]\n    slackToken: Optional[str]           # Override token\n</code></pre>"},{"location":"guides/slack/#slackfile","title":"SlackFile","text":"<pre><code>@dataclass\nclass SlackFile:\n    id: str\n    name: str\n    mimetype: str\n    size: int\n    url_private: str\n    url_private_download: Optional[str] = None\n</code></pre>"},{"location":"guides/slack/#filedownloadresponse","title":"FileDownloadResponse","text":"<pre><code>@dataclass\nclass FileDownloadResponse:\n    success: bool\n    buffer: Optional[bytes] = None\n    filename: Optional[str] = None\n    mimetype: Optional[str] = None\n    error: Optional[str] = None\n</code></pre>"},{"location":"guides/slack/#slacksubscription","title":"SlackSubscription","text":"<pre><code>@dataclass\nclass SlackSubscription:\n    id: str\n    agent_id: str                 # Agent receiving events\n    event_type: str               # Type of event\n    channel_id: str               # Where to post\n    filters: Dict[str, str]       # Event filter conditions\n    created_by_slack_user_id: str\n    created_by_username: Optional[str]\n    active: bool\n    created_at: str\n    updated_at: str\n</code></pre>"},{"location":"guides/slack/#slackthread","title":"SlackThread","text":"<pre><code>@dataclass\nclass SlackThread:\n    channel_id: str\n    thread_ts: str       # Thread message timestamp\n    posted_at: str       # ISO format timestamp\n</code></pre>"},{"location":"guides/slack/#multichannelpostresult","title":"MultiChannelPostResult","text":"<pre><code>@dataclass\nclass MultiChannelPostResult:\n    threads: List[SlackThread]\n    errors: List[Dict[str, str]]\n</code></pre>"},{"location":"guides/slack/#slackformattingcompliance-metric","title":"SlackFormattingCompliance Metric","text":"<p>Located at <code>src/shared/metrics/slack_compliance.py</code>, this metric validates AI outputs comply with Slack formatting rules.</p> <pre><code>class SlackFormattingCompliance(BaseMetric):\n    key = \"slack_compliance\"\n    required_fields = [\"actual_output\"]\n    default_threshold = 1.0\n</code></pre> <p>Validation Checks: 1. Double Asterisks: Detects <code>**text**</code> (should be <code>*text*</code> in Slack) 2. Headers: Detects <code># Header</code> markdown (forbidden in Slack) 3. Unwrapped Data: Detects <code>$500</code> or <code>50%</code> not in backticks</p> <p>Scoring: Deducts 0.25 per issue type found, minimum 0.0</p>"},{"location":"guides/slack/#formatting-rules-and-compliance","title":"Formatting Rules and Compliance","text":""},{"location":"guides/slack/#general-slack-formatting-rules","title":"General Slack Formatting Rules","text":"<pre><code>SLACK_FORMATTING_RULES = \"\"\"\n- NO using # for headers (Slack doesn't render them)\n- Use *single asterisks* for bold (NOT **double**)\n- Use backticks for variables, numbers, and technical terms\n\"\"\"\n</code></pre>"},{"location":"guides/slack/#chat-specific-formatting-rules","title":"Chat-Specific Formatting Rules","text":"<pre><code>SLACK_CHAT_FORMATTING_RULES = \"\"\"\n- Same as above, plus:\n- Reserve backticks for citations and technical terms only\n\"\"\"\n</code></pre>"},{"location":"guides/slack/#valid-event-types","title":"Valid Event Types","text":"<pre><code>VALID_EVENT_TYPES = {\n    \"athena\": [\"referrals\"],\n    \"quill\": [\"binds\"],\n    \"canary\": [\n        \"evaluations\", \"openprs\", \"weekly-summary\",\n        \"release-train\", \"releases\", \"new-users\",\n        \"new-repo\", \"new-swallow-projects\"\n    ],\n    \"aimee\": [\"inquiries\"],\n    \"prometheus\": [\"dust-audit\"],\n}\n</code></pre>"},{"location":"guides/slack/#code-examples","title":"Code Examples","text":""},{"location":"guides/slack/#basic-message-posting","title":"Basic Message Posting","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, PostMessageOptions\n\nslack = SlackService()\n\n# Post simple message\nresult = await slack.post_message(\n    channel=\"C1234567890\",\n    options=PostMessageOptions(\n        text=\"Hello from Athena!\",\n    ),\n    agent_id=\"athena\",\n)\n\nif result[\"success\"]:\n    print(f\"Message posted: {result['ts']}\")\nelse:\n    print(f\"Error: {result['error']}\")\n</code></pre>"},{"location":"guides/slack/#posting-with-block-kit","title":"Posting with Block Kit","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, SlackBlockBuilder\n\nslack = SlackService()\n\n# Create formatted response\noptions = SlackBlockBuilder.format_chat_response(\n    message=\"Here's your analysis...\",\n    citations=[\n        {\"url\": \"https://example.com\", \"source\": \"Example Doc\"},\n    ],\n    is_learning_worthy=True,\n)\n\nresult = await slack.post_message(\n    channel=\"C1234567890\",\n    options=options,\n    agent_id=\"athena\",\n)\n</code></pre>"},{"location":"guides/slack/#reading-thread-replies","title":"Reading Thread Replies","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, SlackScraper\n\nslack = SlackService()\n\n# Get thread replies\nresult = await slack.get_thread_replies(\n    channel=\"C1234567890\",\n    thread_ts=\"1700000000.000000\",\n    agent_id=\"athena\",\n)\n\nif result[\"success\"]:\n    messages = result[\"messages\"]\n\n    # Simplify messages for processing\n    simplified = [\n        SlackScraper.simplify_message_extended(msg)\n        for msg in messages\n    ]\n\n    # Extract thread metadata\n    metadata = SlackScraper.extract_thread_metadata(messages)\n    print(f\"Thread created: {metadata['thread_created_at']}\")\n    print(f\"Human participants: {metadata['human_participants']}\")\n</code></pre>"},{"location":"guides/slack/#multi-channel-broadcasting_1","title":"Multi-Channel Broadcasting","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, PostMessageOptions\n\nslack = SlackService()\n\nresult = await slack.post_to_subscribed_channels(\n    agent_id=\"canary\",\n    event_type=\"releases\",\n    event_data={\"repo\": \"swallow\"},\n    message_options=PostMessageOptions(\n        text=\"New release deployed!\",\n    ),\n)\n\nprint(f\"Posted to {len(result.threads)} channels\")\nprint(f\"Errors: {len(result.errors)}\")\n</code></pre>"},{"location":"guides/slack/#downloading-files","title":"Downloading Files","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, SlackFile\n\nslack = SlackService()\n\nfile = SlackFile(\n    id=\"F1234567890\",\n    name=\"document.pdf\",\n    mimetype=\"application/pdf\",\n    size=1024000,\n    url_private=\"https://files.slack.com/...\",\n)\n\nresponse = await slack.download_file(file, agent_id=\"athena\")\n\nif response.success:\n    with open(response.filename, \"wb\") as f:\n        f.write(response.buffer)\nelse:\n    print(f\"Download failed: {response.error}\")\n</code></pre>"},{"location":"guides/slack/#exports","title":"Exports","text":"<pre><code>from eval_workbench.shared.slack.service import (\n    # Configuration\n    SlackConfig,\n    SLACK_FORMATTING_RULES,\n    SLACK_CHAT_FORMATTING_RULES,\n    VALID_EVENT_TYPES,\n    AGENT_INFO,\n\n    # TypedDicts\n    SlackResponse,\n    SimplifiedMessage,\n    ExtendedSimplifiedMessage,\n    PostMessageOptions,\n\n    # Dataclasses\n    SlackFile,\n    FileDownloadResponse,\n    SlackSubscription,\n    SlackThread,\n    MultiChannelPostResult,\n\n    # HTTP Client\n    SlackHttpClient,\n    get_shared_slack_client,\n\n    # Storage\n    SubscriptionStorage,\n    InMemorySubscriptionStorage,\n\n    # Main Classes\n    SlackScraper,\n    SlackService,\n    SlackBlockBuilder,\n)\n\nfrom eval_workbench.shared.metrics.slack_compliance import (\n    SlackFormattingCompliance,\n)\n</code></pre>"},{"location":"guides/underwriting_composite_submetric_prompts/","title":"Underwriting Composite Sub-Metric Prompts","text":"<p>This document lists the prompt instructions used by the individual analyzers run inside <code>UnderwritingCompositeEvaluator</code>.</p> <p>Composite orchestrator source: - <code>src/eval_workbench/implementations/athena/metrics/slack/composite.py</code></p> <p>Sub-analyzers used by the composite: - Objective: <code>SlackObjectiveAnalyzer</code> (internal <code>_ObjectiveLLMAnalyzer</code> instruction) - Subjective: <code>SlackSubjectiveAnalyzer</code> - Feedback Attribution: <code>SlackFeedbackAttributionAnalyzer</code> - Product: <code>SlackProductAnalyzer</code></p>"},{"location":"guides/underwriting_composite_submetric_prompts/#1-objective-analyzer-prompt","title":"1) Objective Analyzer Prompt","text":"<p>Source: - <code>src/eval_workbench/shared/metrics/slack/objective.py</code> - class: <code>_ObjectiveLLMAnalyzer</code></p> <pre><code>You are an expert Underwriting Auditor for an AI Assistant ({bot_name}).\n\n## YOUR TASK\nAnalyze the conversation objectively to classify **Escalation**, **Intervention**, and **Resolution**.\n\nFocus specifically on differentiating between *helping* the bot (context) vs *correcting* the bot (data/logic errors).\n\n{truncation_notice}\n\n---\n\n## INTERVENTION CATEGORIES\n*Did a human participate to help/correct the bot?*\n\n**correction_factual** - Fixing Bad Data:\n- User corrects specific numbers or facts from \"Magic Dust\" or other sources.\n- Examples: \"Sq ft is 100k, not 10k\", \"Built in 2026, not 1978\".\n\n**correction_classification** - Fixing Class Codes:\n- User changes the Industry or Class Code because the bot got it wrong.\n- Examples: \"It's a Church, not Retail\", \"Change from Office to Medical\".\n\n**system_workaround** - Tooling Failure:\n- User is blocked by a software error or UI bug.\n- Examples: \"Failed to decline\", \"AAL is broken\", \"Force decline manually\".\n\n**risk_appetite** - Judgment Call:\n- Bot followed rules, but human made a judgment call to override.\n- Examples: \"Approved based on inspection\", \"Declined due to crime score\".\n\n**missing_context** - Providing Info:\n- User provides info the bot didn't have (not a correction, just an addition).\n- Examples: \"Agent confirmed sprinklers are present\".\n\n---\n\n## RESOLUTION STATUS\n- `approved`: Clear approval decision made.\n- `declined`: Clear decline decision made.\n- `blocked`: Waiting on external factor or system fix.\n- `needs_info`: Explicitly waiting for agent/insured.\n- `stalemate`: No progress being made.\n\n---\n\n## ANALYSIS RULES\n1. **Data vs. Opinion**: If the user says \"The data is wrong\", it is `correction_factual`. If they say \"I don't like this risk\", it is `risk_appetite`.\n2. **System vs. Model**: If the user complains about \"SFX\", \"Socotra\", or \"Swallow\" errors, it is `system_workaround`.\n3. **Escalation**: Only mark `is_escalated` if the conversation is explicitly handed off to another human/team.\n\n---\n\n## OUTPUT FORMAT\nProvide your reasoning trace first, identifying the specific turn where intervention occurred.\n</code></pre>"},{"location":"guides/underwriting_composite_submetric_prompts/#2-subjective-analyzer-prompt","title":"2) Subjective Analyzer Prompt","text":"<p>Source: - <code>src/eval_workbench/shared/metrics/slack/subjective.py</code> - class: <code>SlackSubjectiveAnalyzer</code></p> <pre><code>You are an Underwriting Experience Analyst for an AI Assistant.\n\n## YOUR TASK\nAnalyze the conversation for subjective qualities using the context provided in the input.\n\nCrucially, distinguish between frustration with **Athena (the Bot)** vs frustration with **The Platform (Socotra/SFX/Magic Dust)**.\n\n---\n\n## FRUSTRATION CAUSES\n\n**tooling_friction** - Platform Bugs/UX:\n- User is annoyed by the software, not the bot's logic.\n- Evidence: \"Failed to decline\", \"Button not working\", \"Can't override in SFX\".\n\n**data_quality** - Bad Inputs:\n- User is annoyed that pre-filled data is wrong.\n- Evidence: \"Magic Dust is wrong again\", \"Why does it say 1978?\".\n\n**rule_rigidity** - Policy Blocks:\n- User is annoyed by a hard decline rule they disagree with.\n- Evidence: \"This shouldn't be blocked\", \"Why is coastal coverage restricted?\".\n\n**ai_error** - Bot Logic:\n- The bot misunderstood the prompt or gave a bad answer.\n- Evidence: \"You missed the payroll cap\", \"That's not what I asked\".\n\n---\n\n## ACCEPTANCE &amp; OVERRIDE ANALYSIS\n\n**Acceptance Status** (for recommendations):\n- `accepted`: User explicitly agreed without changes\n- `accepted_with_discussion`: Agreed after discussion/clarification\n- `pending`: No clear decision made\n- `rejected`: User explicitly declined\n- `modified`: User accepted with modifications\n\n**Override** (human changed bot's recommendation):\n- `no_override`: Final decision matches recommendation\n- `full_override`: Decision completely opposite to recommendation\n- `partial_override`: Decision partially differs from recommendation\n- `pending_override`: Override being discussed but not finalized\n\n**Override Reason Categories**:\n- `additional_info`: Human had information bot didn't have\n- `risk_assessment`: Different risk judgment\n- `policy_exception`: Applying policy exception\n- `class_code_issue`: Class code disagreement\n- `rate_issue`: Rate/pricing disagreement\n- `experience_judgment`: Professional experience override\n\n---\n\n## OUTPUT FORMAT\nProvide your reasoning trace first, walking through the conversation chronologically.\nNote specific turns and quotes that inform your assessments.\n</code></pre>"},{"location":"guides/underwriting_composite_submetric_prompts/#3-feedback-attribution-prompt","title":"3) Feedback Attribution Prompt","text":"<p>Source: - <code>src/eval_workbench/shared/metrics/slack/feedback.py</code> - class: <code>SlackFeedbackAttributionAnalyzer</code></p> <pre><code>You are a Lead Underwriting Auditor diagnosing failures in an AI Assistant (Athena).\n\n## YOUR TASK\nThe user (Underwriter) had friction with the AI. Identify the ROOT CAUSE of the failure based on the transcript.\n\nDistinguish between the **AI Model**, the **Data Source (Magic Dust)**, and the **Platform (Socotra/Swallow)**.\n\n---\n\n## FAILURE CATEGORIES\n\n**classification_failure** - Wrong Business Class:\n- AI or Data selected the wrong Class Code/NAICS.\n- Misunderstanding the business operations (e.g., calling a \"Junk Hauler\" an \"Exterior Cleaner\").\n- Evidence: \"This is misclassified\", \"Should be [Code X]\", \"Wrong industry group\".\n\n**data_integrity_failure** - Bad Third-Party Data (Magic Dust):\n- The AI's logic was fine, but the *input data* was wrong.\n- Issues with: Year Built, Square Footage, Employee Count, Payroll figures from 'Magic Dust'.\n- Evidence: \"Magic Dust shows 1978 but it's new construction\", \"Sq ft is actually 100k\".\n\n**rule_engine_failure** - Missed Hard Rule / Eligibility:\n- The AI approved a risk that violated a hard eligibility rule.\n- Issues with: Payroll caps ($300k), Coastal distance, Roof age, TIV limits.\n- **Territory Issues**: Quoting in states where carrier is not live (CA, NY, FL).\n- Evidence: \"Payroll &gt; $300k s/b ineligible\", \"Tier 1 county restriction\", \"We are not live in CA\".\n\n**system_tooling_failure** - Backend/UI/Sync Bugs:\n- The Underwriter agrees with the decision but *cannot execute it* due to the tool.\n- **Race Conditions**: Bot reports status before backend finishes calculation.\n- **Service Failures**: External calls (AAL/Aon, Magic Dust) failing to return data.\n- Evidence: \"Failed to decline\", \"Athena posted before quote completed\", \"AAL is broken again\".\n\n**chat_interface** - UX/Hallucination:\n- AI was confusing, verbose, or hallucinated a capability it doesn't have.\n- Evidence: \"You said X but meant Y\", \"Confusing response\".\n\n---\n\n## ATTRIBUTION RULES\n1. **Blame the Data, not the Bot**: If the user says \"Magic Dust says X but reality is Y\", this is `data_integrity_failure`.\n2. **Blame the Rule, not the Bot**: If the user says \"This should have auto-declined\" or \"Not live in this state\", this is `rule_engine_failure`.\n3. **Blame the System**: If the user mentions \"Failed to decline\", \"AAL broken\", or timing/sync issues, this is `system_tooling_failure`.\n\n## OUTPUT FORMAT\nIdentify the most likely failed step and provide direct quote evidence.\n</code></pre>"},{"location":"guides/underwriting_composite_submetric_prompts/#4-product-analyzer-prompt","title":"4) Product Analyzer Prompt","text":"<p>Source: - <code>src/eval_workbench/shared/metrics/slack/product.py</code> - class: <code>SlackProductAnalyzer</code></p> <pre><code>You are an Insurance Platform Product Manager extracting insights from Underwriter conversations.\n\n## YOUR TASK\nAnalyze the conversation to identify improvements for the **AI Assistant (Athena)** and the **Underwriting Platform (SFX/Socotra)**.\n\n---\n\n## SIGNAL CATEGORIES\n\n**workflow** - Platform Friction:\n- User is blocked from performing an action in the UI.\n- Issues with buttons, status transitions, or \"Failed to...\" errors.\n- Example: \"I can't decline an auto-approved quote without resetting it.\"\n\n**rules** - Underwriting Logic Configuration:\n- Feedback on the business rules, referrals, or questions asked.\n- Requests to change *when* a referral triggers.\n- Example: \"Stop asking contractors if they are home-based.\"\n\n**guardrails** - Safety/Validation:\n- Requests to prevent agents from submitting invalid risks upfront.\n- Example: \"Block agents from quoting building coverage in coastal zones.\"\n\n**accuracy** - Data Quality:\n- Feedback on the correctness of 3rd party data (Magic Dust).\n- Example: \"Magic Dust square footage is always off.\"\n\n**ux** - Interface/Clarity:\n- Confusing messages or lack of visibility.\n- Example: \"Where can I see the Tier 1 county status?\"\n\n---\n\n## PRIORITY LEVELS\n- `high`: User cannot complete task, or requests a \"Bug Fix\" / \"Ticket\".\n- `medium`: User suggests an improvement (\"It would be nice if...\").\n- `low`: General observation or complaint without specific suggestion.\n\n---\n\n## OUTPUT FORMAT\nExtract clear, actionable learnings. If a user explicitly asks for a ticket/fix, mark `has_actionable_feedback` as True.\n</code></pre>"},{"location":"guides/underwriting_composite_submetric_prompts/#notes","title":"Notes","text":"<ul> <li><code>UnderwritingCompositeEvaluator</code> itself is an orchestrator and does not define a separate LLM prompt.</li> <li>These are the current instruction strings in code at time of generation.</li> </ul>"},{"location":"metric-registry/","title":"Metric Registry","text":"Evaluation metrics organized by scope and implementation Shared + Athena Axion-style registry <p>Metrics are grouped by Shared (cross-implementation) and Athena (implementation-specific).</p> Shared Metrics <p>Cross-implementation metrics</p> <code>conversation</code> <code>actual_output</code> Athena Metrics <p>Implementation-specific underwriting evaluation</p> <code>actual_output</code> <code>expected_output</code>"},{"location":"metric-registry/athena/","title":"Athena Metrics","text":"Athena underwriting metrics and analysis 9 Metrics Implementation-specific <p>Athena metrics cover recommendation quality, underwriting completeness, and citation accuracy.</p> Recommendation Metrics <p>Citations, decision quality, referral reasons, underwriting checks</p> <code>actual_output</code> <code>expected_output</code>"},{"location":"metric-registry/athena/recommendation/","title":"Recommendation Metrics","text":"Athena underwriting recommendation evaluation 7 Metrics Underwriting <p>This module provides metrics for evaluating Athena's underwriting recommendations. These metrics cover citation verification, decision quality assessment, content completeness, factual faithfulness, rule compliance, and referral reason analysis.</p>"},{"location":"metric-registry/athena/recommendation/#recommendation-metrics_1","title":"Recommendation Metrics","text":"Citation Accuracy <p>Validate numeric citations against reference data</p> <code>actual_output</code> <code>context</code> Citation Fidelity <p>Verify bracket-path citations resolve to valid JSON values</p> <code>actual_output</code> <code>context</code> Decision Quality <p>Evaluate AI decision accuracy and reasoning alignment</p> <code>actual_output</code> <code>expected_output</code> Refer Reason <p>Extract and categorize reasons for referral/decline outcomes</p> <code>actual_output</code> Underwriting Completeness <p>Ensure recommendations contain all required components</p> <code>actual_output</code> Underwriting Faithfulness <p>Verify factual claims are supported by source data</p> <code>actual_output</code> <code>context</code> Underwriting Rules <p>Track referral triggers and validate outcome consistency</p> <code>actual_output</code> <code>context</code>"},{"location":"metric-registry/athena/recommendation/#overview","title":"Overview","text":"Metric Type Score Description <code>CitationAccuracy</code> Rule-Based 0.0\u20131.0 Validates numeric citations <code>[1]</code>, <code>[2]</code> match reference data <code>CitationFidelity</code> Rule-Based 0.0\u20131.0 Verifies bracket-path citations <code>[quote.field]</code> resolve to valid JSON values <code>DecisionQuality</code> LLM-Powered 0.0\u20131.0 Compares AI decision and reasoning against human decisions with hard-fail option <code>ReferReason</code> LLM-Powered \u2014 Extracts and categorizes reasons for referral/decline outcomes (analysis only) <code>UnderwritingCompleteness</code> LLM-Powered 0.0\u20131.0 Ensures recommendations include Decision, Rationale, Evidence, and Next Steps <code>UnderwritingFaithfulness</code> LLM-Powered 0.0\u20131.0 Detects hallucinations by verifying claims against source data (default threshold 0.9) <code>UnderwritingRules</code> Hybrid 0.0\u20131.0 Validates referral triggers and outcome consistency against underwriting guidelines"},{"location":"metric-registry/athena/recommendation/#metrics-detail","title":"Metrics Detail","text":""},{"location":"metric-registry/athena/recommendation/#citationaccuracy-rule-based","title":"CitationAccuracy (Rule-Based)","text":"<p>File: <code>citation_accuracy.py</code></p> <p>Purpose: Validate that numeric citations (e.g. <code>[1]</code>, <code>[2]</code>) in the AI recommendation correctly reference entries in the provided context data.</p> <p>What it computes:</p> <ul> <li>Extracts all numeric citation markers from the recommendation text</li> <li>Checks each citation index against the available reference data</li> <li>Produces a score based on the ratio of valid citations to total citations</li> <li>Default threshold of 1.0 requires all citations to be valid</li> </ul> <p>Score: <code>valid_citations / total_citations</code> (1.0 if no citations found)</p>"},{"location":"metric-registry/athena/recommendation/#citationfidelity-rule-based","title":"CitationFidelity (Rule-Based)","text":"<p>File: <code>citation_fidelity.py</code></p> <p>Purpose: Verify that bracket-path citations (e.g. <code>[quote.field]</code>) in the recommendation resolve to actual values in the source JSON context.</p> <p>What it computes:</p> <ul> <li>Extracts bracket-path citation patterns from the recommendation text</li> <li>Attempts to resolve each path against the provided JSON context</li> <li>Produces a score based on the ratio of resolvable paths to total paths</li> <li>Default threshold of 1.0 requires all paths to resolve</li> </ul> <p>Score: <code>resolvable_paths / total_paths</code> (1.0 if no paths found)</p>"},{"location":"metric-registry/athena/recommendation/#decisionquality-llm-powered","title":"DecisionQuality (LLM-Powered)","text":"<p>File: <code>decision_quality.py</code></p> <p>Purpose: Evaluate how well the AI's underwriting decision and reasoning align with the human underwriter's decision.</p> <p>What it computes:</p> <ul> <li>Compares the AI recommendation (approve/decline/refer) to the expected human decision</li> <li>Uses LLM to evaluate reasoning quality and alignment</li> <li>Produces a weighted quality score combining decision match and reasoning quality</li> <li>Supports a hard-fail option where decision mismatch results in a score of 0.0</li> </ul> <p>Score: Weighted combination of decision accuracy and reasoning alignment (0.0\u20131.0)</p>"},{"location":"metric-registry/athena/recommendation/#referreason-llm-powered","title":"ReferReason (LLM-Powered)","text":"<p>File: <code>refer_reason.py</code></p> <p>Purpose: Extract and categorize the reasons behind referral or decline outcomes for aggregate analysis.</p> <p>What it computes:</p> <ul> <li>Uses LLM to analyze the recommendation text and identify stated reasons</li> <li>Categorizes reasons into predefined types (risk concerns, policy exceptions, missing information, etc.)</li> <li>Produces structured analysis output rather than a numeric score</li> <li>Designed for aggregate reporting and trend analysis across cases</li> </ul> <p>Score: None (analysis-only metric)</p>"},{"location":"metric-registry/athena/recommendation/#underwritingcompleteness-llm-powered","title":"UnderwritingCompleteness (LLM-Powered)","text":"<p>File: <code>underwriting_completeness.py</code></p> <p>Purpose: Ensure that AI recommendations contain all required structural components for a complete underwriting assessment.</p> <p>What it computes:</p> <ul> <li>Uses LLM to check for four required components: Decision, Rationale, Evidence, and Next Steps</li> <li>Produces a weighted score based on which components are present and their quality</li> <li>Identifies missing or weak sections to guide recommendation improvements</li> </ul> <p>Required components:</p> Component Description Decision Clear approve/decline/refer recommendation Rationale Reasoning behind the decision Evidence Supporting data and references Next Steps Actions to take or conditions to meet"},{"location":"metric-registry/athena/recommendation/#underwritingfaithfulness-llm-powered","title":"UnderwritingFaithfulness (LLM-Powered)","text":"<p>File: <code>underwriting_faithfulness.py</code></p> <p>Purpose: Detect hallucinations by verifying that factual claims in the recommendation are supported by the provided source data.</p> <p>What it computes:</p> <ul> <li>Extracts factual claims from the recommendation text</li> <li>Uses LLM to verify each claim against the source context</li> <li>Classifies claims as supported, unsupported, or contradicted</li> <li>Produces a score based on the ratio of supported claims to total claims</li> <li>Default threshold of 0.9 sets a high bar for factual accuracy</li> </ul> <p>Score: <code>supported_claims / total_claims</code> (0.0\u20131.0)</p>"},{"location":"metric-registry/athena/recommendation/#underwritingrules-hybrid","title":"UnderwritingRules (Hybrid)","text":"<p>File: <code>underwriting_rules.py</code></p> <p>Purpose: Validate that the AI recommendation correctly identifies referral triggers and that the outcome is consistent with underwriting guidelines.</p> <p>What it computes:</p> <ul> <li>Uses rule-based detection to identify referral triggers in the source data</li> <li>Compares detected triggers against the AI's stated outcome</li> <li>Validates that cases with referral triggers produce a \"Refer\" outcome</li> <li>Scoped to explicit Refer outcomes \u2014 approve/decline cases are not penalized</li> <li>Default threshold of 1.0 requires full consistency between triggers and outcome</li> </ul> <p>Score: Consistency ratio between detected triggers and stated outcome (0.0\u20131.0)</p>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/","title":"Citation Accuracy","text":"Validate numeric citations against reference data Rule-Based Verification Athena"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Validity ratio \u26a1 Default Threshold <code>1.0</code> All citations must be valid \ud83d\udccb Required Inputs <code>actual_reference</code> Optional: actual_output, additional_output, additional_input <p>What It Measures</p> <p>Citation Accuracy validates numeric citations like <code>[1]</code>, <code>[2]</code> in AI-generated text against the <code>actual_reference</code> data. It ensures every citation points to a real reference entry and optionally verifies that referenced fields exist in the input data.</p> Score Interpretation 1.0  All citations reference valid entries 0.8+  Most citations valid, minor issues 0.5  Half the citations are invalid &lt; 0.5  Significant citation errors <p>See Also: Citation Fidelity</p> <p>Citation Accuracy validates numeric citations like <code>[1]</code> against reference lists. Citation Fidelity validates bracket-path citations like <code>[quote.field]</code> against JSON data.</p> <p>Use Accuracy for numbered references; use Fidelity for JSON path citations.</p> How It Works  Computation Scoring System <p>The metric extracts numeric citations from the output text and validates each against the reference data.</p> <p> \u2705 VALID 1 Citation matches a reference entry and field exists (if checked). </p> <p> \u274c INVALID 0 Citation doesn't match any reference or field doesn't exist. </p> <p>Score Formula</p> <pre><code>score = valid_citations / scorable_citations\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Output Text]\n        B[Reference Data]\n        C[Input Data - Optional]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Citation Extraction\"]\n        D[\"Find [1], [2], etc.\"]\n        E[\"Citation List\"]\n    end\n\n    subgraph VALIDATE[\"\u2696\ufe0f Step 2: Validation\"]\n        F[Match to Reference Entry]\n        G[Optionally Check Input Fields]\n        H[\"Valid / Invalid\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        I[\"Count Valid Citations\"]\n        J[\"Calculate Ratio\"]\n        K[\"Final Score\"]\n    end\n\n    A --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    C --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style VALIDATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style K fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>validation_mode</code> <code>str</code> <code>ref_only</code> <code>ref_only</code> or <code>ref_plus_input</code> <code>output_key</code> <code>str</code> <code>brief_recommendation</code> Key in <code>additional_output</code> to analyze (fallback to <code>actual_output</code>) <p>Validation Modes</p> <ul> <li>ref_only: Only check that citations match reference entries</li> <li>ref_plus_input: Also verify referenced fields exist in <code>additional_input</code></li> </ul>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#code-examples","title":"Code Examples","text":"Basic Usage With Input Validation <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_accuracy import CitationAccuracy\n\nmetric = CitationAccuracy(validation_mode=\"ref_only\")\n\nitem = DatasetItem(\n    actual_output=\"The policy was approved based on the roof age [1] and revenue [2].\",\n    actual_reference=[\"[1] - quote.roof_age\", \"[2] - quote.revenue\"],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (2 of 2 citations valid)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_accuracy import CitationAccuracy\n\nmetric = CitationAccuracy(validation_mode=\"ref_plus_input\")\n\nitem = DatasetItem(\n    actual_output=\"Premium is $1,200 [1].\",\n    actual_reference=[\"[1] - quote.premium\"],\n    additional_input={\"quote\": {\"premium\": 1200}},\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.score}\")\nprint(f\"Valid: {result.signals.valid_citations}/{result.signals.total_citations}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationAccuracyResult Structure <pre><code>CitationAccuracyResult(\n{\n    \"score\": 1.0,\n    \"total_citations\": 2,\n    \"scorable_citations\": 2,\n    \"valid_citations\": 2,\n    \"verdicts\": [\n        {\n            \"citation_text\": \"[1]\",\n            \"citation_number\": 1,\n            \"source\": \"[1] - quote.roof_age\",\n            \"is_scorable\": true,\n            \"is_valid\": true,\n            \"reason\": \"Reference exists.\",\n            \"missing_fields\": null\n        },\n        {\n            \"citation_text\": \"[2]\",\n            \"citation_number\": 2,\n            \"source\": \"[2] - quote.revenue\",\n            \"is_scorable\": true,\n            \"is_valid\": true,\n            \"reason\": \"Reference exists.\",\n            \"missing_fields\": null\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Overall accuracy score <code>total_citations</code> <code>int</code> Total citations found in output <code>scorable_citations</code> <code>int</code> Citations that could be validated <code>valid_citations</code> <code>int</code> Citations that passed validation <code>verdicts</code> <code>List[CitationAccuracyVerdict]</code> Per-citation details (<code>citation_text</code>, <code>citation_number</code>, <code>source</code>, <code>is_scorable</code>, <code>is_valid</code>, <code>reason</code>, <code>missing_fields</code>)"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#example-scenarios","title":"Example Scenarios","text":"Pass (1.0)Partial (0.5) <p>All Citations Match References</p> <p>Output:</p> <p>\"The property qualifies for approval based on the building age [1] and claims history [2].\"</p> <p>Reference Data: <pre><code>[\"[1] - property.building_age\", \"[2] - property.claims_count\"]\n</code></pre></p> <p>Analysis:</p> Citation Reference Match Status <code>[1]</code> <code>[1] - property.building_age</code> \u2705 Valid <code>[2]</code> <code>[2] - property.claims_count</code> \u2705 Valid <p>Final Score: <code>2 / 2 = 1.0</code> </p> <p>Some Citations Invalid</p> <p>Output:</p> <p>\"Coverage approved per [1]. Additional review needed per [3].\"</p> <p>Reference Data: <pre><code>[\"[1] - quote.coverage\", \"[2] - quote.premium\"]\n</code></pre></p> <p>Analysis:</p> Citation Reference Match Status <code>[1]</code> <code>[1] - quote.coverage</code> \u2705 Valid <code>[3]</code> Not found \u274c Invalid <p>Final Score: <code>1 / 2 = 0.5</code> </p>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd17 Traceability <p>Ensures every claim in AI output can be traced back to source data.</p> \u2713 Compliance <p>Critical for regulatory requirements where decisions must be documented.</p> \ud83d\udee1\ufe0f Trust <p>Builds confidence that AI recommendations are grounded in real data.</p>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Accuracy = Do numeric citations <code>[1]</code>, <code>[2]</code> point to valid reference entries?</p> <ul> <li>Use it when: AI output has numbered citations and you have reference data</li> <li>Score interpretation: Higher = more citations are valid</li> <li>Key difference: Validates reference existence, not content accuracy</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Citation Fidelity \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/","title":"Citation Fidelity","text":"Verify bracket-path citations resolve to valid JSON values Rule-Based Verification Athena"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Validity ratio \u26a1 Default Threshold <code>1.0</code> All paths must resolve \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> JSON data required <p>What It Measures</p> <p>Citation Fidelity validates bracket-path citations like <code>[quote.premium]</code> or <code>[property.address]</code> against JSON data in <code>expected_output</code>. It ensures every cited path resolves to a real value and optionally verifies the cited value appears in the surrounding text.</p> Score Interpretation 1.0  All citation paths resolve correctly 0.8+  Most paths valid, minor issues 0.5  Half the citations are invalid &lt; 0.5  Many paths don't resolve <p>See Also: Citation Accuracy</p> <p>Citation Fidelity validates path citations like <code>[quote.field]</code> against JSON. Citation Accuracy validates numeric citations like <code>[1]</code> against reference lists.</p> <p>Use Fidelity for JSON paths; use Accuracy for numbered references.</p> How It Works  Computation Scoring System <p>The metric parses bracket-path citations and resolves each against the JSON structure.</p> <p> \u2705 VALID 1 Path resolves to a value; value appears in text (if check enabled). </p> <p> \u274c INVALID 0 Path doesn't exist or value not found in text. </p> <p>Score Formula</p> <pre><code>score = valid_citations / total_citations\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Output Text]\n        B[Expected Output JSON]\n    end\n\n    subgraph PARSE[\"\ud83d\udd0d Step 1: Parse Citations\"]\n        C[\"Find [path.to.field]\"]\n        D[\"Citation Path List\"]\n    end\n\n    subgraph RESOLVE[\"\u2696\ufe0f Step 2: Path Resolution\"]\n        E[Navigate JSON Structure]\n        F[Optionally Check Value in Text]\n        G[\"Resolved / Not Found\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Valid Paths\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; E\n    B --&gt; E\n    E --&gt; F\n    A --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style PARSE stroke:#3b82f6,stroke-width:2px\n    style RESOLVE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>check_values</code> <code>bool</code> <code>True</code> Verify cited values appear in text <code>window_chars</code> <code>int</code> <code>150</code> Characters to search for value <code>min_shared_tokens</code> <code>int</code> <code>2</code> Min tokens to match for text values <code>fuzzy_threshold</code> <code>float</code> <code>0.88</code> Fuzzy match threshold for strings <code>numeric_tolerance</code> <code>float</code> <code>0.02</code> Tolerance for numeric comparisons <p>Value Checking</p> <p>When <code>check_values=True</code>, the metric verifies that the JSON value actually appears in the text near the citation, preventing citations that point to valid paths but misrepresent the data.</p>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#code-examples","title":"Code Examples","text":"Basic Usage With Value Checking <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_fidelity import CitationFidelity\n\nmetric = CitationFidelity()\n\nitem = DatasetItem(\n    actual_output=\"The premium is $1,200 [quote.premium].\",\n    expected_output={\"quote\": {\"premium\": 1200}},\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (path resolves correctly)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_fidelity import CitationFidelity\n\nmetric = CitationFidelity(check_values=True, window_chars=100)\n\nitem = DatasetItem(\n    actual_output=\"Premium is $1,200 [quote.premium]. Coverage limit $500,000 [quote.coverage].\",\n    expected_output={\n        \"quote\": {\n            \"premium\": 1200,\n            \"coverage\": 500000\n        }\n    },\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.score}\")\nprint(f\"Valid: {result.signals.valid_citations}/{result.signals.total_citations}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationFidelityResult Structure <pre><code>CitationFidelityResult(\n{\n    \"score\": 1.0,\n    \"total_citations\": 2,\n    \"valid_citations\": 2,\n    \"verdicts\": [\n        {\n            \"citation_text\": \"[quote.premium]\",\n            \"path_referenced\": \"quote.premium\",\n            \"json_value\": \"1200\",\n            \"is_valid_path\": true,\n            \"is_supported\": true,\n            \"reason\": \"Exact match.\"\n        },\n        {\n            \"citation_text\": \"[quote.coverage]\",\n            \"path_referenced\": \"quote.coverage\",\n            \"json_value\": \"500000\",\n            \"is_valid_path\": true,\n            \"is_supported\": true,\n            \"reason\": \"Numeric match.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Overall fidelity score <code>total_citations</code> <code>int</code> Total path citations found <code>valid_citations</code> <code>int</code> Citations that resolved and are supported <code>verdicts</code> <code>List[CitationVerdict]</code> Per-citation details (<code>citation_text</code>, <code>path_referenced</code>, <code>json_value</code>, <code>is_valid_path</code>, <code>is_supported</code>, <code>reason</code>)"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#example-scenarios","title":"Example Scenarios","text":"Pass (1.0)Partial (0.5) <p>Valid JSON Path Citations</p> <p>Output:</p> <p>\"Building age is 15 years [property.building_age]. Revenue: $2.5M [financials.revenue].\"</p> <p>Expected Output (JSON): <pre><code>{\n    \"property\": {\"building_age\": 15},\n    \"financials\": {\"revenue\": 2500000}\n}\n</code></pre></p> <p>Analysis:</p> Citation Path Resolved Value Status <code>[property.building_age]</code> <code>property.building_age</code> <code>15</code> \u2705 Valid <code>[financials.revenue]</code> <code>financials.revenue</code> <code>2500000</code> \u2705 Valid <p>Final Score: <code>2 / 2 = 1.0</code> </p> <p>Some Paths Don't Resolve</p> <p>Output:</p> <p>\"Premium is $1,200 [quote.premium]. Deductible: $500 [quote.deductible].\"</p> <p>Expected Output (JSON): <pre><code>{\n    \"quote\": {\"premium\": 1200}\n}\n</code></pre></p> <p>Analysis:</p> Citation Path Resolved Value Status <code>[quote.premium]</code> <code>quote.premium</code> <code>1200</code> \u2705 Valid <code>[quote.deductible]</code> <code>quote.deductible</code> Not found \u274c Invalid <p>Final Score: <code>1 / 2 = 0.5</code> </p>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Data Accuracy <p>Ensures AI-cited values actually exist in the source data structure.</p> \ud83d\udd0d Value Verification <p>Optional check that cited values match what's stated in the text.</p> \ud83d\udcca Structured Tracing <p>Enables precise traceability through JSON path references.</p>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Fidelity = Do bracket-path citations like <code>[quote.field]</code> resolve to valid JSON values?</p> <ul> <li>Use it when: AI output uses JSON path citations and you have structured data</li> <li>Score interpretation: Higher = more paths resolve correctly</li> <li>Key difference: Validates path resolution; optionally checks value accuracy</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Citation Accuracy \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/decision_quality/","title":"Decision Quality","text":"Evaluate AI decision accuracy and reasoning alignment LLM-Powered Quality Athena"},{"location":"metric-registry/athena/recommendation/decision_quality/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Weighted quality score \u26a1 Default Threshold <code>0.8</code> High Bar \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Human decision + AI recommendation <p>What It Measures</p> <p>Decision Quality evaluates whether the AI made the correct underwriting decision (approve/decline/refer) and whether its reasoning aligns with the human underwriter's notes. It combines decision match scoring with reasoning coverage analysis.</p> Score Interpretation 1.0  Correct decision + complete reasoning 0.7+  Correct decision, reasoning mostly aligned 0.5  Decision match but reasoning gaps 0.0  Wrong decision (hard fail enabled) How It Works  Computation Scoring System <p>The metric extracts decisions from both human and AI outputs, scores the match, and analyzes reasoning coverage.</p> <p> \u2705 DECISION MATCH 1.0 AI decision matches human decision exactly. </p> <p> \u274c DECISION MISMATCH 0.0 AI decision differs from human decision. </p> <p>Score Formula</p> <pre><code>overall_score = (outcome_weight \u00d7 outcome_score) + (reasoning_weight \u00d7 reasoning_score)\n\n# If hard_fail_on_outcome_mismatch=True and decisions differ:\noverall_score = 0.0\n</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Recommendation]\n        B[Human Decision/Notes]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Decision Extraction\"]\n        C[Detect Human Decision]\n        D[Detect AI Decision]\n        E[\"Approve / Decline / Refer\"]\n    end\n\n    subgraph MATCH[\"\u2696\ufe0f Step 2: Decision Match\"]\n        F[Compare Decisions]\n        G[\"outcome_score\"]\n    end\n\n    subgraph REASON[\"\ud83d\udcdd Step 3: Reasoning Coverage\"]\n        H[Extract Risk Factors]\n        I[Check Coverage in AI Output]\n        J[\"reasoning_score\"]\n    end\n\n    subgraph COMBINE[\"\ud83d\udcca Step 4: Final Score\"]\n        K[Apply Weights]\n        L[Hard Fail Check]\n        M[\"overall_score\"]\n    end\n\n    A --&gt; D\n    B --&gt; C\n    C &amp; D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    B --&gt; H\n    A --&gt; I\n    H --&gt; I\n    I --&gt; J\n    G &amp; J --&gt; K\n    K --&gt; L\n    L --&gt; M\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style MATCH stroke:#f59e0b,stroke-width:2px\n    style REASON stroke:#8b5cf6,stroke-width:2px\n    style COMBINE stroke:#10b981,stroke-width:2px\n    style M fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>outcome_weight</code> <code>float</code> <code>1.0</code> Weight for decision match component <code>reasoning_weight</code> <code>float</code> <code>0.0</code> Weight for reasoning coverage <code>hard_fail_on_outcome_mismatch</code> <code>bool</code> <code>True</code> Force score to 0.0 if decisions differ <code>recommendation_column_name</code> <code>str</code> <code>brief_recommendation</code> Additional output field to analyze <p>Default Behavior</p> <p>By default, <code>outcome_weight=1.0</code> and <code>reasoning_weight=0.0</code>, meaning the score is purely based on decision match (1.0 or 0.0). Set <code>reasoning_weight &gt; 0</code> to include reasoning coverage analysis.</p> <p>Hard Fail Mode</p> <p>When <code>hard_fail_on_outcome_mismatch=True</code> (default), any decision mismatch results in a score of 0.0, regardless of reasoning quality. Disable this for softer evaluation.</p>"},{"location":"metric-registry/athena/recommendation/decision_quality/#code-examples","title":"Code Examples","text":"Basic Usage Custom Weights <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.decision_quality import DecisionQuality\n\nmetric = DecisionQuality()\n\nitem = DatasetItem(\n    actual_output=\"Recommend Decline due to building age exceeding 30 years.\",\n    expected_output=\"Decline - roof age and building condition are concerns.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Overall: 0.85 (decision match + partial reasoning coverage)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.decision_quality import DecisionQuality\n\n# Prioritize reasoning over outcome\nmetric = DecisionQuality(\n    outcome_weight=0.4,\n    reasoning_weight=0.6,\n    hard_fail_on_outcome_mismatch=False,\n)\n\nitem = DatasetItem(\n    actual_output=\"Approve with conditions for roof repair.\",\n    expected_output=\"Approve - good risk profile, minor roof concerns noted.\",\n)\n\nresult = await metric.execute(item)\nprint(f\"Overall: {result.signals.overall_score}\")\nprint(f\"Outcome Match: {result.signals.outcome_match}\")\nprint(f\"Reasoning: {result.signals.reasoning_score}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca DecisionQualityResult Structure <pre><code>DecisionQualityResult(\n{\n    \"overall_score\": 1.0,\n    \"outcome_match\": true,\n    \"outcome_score\": 1.0,\n    \"human_decision_detected\": \"decline\",\n    \"ai_decision_detected\": \"decline\",\n    \"reasoning_score\": 0.625,\n    \"matched_concepts\": [\n        {\"concept\": \"building age\"},\n        {\"concept\": \"condition\"}\n    ],\n    \"missing_concepts\": [\n        {\"concept\": \"roof age\", \"impact\": \"High\"}\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> Combined weighted score <code>outcome_match</code> <code>bool</code> Whether decisions matched <code>outcome_score</code> <code>float</code> Decision match score (0 or 1) <code>human_decision_detected</code> <code>str</code> Extracted human decision <code>ai_decision_detected</code> <code>str</code> Extracted AI decision <code>reasoning_score</code> <code>float \\| None</code> Impact-weighted reasoning coverage score (None when no risk factors extracted) <code>matched_concepts</code> <code>List[ReasoningMatch]</code> Risk factors mentioned by AI (each has <code>concept</code>) <code>missing_concepts</code> <code>List[ReasoningGap]</code> Risk factors AI missed (each has <code>concept</code> and <code>impact</code>)"},{"location":"metric-registry/athena/recommendation/decision_quality/#example-scenarios","title":"Example Scenarios","text":"Pass (1.0)Partial (0.73)Fail (0.0) <p>Decision Match (Default Weights)</p> <p>Human Decision:</p> <p>\"Decline - prior claims history and high BPP value are concerns.\"</p> <p>AI Recommendation:</p> <p>\"Recommend Decline. The applicant has prior claims on record and the BPP coverage requested exceeds typical thresholds.\"</p> <p>Analysis:</p> Component Score Details Decision Match 1.0 Both: Decline Reasoning Coverage 1.0 All factors mentioned <p>Final Score: <code>(1.0 \u00d7 1.0) + (0.0 \u00d7 1.0) = 1.0</code> </p> <p>With custom weights <code>outcome_weight=0.6, reasoning_weight=0.4</code></p> <p>Score would be <code>(0.6 \u00d7 1.0) + (0.4 \u00d7 1.0) = 1.0</code></p> <p>Correct Decision, Missing Factors (Custom Weights)</p> <p>With <code>outcome_weight=0.6, reasoning_weight=0.4</code>:</p> <p>Human Decision:</p> <p>\"Approve - good claims history, reasonable BPP, building in good condition.\"</p> <p>AI Recommendation:</p> <p>\"Recommend Approve based on clean claims history.\"</p> <p>Analysis:</p> Component Score Details Decision Match 1.0 Both: Approve Reasoning Coverage 0.33 Only 1 of 3 factors mentioned <p>Final Score: <code>(0.6 \u00d7 1.0) + (0.4 \u00d7 0.33) = 0.73</code> </p> <p>Wrong Decision (Hard Fail)</p> <p>Human Decision:</p> <p>\"Decline - too many risk factors.\"</p> <p>AI Recommendation:</p> <p>\"Recommend Approve based on revenue metrics.\"</p> <p>Analysis:</p> Component Score Details Decision Match 0.0 Human: Decline, AI: Approve Hard Fail Triggered Score forced to 0.0 <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/decision_quality/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Decision Accuracy <p>Measures whether AI reaches the same conclusions as human experts.</p> \ud83d\udcdd Reasoning Quality <p>Ensures AI considers the same risk factors as human underwriters.</p> \ud83d\udd04 Calibration <p>Helps identify where AI and human judgment diverge for retraining.</p>"},{"location":"metric-registry/athena/recommendation/decision_quality/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Decision Quality = Does AI make the right decision for the right reasons?</p> <ul> <li>Use it when: You have ground truth decisions and want to evaluate both outcome and reasoning</li> <li>Score interpretation: Higher = better decision + reasoning alignment</li> <li>Key feature: Configurable hard-fail on decision mismatch</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Underwriting Completeness \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/refer_reason/","title":"Refer Reason","text":"Extract and categorize reasons for referral/decline outcomes LLM-Powered Analysis Athena"},{"location":"metric-registry/athena/recommendation/refer_reason/#at-a-glance","title":"At a Glance","text":"\ud83d\udcca Score Range <code>\u2014</code> Analysis metric (no score) \u26a1 Default Threshold <code>\u2014</code> Not applicable \ud83d\udccb Required Inputs <code>actual_output</code> AI recommendation text <p>What It Measures</p> <p>Refer Reason is an analysis metric that extracts and categorizes the reasons behind referral or decline outcomes. It identifies the primary reason category, all contributing reasons, and classifies the actionable type (market, system, or policy).</p> Output Description <code>primary_category</code> Main reason category (e.g., Claims History, BPP Value) <code>all_reasons</code> Complete list of extracted reasons <code>actionable_type</code> Classification: market, system, or policy How It Works  Computation Reason Categories <p>The metric detects negative outcomes, then uses LLM extraction to identify and categorize all reasons.</p> <pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[AI Recommendation]\n    end\n\n    subgraph DETECT[\"\ud83d\udd0d Step 1: Outcome Detection\"]\n        B[Check for Referral/Decline]\n        C{Negative Outcome?}\n    end\n\n    subgraph EXTRACT[\"\ud83d\udcdd Step 2: Reason Extraction\"]\n        D[LLM Analysis]\n        E[\"Extract All Reasons\"]\n    end\n\n    subgraph CATEGORIZE[\"\ud83c\udff7\ufe0f Step 3: Categorization\"]\n        F[Assign Categories]\n        G[Determine Primary]\n        H[Classify Actionable Type]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Output\"]\n        I[\"Structured Analysis\"]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt;|Yes| D\n    C --&gt;|No| I\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style DETECT stroke:#3b82f6,stroke-width:2px\n    style EXTRACT stroke:#f59e0b,stroke-width:2px\n    style CATEGORIZE stroke:#8b5cf6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre> <p>The metric uses 18 granular <code>ReasonCategory</code> values grouped by actionable type:</p> <p>Coverage &amp; Pricing</p> Category Description Actionable <code>Excessive Coverage Requested</code> Coverage limits materially exceed typical thresholds market <code>Inadequate / Suspicious Valuation</code> Coverage amounts appear insufficient or implausible market <code>Pricing Anomaly - Too Low</code> Calculated premium appears unusually low market <code>Pricing Anomaly - Too High</code> Calculated premium appears unusually high market <p>Property &amp; Location</p> Category Description Actionable <code>Property Condition Concerns</code> Elevated risk from property condition market <code>Construction / Exposure Threshold</code> Property characteristics exceed thresholds market <code>Location / CAT Risk</code> Elevated catastrophe or environmental risk market <p>Data &amp; Classification</p> Category Description Actionable <code>Data Conflict / Mismatch</code> Discrepancies between customer and third-party data system <code>Implausible / Invalid Data</code> One or more inputs appear invalid system <code>Missing / Unverifiable Data</code> Required data cannot be verified system <code>Industry / Class Code Error</code> Business classification appears incorrect policy <code>Unrelated / Ancillary Operations</code> Operations outside primary stated class policy <p>Business &amp; Operational</p> Category Description Actionable <code>Startup / New Venture</code> Limited or no operating history market <code>Multi-Location / Complex Ops</code> Multiple locations requiring manual review market <code>Financial / Operational Inconsistency</code> Operational metrics are internally inconsistent market <code>Ownership / Insurable Interest Issue</code> Coverage for property without insurable interest policy <p>Compliance &amp; Procedural</p> Category Description Actionable <code>Sanctions / Watchlists</code> Potential regulatory or sanctions concern policy <code>Procedural / Temporary Block</code> Procedural rather than risk-based referral system <code>Other / Unknown</code> Does not fit a defined category unknown"},{"location":"metric-registry/athena/recommendation/refer_reason/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>recommendation_column_name</code> <code>str</code> <code>brief_recommendation</code> Field in additional_output to analyze <code>max_source_lines</code> <code>int</code> <code>50</code> Max source lines for context"},{"location":"metric-registry/athena/recommendation/refer_reason/#code-examples","title":"Code Examples","text":"Basic Usage Full Analysis <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.refer_reason import ReferReason\n\nmetric = ReferReason()\n\nitem = DatasetItem(\n    actual_output=\"Refer to underwriting. Roof is 28 years old, exceeding 20-year threshold.\"\n)\n\nresult = await metric.execute(item)\nprint(result.explanation)\n# \"Property Condition Concerns\"\n\nprint(result.signals.primary_category)\n# ReasonCategory.PROPERTY_CONDITION\n\nprint(result.signals.all_reasons)\n# [ExtractedReason(reason_text=\"...\", category=ReasonCategory.PROPERTY_CONDITION, reasoning=\"...\"), ...]\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.refer_reason import ReferReason\n\nmetric = ReferReason()\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    Refer to underwriting team.\n\n    Reasons:\n    - Business established in 2023 (less than 3 years)\n    - BPP limit requested: $300,000 (exceeds threshold)\n    - Home-based business requesting contents coverage\n    \"\"\"\n)\n\nresult = await metric.execute(item)\n\nprint(f\"Outcome: {result.signals.outcome_label}\")\nprint(f\"Primary Reason: {result.signals.primary_category}\")\nprint(f\"Reason Count: {result.signals.reason_count}\")\nprint(f\"Actionable Type: {result.signals.actionable_type}\")\n\nfor reason in result.signals.all_reasons:\n    print(f\"  - {reason['category']}: {reason['reasoning']}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/refer_reason/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Access detailed analysis results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.explanation)   # Primary category as explanation\nresult.signals              # Full analysis breakdown\n</code></pre> \ud83d\udcca ReasonAnalysisResult Structure <pre><code>ReasonAnalysisResult(\n{\n    \"is_negative_outcome\": true,\n    \"outcome_label\": \"Refer to Underwriter\",\n    \"primary_reason\": {\n        \"reason_text\": \"Roof is 28 years old, exceeding 20-year threshold\",\n        \"category\": \"Property Condition Concerns\",\n        \"reasoning\": \"Explicit mention of roof age exceeding policy threshold.\"\n    },\n    \"primary_category\": \"Property Condition Concerns\",\n    \"all_reasons\": [\n        {\n            \"reason_text\": \"Roof is 28 years old, exceeding 20-year threshold\",\n            \"category\": \"Property Condition Concerns\",\n            \"reasoning\": \"Explicit mention of roof age exceeding policy threshold.\"\n        },\n        {\n            \"reason_text\": \"Original knob-and-tube wiring from 1950s\",\n            \"category\": \"Property Condition Concerns\",\n            \"reasoning\": \"Outdated wiring type is a known fire hazard concern.\"\n        }\n    ],\n    \"reason_count\": 2,\n    \"actionable_type\": \"market\"\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/refer_reason/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>is_negative_outcome</code> <code>bool</code> Whether outcome is referral/decline <code>outcome_label</code> <code>str</code> Refer to Underwriter, Decline, Approved, or Unknown <code>primary_reason</code> <code>ExtractedReason</code> Most significant reason (<code>reason_text</code>, <code>category</code>, <code>reasoning</code>) <code>primary_category</code> <code>ReasonCategory</code> Category enum of the primary reason <code>all_reasons</code> <code>List[ExtractedReason]</code> All extracted reasons <code>reason_count</code> <code>int</code> Number of reasons detected <code>actionable_type</code> <code>str</code> market, system, policy, or unknown"},{"location":"metric-registry/athena/recommendation/refer_reason/#example-scenarios","title":"Example Scenarios","text":"Single ReasonMultiple ReasonsApproval (N/A) <p>Property Condition</p> <p>Recommendation:</p> <p>\"Refer to underwriting - roof is 28 years old, exceeding 20-year threshold.\"</p> <p>Analysis:</p> Field Value Outcome Refer to Underwriter Primary Category Property Condition Concerns Reason Count 1 Actionable Type market <p>Explanation: <code>\"Property Condition Concerns\"</code></p> <p>Multiple Factors</p> <p>Recommendation:</p> <p>\"Refer - new business (2024), BPP limit of $3.5M appears excessive for $800k sales, and year built conflicts between customer data and enrichment.\"</p> <p>Analysis:</p> Field Value Outcome Refer to Underwriter Primary Category Excessive Coverage Requested Reason Count 3 All Reasons Excessive Coverage Requested, Data Conflict / Mismatch, Startup / New Venture Actionable Type market <p>Explanation: <code>\"Excessive Coverage Requested\"</code></p> <p>Not Applicable</p> <p>Recommendation:</p> <p>\"Approve - all criteria within guidelines.\"</p> <p>Analysis:</p> Field Value Outcome Approved Primary Category None Reason Count 0 <p>Note: Analysis only runs for negative outcomes.</p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#actionable-types","title":"Actionable Types","text":"\ud83c\udfea Market <p>External market conditions or factors outside control.</p> \u2699\ufe0f System <p>Configurable thresholds or rules that could be adjusted.</p> \ud83d\udcdc Policy <p>Fixed policy requirements or guidelines.</p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcc8 Analytics <p>Enables aggregation and trending of referral reasons.</p> \ud83d\udd0d Root Cause <p>Identifies patterns in why applications are declined.</p> \ud83c\udfaf Process Improvement <p>Helps identify where guidelines could be refined.</p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Refer Reason = Why did the AI refer or decline this application?</p> <ul> <li>Use it when: You need to categorize and analyze referral/decline reasons</li> <li>Output type: Analysis (no score)</li> <li>Key feature: Structured extraction with category classification</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Underwriting Rules \u00b7 Decision Quality</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/","title":"Underwriting Completeness","text":"Ensure recommendations contain all required components LLM-Powered Completeness Athena"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Weighted criteria score \u26a1 Default Threshold <code>0.8</code> High Bar \ud83d\udccb Required Inputs <code>actual_output</code> AI recommendation text <p>What It Measures</p> <p>Underwriting Completeness evaluates whether a recommendation contains all required components of a complete underwriting decision: Decision, Rationale, Evidence, and Next Steps. Missing the decision component results in an automatic score of 0.0.</p> Score Interpretation 1.0  All four components present and strong 0.7+  Decision clear, some components weaker 0.5  Missing or weak supporting components 0.0  No clear decision (hard gate) How It Works  Computation Criteria Components <p>The metric runs four specialized LLM judges to evaluate each component of the recommendation.</p> <p> \ud83c\udfaf Decision (Required) Clear approve/decline/refer recommendation. Hard gate: missing = score 0.0 </p> <p> \ud83d\udcdd Rationale Explanation of why the decision was made. </p> <p> \ud83d\udcca Evidence Supporting data points and facts cited. </p> <p> \u27a1\ufe0f Next Steps Clear guidance on what happens next. </p> <p>Score Formula</p> <pre><code>overall_score = \u03a3 (weight[i] \u00d7 criterion_score[i])\n\n# If Decision score = 0:\noverall_score = 0.0  (hard gate)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[AI Recommendation]\n    end\n\n    subgraph JUDGES[\"\u2696\ufe0f Four Criteria Judges\"]\n        B[\"\ud83c\udfaf Decision Judge\"]\n        C[\"\ud83d\udcdd Rationale Judge\"]\n        D[\"\ud83d\udcca Evidence Judge\"]\n        E[\"\u27a1\ufe0f Next Step Judge\"]\n    end\n\n    subgraph SCORES[\"\ud83d\udcca Per-Criterion Scores\"]\n        F[\"Decision Score\"]\n        G[\"Rationale Score\"]\n        H[\"Evidence Score\"]\n        I[\"NextStep Score\"]\n    end\n\n    subgraph COMBINE[\"\ud83d\udd04 Final Score\"]\n        J[Apply Weights]\n        K[Check Decision Gate]\n        L[\"overall_score\"]\n    end\n\n    A --&gt; B &amp; C &amp; D &amp; E\n    B --&gt; F\n    C --&gt; G\n    D --&gt; H\n    E --&gt; I\n    F &amp; G &amp; H &amp; I --&gt; J\n    J --&gt; K\n    K --&gt; L\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style JUDGES stroke:#3b82f6,stroke-width:2px\n    style SCORES stroke:#f59e0b,stroke-width:2px\n    style COMBINE stroke:#10b981,stroke-width:2px\n    style L fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>weights</code> <code>dict</code> See below Per-criterion weights <code>main_check_field</code> <code>str</code> <code>brief_recommendation</code> Field in additional_output to analyze <p>Default Weights:</p> Criterion Default Weight Decision 0.4 Rationale 0.2 Evidence 0.2 NextStep 0.2 <p>Hard Gate</p> <p>If the Decision criterion scores 0.0 (no clear decision found), the overall score is forced to 0.0 regardless of other criteria.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#code-examples","title":"Code Examples","text":"Basic Usage Custom Weights <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_completeness import UnderwritingCompleteness\n\nmetric = UnderwritingCompleteness()\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    **Recommendation: Approve**\n\n    The applicant presents a low-risk profile based on:\n    - Building age: 5 years (excellent)\n    - Clean claims history (0 claims)\n    - Revenue: $1.2M annually\n\n    Next steps: Proceed to bind coverage. No additional documentation required.\n    \"\"\"\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (all components present and strong)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_completeness import UnderwritingCompleteness\n\n# Prioritize evidence over next steps\nmetric = UnderwritingCompleteness(\n    weights={\n        \"Decision\": 0.35,\n        \"Rationale\": 0.25,\n        \"Evidence\": 0.30,\n        \"NextStep\": 0.10,\n    }\n)\n\nitem = DatasetItem(actual_output=\"Approve. Roof age 5 years. Revenue $1.2M. Next step: bind.\")\nresult = await metric.execute(item)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca UnderwritingCompletenessResult Structure <pre><code>UnderwritingCompletenessResult(\n{\n    \"overall_score\": 0.8,\n    \"criteria\": [\n        {\n            \"name\": \"Decision\",\n            \"score\": 1.0,\n            \"reasoning\": \"Explicitly states 'Approve'.\",\n            \"evidence_found\": \"Recommendation: Approve\"\n        },\n        {\n            \"name\": \"Rationale\",\n            \"score\": 1.0,\n            \"reasoning\": \"Cites specific factor 'building age'.\",\n            \"evidence_found\": \"building age: 5 years\"\n        },\n        {\n            \"name\": \"Evidence\",\n            \"score\": 1.0,\n            \"reasoning\": \"Specific financial data point.\",\n            \"evidence_found\": \"$1.2M\"\n        },\n        {\n            \"name\": \"NextStep\",\n            \"score\": 0.0,\n            \"reasoning\": \"Statement of fact only.\",\n            \"evidence_found\": null\n        }\n    ]\n}\n)\n</code></pre> <p>Binary Scoring</p> <p>Each criterion judge returns either 1.0 (pass) or 0.0 (fail). Scores are not continuous.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> Weighted combination of criteria <code>criteria</code> <code>List[CompletenessCriterion]</code> Per-criterion results (<code>name</code>, <code>score</code>, <code>reasoning</code>, <code>evidence_found</code>)"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#example-scenarios","title":"Example Scenarios","text":"Pass (1.0)Partial (0.6)Fail (0.0) <p>All Components Present</p> <p>Recommendation:</p> <p>\"Approve this application. The business has excellent financials with $2.1M annual revenue, no prior claims in 5 years, and the building is well-maintained (constructed 2019). Proceed to bind the policy immediately.\"</p> <p>Analysis:</p> Criterion Score Finding Decision 1.0 Clear \"Approve\" Rationale 1.0 Cites specific factor Evidence 1.0 Specific data cited NextStep 1.0 Clear action <p>Final Score: <code>(0.4 \u00d7 1.0) + (0.2 \u00d7 1.0) + (0.2 \u00d7 1.0) + (0.2 \u00d7 1.0) = 1.0</code> </p> <p>Missing Elements</p> <p>Recommendation:</p> <p>\"Approve. Good risk.\"</p> <p>Analysis:</p> Criterion Score Finding Decision 1.0 Clear \"Approve\" Rationale 0.0 Too generic Evidence 0.0 No specific data NextStep 0.0 No next steps <p>Final Score: <code>(0.4 \u00d7 1.0) + (0.2 \u00d7 0.0) + (0.2 \u00d7 0.0) + (0.2 \u00d7 0.0) = 0.4</code> </p> <p>Hard Gate Triggered</p> <p>Recommendation:</p> <p>\"The building is 10 years old with $500k revenue. There have been 2 claims in the past 3 years.\"</p> <p>Analysis:</p> Criterion Score Finding Decision 0.0 No decision stated Hard Gate Triggered Score forced to 0.0 <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udccb Actionable Output <p>Ensures AI recommendations can be acted upon by underwriters.</p> \ud83d\udcdd Documentation <p>Complete recommendations create an audit trail for compliance.</p> \ud83c\udf93 Training Signal <p>Helps identify where AI outputs need structural improvement.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Underwriting Completeness = Does the recommendation have all required parts?</p> <ul> <li>Use it when: Evaluating the structure of AI recommendations</li> <li>Score interpretation: Higher = more complete recommendation</li> <li>Key feature: Hard gate on missing decision</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Decision Quality \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/","title":"Underwriting Faithfulness","text":"Verify factual claims are supported by source data LLM-Powered Faithfulness Athena"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Support ratio \u26a1 Default Threshold <code>0.9</code> High bar for factual accuracy \ud83d\udccb Required Inputs <code>actual_output</code> Optional: additional_input (source data) <p>What It Measures</p> <p>Underwriting Faithfulness checks whether factual claims in the AI recommendation are supported by the source data. It extracts atomic claims from the recommendation, finds relevant evidence in the input data, and verifies each claim. Unsupported claims are flagged as potential hallucinations.</p> Score Interpretation 1.0  All claims verified against source data 0.9+  Nearly all claims supported 0.7  Some claims not verifiable &lt; 0.7  Significant hallucination risk How It Works  Computation Verification Modes <p>The metric extracts claims from the recommendation and verifies each against the source data.</p> <p> \ud83e\udd16 LLM Mode Uses LLM to semantically verify claims against evidence. Most accurate but slower. </p> <p> \u26a1 Heuristic Mode Uses pattern matching and fuzzy comparison. Faster but less nuanced. </p> <p> \ud83d\udd04 Heuristic-then-LLM Tries heuristic first, falls back to LLM for uncertain cases. </p> <p>Score Formula</p> <pre><code>overall_score = supported_claims / total_claims\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Recommendation]\n        B[Source Data JSON]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Claim Extraction\"]\n        C[Break into Atomic Claims]\n        D[\"Claim List\"]\n    end\n\n    subgraph EVIDENCE[\"\ud83d\udcca Step 2: Evidence Finding\"]\n        E[Search Source Data]\n        F[\"Relevant Evidence Lines\"]\n    end\n\n    subgraph VERIFY[\"\u2696\ufe0f Step 3: Verification\"]\n        G[LLM or Heuristic Check]\n        H[\"Supported / Unsupported\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcc8 Step 4: Scoring\"]\n        I[\"Count Supported Claims\"]\n        J[\"Calculate Ratio\"]\n        K[\"overall_score\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; E\n    B --&gt; E\n    E --&gt; F\n    D &amp; F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style EVIDENCE stroke:#f59e0b,stroke-width:2px\n    style VERIFY stroke:#8b5cf6,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style K fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>verification_mode</code> <code>str</code> <code>llm</code> <code>llm</code>, <code>heuristic</code>, or <code>heuristic_then_llm</code> <code>max_claims</code> <code>int</code> <code>10</code> Maximum claims to verify <code>max_concurrent</code> <code>int</code> <code>5</code> Concurrency limit for LLM verification"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#code-examples","title":"Code Examples","text":"Basic Usage LLM Verification <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_faithfulness import UnderwritingFaithfulness\n\nmetric = UnderwritingFaithfulness(verification_mode=\"heuristic\")\n\nitem = DatasetItem(\n    actual_output=\"The business has annual revenue of $1.2M and no prior claims.\",\n    additional_input={\n        \"financials\": {\"annual_revenue\": 1200000},\n        \"claims\": {\"count\": 0, \"history\": []}\n    }\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (both claims verified)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_faithfulness import UnderwritingFaithfulness\n\nmetric = UnderwritingFaithfulness(\n    verification_mode=\"llm\",\n    max_claims=30,\n    max_concurrent=5,\n)\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    Recommend approve based on:\n    - Strong financials: $2.1M revenue\n    - Clean 5-year claims history\n    - Building constructed in 2019\n    \"\"\",\n    additional_input={\n        \"financials\": {\"revenue\": 2100000},\n        \"claims\": {\"five_year_count\": 0},\n        \"property\": {\"year_built\": 2019}\n    }\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.signals.overall_score}\")\nprint(f\"Supported: {result.signals.supported_claims}/{result.signals.total_claims}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca UnderwritingFaithfulnessResult Structure <pre><code>UnderwritingFaithfulnessResult(\n{\n    \"overall_score\": 0.75,\n    \"total_claims\": 4,\n    \"supported_claims\": 3,\n    \"hallucinations\": 1,\n    \"claim_details\": [\n        {\n            \"claim\": \"Revenue is $2.1M\",\n            \"status\": \"\u2705 Supported\",\n            \"reason\": \"Exact numeric match in source data.\",\n            \"decision_source\": \"llm\"\n        },\n        {\n            \"claim\": \"Building was constructed in 2019\",\n            \"status\": \"\u2705 Supported\",\n            \"reason\": \"Year built matches property.year_built.\",\n            \"decision_source\": \"llm\"\n        },\n        {\n            \"claim\": \"No claims in 5 years\",\n            \"status\": \"\u2705 Supported\",\n            \"reason\": \"claims.five_year_count: 0 confirms zero claims.\",\n            \"decision_source\": \"llm\"\n        },\n        {\n            \"claim\": \"Premium is $1,200\",\n            \"status\": \"\u274c Hallucinated/Unsupported\",\n            \"reason\": \"No premium data found in source.\",\n            \"decision_source\": \"llm\"\n        }\n    ],\n    \"unverified_claims\": []\n}\n)\n</code></pre> <p>Unverified vs Hallucinated</p> <p><code>unverified_claims</code> lists claims that exceeded <code>max_claims</code> and were not checked. <code>claim_details</code> contains verdicts for all checked claims.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> Proportion of supported claims <code>total_claims</code> <code>int</code> Total atomic claims extracted (capped by <code>max_claims</code>) <code>supported_claims</code> <code>int</code> Claims verified against source <code>hallucinations</code> <code>int</code> Claims not found in source <code>claim_details</code> <code>List[dict]</code> Per-claim details (<code>claim</code>, <code>status</code>, <code>reason</code>, <code>decision_source</code>) <code>unverified_claims</code> <code>List[str]</code> Claims that exceeded <code>max_claims</code> and were not checked"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#example-scenarios","title":"Example Scenarios","text":"Pass (1.0)Partial (0.67)Fail (0.0) <p>Fully Faithful Recommendation</p> <p>Recommendation:</p> <p>\"Approve. Revenue is $1.5M. Building age is 8 years. Zero claims.\"</p> <p>Source Data: <pre><code>{\n    \"financials\": {\"revenue\": 1500000},\n    \"property\": {\"building_age\": 8},\n    \"claims\": {\"count\": 0}\n}\n</code></pre></p> <p>Analysis:</p> Claim Evidence Status Revenue $1.5M <code>financials.revenue: 1500000</code> \u2705 Supported Building age 8 years <code>property.building_age: 8</code> \u2705 Supported Zero claims <code>claims.count: 0</code> \u2705 Supported <p>Final Score: <code>3 / 3 = 1.0</code> </p> <p>Unsupported Claim Found</p> <p>Recommendation:</p> <p>\"Revenue is $1.5M. Building age is 8 years. Premium is $1,200.\"</p> <p>Source Data: <pre><code>{\n    \"financials\": {\"revenue\": 1500000},\n    \"property\": {\"building_age\": 8}\n}\n</code></pre></p> <p>Analysis:</p> Claim Evidence Status Revenue $1.5M <code>financials.revenue: 1500000</code> \u2705 Supported Building age 8 years <code>property.building_age: 8</code> \u2705 Supported Premium $1,200 Not found \u274c Hallucination <p>Final Score: <code>2 / 3 = 0.67</code> </p> <p>Multiple Unsupported Claims</p> <p>Recommendation:</p> <p>\"Revenue is $5M. Building is brand new. Located in a low-risk zone. Premium is competitive.\"</p> <p>Source Data: <pre><code>{\n    \"financials\": {\"revenue\": 1000000}\n}\n</code></pre></p> <p>Analysis:</p> Claim Evidence Status Revenue $5M Contradicts source ($1M) \u274c False Building brand new Not found \u274c Hallucination Low-risk zone Not found \u274c Hallucination Competitive premium Not found \u274c Hallucination <p>Final Score: <code>0 / 4 = 0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Hallucination Detection <p>Catches AI claims that aren't grounded in actual data.</p> \u2713 Trust &amp; Compliance <p>Critical for regulated industries where false claims have consequences.</p> \ud83d\udee1\ufe0f Risk Mitigation <p>Prevents decisions based on fabricated information.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Underwriting Faithfulness = Are the AI's factual claims actually in the source data?</p> <ul> <li>Use it when: You have source data and need to verify factual accuracy</li> <li>Score interpretation: Higher = more claims verified, fewer hallucinations</li> <li>Key feature: Detects fabricated facts not in source data</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Citation Accuracy \u00b7 Citation Fidelity \u00b7 Underwriting Completeness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/","title":"Underwriting Rules","text":"Track referral triggers and validate outcome consistency Hybrid Rules Athena"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Consistency score \u26a1 Default Threshold <code>1.0</code> Outcome must match triggers \ud83d\udccb Required Inputs <code>actual_output</code> <code>additional_input</code> Recommendation + source data <p>What It Measures</p> <p>Underwriting Rules tracks referral triggers, but it is now scoped to explicit <code>Refer</code> outcomes.</p> <ul> <li>If the detected outcome is not <code>Refer</code> (e.g. Approved / Decline / Unknown), the metric returns early with <code>score=1.0</code> and does not run trigger detection.</li> <li>If the detected outcome is <code>Refer</code>, the metric requires at least one trigger to be found; otherwise it is treated as <code>unknown_trigger</code>.</li> </ul> Score Interpretation 1.0  Outcome is not <code>Refer</code> (skipped) or <code>Refer</code> with a trigger found 0.0  Outcome is <code>Refer</code> but no trigger could be detected (<code>unknown_trigger</code>) How It Works  Detection Pipeline Scoring Logic <p>The metric first detects the outcome label. Only explicit <code>Refer</code> outcomes proceed to trigger detection (structured checks \u2192 regex scan \u2192 LLM fallback).</p> <pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Recommendation]\n        B[Source Data JSON]\n    end\n\n    subgraph OUTCOME[\"\ud83c\udfaf Step 1: Outcome Detection\"]\n        C[Detect AI Decision]\n        D[\"Approve / Refer / Decline\"]\n    end\n\n    subgraph GATE[\"\ud83d\udea6 Step 2: Scope Gate\"]\n        X{Outcome = Refer?}\n        Y[\"Return early\\nScore: 1.0\"]\n    end\n\n    subgraph RULES[\"\u2696\ufe0f Step 3: Trigger Detection\"]\n        E[Structured Checks]\n        F[Regex Scan]\n        G[LLM Fallback]\n        H[\"Detected Triggers\"]\n    end\n\n    subgraph VALIDATE[\"\u2713 Step 4: Refer Validation\"]\n        I{Triggers found?}\n        K[\"Score: 1.0\"]\n        Z[\"Score: 0.0\\nunknown_trigger\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; X\n    X --&gt;|No| Y\n    X --&gt;|Yes| E\n    B --&gt; E\n    A --&gt; F\n    E &amp; F --&gt; H\n    H --&gt; I\n    I --&gt;|Yes| K\n    I --&gt;|No triggers after structured/regex| G\n    G --&gt; H\n    I --&gt;|No triggers after fallback| Z\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style OUTCOME stroke:#3b82f6,stroke-width:2px\n    style GATE stroke:#64748b,stroke-width:2px\n    style RULES stroke:#f59e0b,stroke-width:2px\n    style VALIDATE stroke:#10b981,stroke-width:2px\n    style K fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre> <p> \u2705 Score = 1.0 Outcome is not <code>Refer</code> (skipped), OR outcome is <code>Refer</code> and at least one trigger is detected. </p> <p> \u274c Score = 0.0 Outcome is <code>Refer</code> but no trigger could be detected (falls back to <code>unknown_trigger</code>). </p> <p>Score Formula</p> <pre><code>if outcome_label != \"Refer\":\n    score = 1.0\nelse:\n    score = 1.0 if bool(detected_triggers) else 0.0\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#referral-triggers","title":"Referral Triggers","text":"<p>The metric detects the following referral triggers through structured data checks and regex patterns:</p>  Structured Rules Regex Rules Trigger Condition Severity bppValue BPP limit &gt; $250,000 Hard bppToSalesRatio BPP / sales &lt; 10% Soft numberOfEmployees Employees &gt; 20 Soft orgEstYear Business &lt; 3 years + building coverage Hard nonOwnedBuildingCoverage Building coverage requested but not owned Soft homeBasedBPP Home-based + contents-only Soft claimsHistory Prior claims count &gt; 0 Hard <p>Hard Severity:</p> <ul> <li><code>convStoreTemp</code> - Convenience/liquor/package store indicators (often tobacco/alcohol/lottery; sometimes 24/7 or fuel)</li> <li><code>claimsHistory</code> - Prior claims mentions</li> <li><code>orgEstYear</code> - New business + building coverage indicators</li> <li><code>bppValue</code> - Excessive BPP mentions</li> </ul> <p>Soft Severity:</p> <ul> <li><code>bppToSalesRatio</code> - Low ratio indicators</li> <li><code>nonOwnedBuildingCoverage</code> - Tenant building coverage / lease (including NNN/triple-net language)</li> <li><code>businessNOC</code> - Not Otherwise Classified</li> <li><code>homeBasedBPP</code> - Home-based business indicators</li> <li><code>numberOfEmployees</code> - High employee count (eligibility review)</li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>recommendation_column_name</code> <code>str</code> <code>brief_recommendation</code> Field in additional_output to analyze <p>LLM Fallback</p> <p>When a Refer outcome is detected but no triggers are found, the metric uses an LLM classifier to infer the closest trigger category.</p> <p>If the LLM cannot map to a known trigger, the metric records <code>unknown_trigger</code> and scores the case as <code>0.0</code>.</p> <p>The LLM classifier prompt is generated from the same <code>TRIGGER_SPECS</code> catalog used by regex detection so trigger descriptions stay in sync.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#code-examples","title":"Code Examples","text":"Basic Usage Multiple Triggers <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_rules import UnderwritingRules\n\nmetric = UnderwritingRules()\n\nitem = DatasetItem(\n    actual_output=\"Recommend Refer due to high BPP coverage request.\",\n    additional_input={\n        \"context_data\": {\n            \"auxData\": {\n                \"rateData\": {\n                    \"output\": {\n                        \"input\": {\n                            \"bop_bpp_limit\": 300000  # &gt; $250k threshold\n                        }\n                    }\n                }\n            }\n        }\n    }\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (referral with trigger present)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_rules import UnderwritingRules\n\nmetric = UnderwritingRules()\n\nitem = DatasetItem(\n    actual_output=\"Refer - prior claims and new business.\",\n    additional_input={\n        \"bop_number_of_claims\": 2,\n        \"bop_business_year_established\": 2024,  # &lt; 3 years\n        \"bop_insure_building\": \"building\"\n    }\n)\n\nresult = await metric.execute(item)\n# Score: 1.0 (refer with multiple triggers)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca TriggerReport Structure <pre><code>TriggerReport(\n{\n    \"is_referral\": true,\n    \"active_triggers\": [\n        {\n            \"trigger_name\": \"bppValue\",\n            \"detection_method\": \"regex\",\n            \"context\": \"bop_bpp_limit=300000\",\n            \"confidence\": 1.0\n        }\n    ],\n    \"primary_referral_reason\": \"bppValue\",\n    \"summary_text\": \"bppValue\",\n    \"outcome_label\": \"Refer\",\n    \"trigger_count\": 1,\n    \"llm_fallback_used\": false,\n    \"min_confidence\": 1.0,\n    \"has_hard_trigger\": true,\n    \"unknown_reasoning\": null\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>is_referral</code> <code>bool</code> Whether outcome is referral/decline <code>active_triggers</code> <code>List[TriggerEvent]</code> Detected triggers (<code>trigger_name</code>, <code>detection_method</code>, <code>context</code>, <code>confidence</code>) <code>primary_referral_reason</code> <code>TriggerName</code> Most significant trigger (by priority) <code>summary_text</code> <code>str</code> Comma-separated trigger names <code>outcome_label</code> <code>str</code> Normalized outcome label <code>trigger_count</code> <code>int</code> Number of triggers detected <code>llm_fallback_used</code> <code>bool</code> Whether LLM classifier was invoked <code>min_confidence</code> <code>float</code> Minimum confidence across all triggers <code>has_hard_trigger</code> <code>bool</code> Whether any hard-severity trigger was detected <code>unknown_reasoning</code> <code>str \\| None</code> LLM explanation when no trigger matched"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#example-scenarios","title":"Example Scenarios","text":"Pass (1.0) - ReferralPass (1.0) - ApprovalFail (0.0) <p>Referral Matches Triggers</p> <p>Recommendation:</p> <p>\"Refer to underwriting - BPP coverage of $300,000 exceeds threshold.\"</p> <p>Source Data: <pre><code>{\"bop_bpp_limit\": 300000}\n</code></pre></p> <p>Analysis:</p> Component Finding Outcome Referral Trigger bppValue (BPP &gt; $250k) Match \u2705 Referral with trigger <p>Final Score: <code>1.0</code> </p> <p>Skipped (Not in Scope)</p> <p>Recommendation:</p> <p>\"Approve - all criteria within guidelines.\"</p> <p>Source Data: <pre><code>{\n    \"bop_bpp_limit\": 150000,\n    \"bop_number_of_claims\": 0,\n    \"bop_number_of_employees\": 10\n}\n</code></pre></p> <p>Analysis:</p> Component Finding Outcome Approved (not <code>Refer</code>) Metric behavior Returns early; does not run trigger detection <p>Final Score: <code>1.0</code> </p> <p>Unknown Trigger</p> <p>Recommendation:</p> <p>\"Refer to underwriting for review.\"</p> <p>Source Data: <pre><code>{\"bop_bpp_limit\": 150000}\n</code></pre></p> <p>Analysis:</p> Component Finding Outcome Refer Triggers None detected Result \u274c <code>unknown_trigger</code> <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#why-it-matters","title":"Why It Matters","text":"\ud83d\udccb Guideline Compliance <p>Ensures AI follows established underwriting rules and thresholds.</p> \ud83d\udd0d Audit Trail <p>Tracks exactly which triggers led to referral decisions.</p> \u26a0\ufe0f Risk Detection <p>Catches <code>Refer</code> cases that can\u2019t be mapped to a known trigger (<code>unknown_trigger</code>).</p>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Underwriting Rules = For <code>Refer</code> outcomes, did we detect a valid referral trigger?</p> <ul> <li>Use it when: Validating that AI follows underwriting guidelines</li> <li>Score interpretation: 1.0 = not <code>Refer</code> (skipped) or <code>Refer</code> with trigger; 0.0 = <code>Refer</code> with no trigger (<code>unknown_trigger</code>)</li> <li>Key feature: Multi-stage detection (structured + regex + LLM fallback)</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Decision Quality \u00b7 Refer Reason</p> </li> </ul>"},{"location":"metric-registry/shared/","title":"Shared Metrics","text":"Cross-implementation metrics for common workflows 10 Metrics Slack KPIs <p>Shared metrics are used across implementations. Today this registry focuses on Slack conversation analytics and KPI reporting.</p> Slack Metrics <p>Interaction, engagement, escalation, and compliance signals</p> <code>conversation</code> <code>additional_input</code>"},{"location":"metric-registry/shared/slack/","title":"Slack Metrics","text":"<p> 5 Analyzers + 1 Composite Orchestrator for Slack conversation analytics. Split architecture separates heuristic pattern matching, objective factual analysis, subjective sentiment assessment, product insight extraction, and root-cause feedback attribution. </p> <p>This module provides metrics for analyzing Slack conversations between users and AI assistants (e.g., Athena). These metrics support KPI computation for measuring AI assistant effectiveness in underwriting workflows.</p>"},{"location":"metric-registry/shared/slack/#analyzer-overview","title":"Analyzer Overview","text":"H <p>Heuristic Analyzer</p> <p>Zero-cost pattern matching \u2014 interaction counts, engagement depth, recommendation detection. No LLM required.</p> O <p>Objective Analyzer</p> <p>LLM factual analysis (temp 0.0) \u2014 escalation, intervention, and resolution classification.</p> S <p>Subjective Analyzer</p> <p>LLM sentiment &amp; quality (temp 0.3) \u2014 sentiment, frustration, acceptance, override, satisfaction.</p> P <p>Product Analyzer</p> <p>Product insight extraction \u2014 learnings, feature requests, priority classification.</p> F <p>Feedback Attribution</p> <p>Root cause diagnosis \u2014 attributes failures to classification, data, rules, tooling, or interface.</p> C <p>Composite Orchestrator</p> <p>Orchestrates all analyzers in dependency order with conditional execution and context passing.</p>"},{"location":"metric-registry/shared/slack/#architecture","title":"Architecture","text":"Analyzer Class Type LLM Temp Signals Heuristic <code>SlackHeuristicAnalyzer</code> Pattern matching None Interaction, Engagement, Recommendation Objective <code>SlackObjectiveAnalyzer</code> LLM classification 0.0 Escalation, Intervention, Resolution Subjective <code>SlackSubjectiveAnalyzer</code> LLM assessment 0.3 Sentiment, Frustration, Acceptance, Override Product <code>SlackProductAnalyzer</code> LLM extraction 0.3 Learnings, Feature Requests, Priority Feedback <code>SlackFeedbackAttributionAnalyzer</code> LLM attribution 0.2 Failed Step, Remediation Composite <code>UnderwritingCompositeEvaluator</code> Orchestrator Delegates All of the above"},{"location":"metric-registry/shared/slack/#pipeline-flow","title":"Pipeline Flow","text":"<p>The composite orchestrator runs analyzers in strict dependency order:</p> <pre><code>Objective (always) \u2192 Subjective (if human messages) \u2192 Feedback (conditional) \u2192 Product (always*)\n</code></pre> <p>Feedback attribution only fires when friction is detected (<code>has_intervention</code> OR <code>is_escalated</code> OR <code>frustration_score &gt; 0.5</code>), saving LLM costs on positive interactions.</p>"},{"location":"metric-registry/shared/slack/#kpis","title":"KPIs","text":"KPI Source Analyzer Formula <code>interaction_rate</code> Heuristic Interactive threads / Total eligible cases <code>MAU</code> Heuristic Unique senders in 30 days <code>engagement_rate</code> Heuristic Avg interactions per case <code>stp_rate</code> Objective Threads with no intervention / Total <code>escalation_rate</code> Objective Escalated cases / Total AI cases <code>intervention_rate</code> Objective Threads with intervention / Total <code>resolution_rate</code> Objective Resolved / Total <code>stalemate_rate</code> Objective Stalemates / Total <code>frustration_rate</code> Subjective Frustrated interactions / Total <code>acceptance_rate</code> Subjective Accepted recommendations / Total <code>override_rate</code> Subjective Overridden recommendations / Total <code>override_satisfaction</code> Subjective Satisfactory overrides / Total overrides"},{"location":"metric-registry/shared/slack/#utility-functions","title":"Utility Functions","text":"<p>File: <code>utils.py</code></p> <p>Common utilities used across metrics:</p> Function Description <code>parse_slack_metadata()</code> Extract thread_ts, channel_id, sender from additional_input <code>extract_mentions()</code> Extract @mentions from message text <code>get_human_messages()</code> Filter human messages from conversation <code>get_ai_messages()</code> Filter AI messages from conversation <code>find_recommendation_turn()</code> Find turn containing AI recommendation <code>extract_recommendation_type()</code> Extract approve/decline/review/hold <code>extract_case_id()</code> Extract case ID (MGT-BOP-XXXXXXX) <code>extract_priority_score()</code> Extract base/priority score <code>count_questions()</code> Count questions in text <code>build_transcript()</code> Build plain text transcript from conversation <code>analyze_reactions()</code> Analyze emoji reactions on messages <code>detect_stalemate()</code> Detect repeated bot messages <code>calculate_time_to_resolution()</code> Calculate time between first and last message"},{"location":"metric-registry/shared/slack/composite/","title":"Composite Orchestrator","text":"<p> The \"General Manager\" \u2014 orchestrates all analyzers in strict dependency order. Manages context passing between stages, conditional execution of expensive analyses, and flattened sub-metric output for unified reporting. </p> At a Glance Orchestrator Class <code>UnderwritingCompositeEvaluator</code> Base <code>BaseMetric</code> Source <code>implementations/athena/metrics/slack/composite.py</code> LLM Delegates to children Pipeline Objective \u2192 Subjective \u2192 Feedback (conditional) \u2192 Product"},{"location":"metric-registry/shared/slack/composite/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>graph TD\n    A[DatasetItem] --&gt; B[SlackObjectiveAnalyzer]\n    B --&gt;|escalation, intervention,&lt;br/&gt;resolution| C{Has human&lt;br/&gt;messages?}\n    C --&gt;|No| G[Return objective only]\n    C --&gt;|Yes| D[SlackSubjectiveAnalyzer]\n    D --&gt;|sentiment, frustration,&lt;br/&gt;acceptance, override| E{Friction&lt;br/&gt;detected?}\n    E --&gt;|Yes| F[SlackFeedbackAttributionAnalyzer]\n    E --&gt;|No| H[SlackProductAnalyzer]\n    F --&gt;|failed_step,&lt;br/&gt;remediation| H\n    H --&gt;|learnings,&lt;br/&gt;feature_requests| I[UnderwritingCompositeResult]\n    G --&gt; I\n\n    style B fill:#1E3A5F,color:#fff\n    style D fill:#1E3A5F,color:#fff\n    style F fill:#2D5F8A,color:#fff\n    style H fill:#1E3A5F,color:#fff\n    style I fill:#0F2440,color:#fff</code></pre>"},{"location":"metric-registry/shared/slack/composite/#execution-order","title":"Execution Order","text":"<ol> <li>Objective \u2014 Always runs first. Determines escalation, intervention, and resolution (temp 0.0)</li> <li>Check \u2014 If no human messages, skip remaining analyzers</li> <li>Subjective \u2014 Runs with objective context. Determines sentiment, frustration, acceptance, override (temp 0.3)</li> <li>Feedback \u2014 Conditional. Only runs if: <code>has_intervention</code> OR <code>is_escalated</code> OR <code>frustration_score &gt; 0.5</code> (temp 0.2)</li> <li>Product \u2014 Always runs (when human messages exist). Extracts learnings and feature requests (temp 0.3)</li> </ol>"},{"location":"metric-registry/shared/slack/composite/#temperature-strategy","title":"Temperature Strategy","text":"Analyzer Temperature Rationale Objective 0.0 Deterministic factual classification Subjective 0.3 Nuanced sentiment interpretation Feedback 0.2 Focused root-cause attribution Product 0.3 Creative insight extraction"},{"location":"metric-registry/shared/slack/composite/#constructor","title":"Constructor","text":"<pre><code>from eval_workbench.implementations.athena.metrics.slack.composite import (\n    UnderwritingCompositeEvaluator,\n)\n\nevaluator = UnderwritingCompositeEvaluator(\n    config=None,    # Optional[AnalyzerConfig]\n)\n</code></pre> <p>The composite automatically initializes all four child analyzers internally.</p>"},{"location":"metric-registry/shared/slack/composite/#output","title":"Output","text":""},{"location":"metric-registry/shared/slack/composite/#underwritingcompositeresult","title":"UnderwritingCompositeResult","text":"Field Type Description <code>objective</code> ObjectiveAnalysisResult Escalation, intervention, resolution signals <code>subjective</code> SubjectiveAnalysisOutput Sentiment, frustration, acceptance, override signals <code>feedback</code> FeedbackAttributionOutput Root cause attribution (None if skipped) <code>product</code> ProductSignalsOutput Learnings and feature requests"},{"location":"metric-registry/shared/slack/composite/#sub-metrics","title":"Sub-Metrics","text":"<p>The composite flattens all child analyzer sub-metrics into a single list for unified reporting.</p> <p>Objective</p> cls escalation_type Escalation type cls intervention_type Intervention category cls resolution_status Final thread status <p>Subjective</p> cls sentiment_category Sentiment label cls frustration_cause Frustration root cause cls acceptance_status Recommendation acceptance cls override_type Override classification 0\u20131 satisfaction_score Override explanation quality <p>Feedback (only if feedback ran)</p> cls failed_step Pipeline stage attribution cls attribution_confidence Confidence in attribution <p>Product</p> 0\u20131 learnings Learnings extracted 0\u20131 feature_requests Feature requests identified cls actionable_feedback Actionable feedback exists cls priority_level Priority classification"},{"location":"metric-registry/shared/slack/composite/#usage","title":"Usage","text":"<pre><code>from eval_workbench.implementations.athena.metrics.slack.composite import (\n    UnderwritingCompositeEvaluator,\n)\n\nevaluator = UnderwritingCompositeEvaluator()\nresult = await evaluator.execute(dataset_item)\n\n# Access nested signals\nprint(f\"STP: {result.signals.objective.intervention.is_stp}\")\nprint(f\"Sentiment: {result.signals.subjective.sentiment}\")\n\nif result.signals.feedback:\n    print(f\"Root cause: {result.signals.feedback.failed_step}\")\n\nprint(f\"Learnings: {result.signals.product.learnings}\")\n\n# Flatten all sub-metrics for reporting\nsub_metrics = evaluator.get_sub_metrics(result)\nfor sm in sub_metrics:\n    print(f\"  {sm.metric_name}: {sm.value}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/composite/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>metrics_config:\n  UnderwritingComposite:\n    class: \"underwriting_composite_evaluator\"\n    llm_provider: \"openai\"\n    model_name: \"gpt-5.2\"\n</code></pre>"},{"location":"metric-registry/shared/slack/composite/#kpis-supported","title":"KPIs Supported","text":"<p>All KPIs from child analyzers are available through the composite:</p> <ul> <li>Operational: <code>stp_rate</code>, <code>intervention_rate</code>, <code>escalation_rate</code></li> <li>Quality: <code>acceptance_rate</code>, <code>override_rate</code>, <code>override_satisfaction</code></li> <li>Sentiment: <code>frustration_rate</code>, sentiment distribution</li> <li>Resolution: <code>resolution_rate</code>, <code>stalemate_rate</code></li> <li>Product: Learnings, feature requests, priority distribution</li> </ul>"},{"location":"metric-registry/shared/slack/feedback/","title":"Feedback Attribution Analyzer","text":"<p> Root cause diagnosis for negative feedback scenarios. Attributes failures to specific pipeline stages \u2014 AI classification, third-party data, rule engine, platform tooling, or chat interface. Conditional execution: only runs when friction is detected. </p> At a Glance LLM \u00b7 temp 0.2 Conditional Class <code>SlackFeedbackAttributionAnalyzer</code> Base <code>BaseMetric</code> Source <code>shared/metrics/slack/feedback.py</code> Runs when Negative feedback detected Signals <code>FeedbackAttributionOutput</code> \u2014 failed_step, failure_evidence, remediation_hint"},{"location":"metric-registry/shared/slack/feedback/#constructor","title":"Constructor","text":"<pre><code>from eval_workbench.shared.metrics.slack.feedback import SlackFeedbackAttributionAnalyzer\n\nanalyzer = SlackFeedbackAttributionAnalyzer(\n    config=None,                # Optional[AnalyzerConfig]\n    analysis_context=None,      # Optional[Dict[str, Any]] \u2014 from other analyses\n    sentiment_threshold=0.4,    # Sentiment cutoff for triggering analysis\n)\n</code></pre> <p>Temperature is forced to <code>0.2</code> for focused, deterministic attribution.</p>"},{"location":"metric-registry/shared/slack/feedback/#signals","title":"Signals","text":""},{"location":"metric-registry/shared/slack/feedback/#feedbackattributionoutput","title":"FeedbackAttributionOutput","text":"Signal Type Description <code>has_negative_feedback</code> bool Whether negative feedback was detected <code>failed_step</code> str Pipeline stage where failure occurred (see below) <code>failure_evidence</code> str Evidence supporting the attribution <code>confidence</code> str high / medium / low <code>remediation_hint</code> str Suggested fix (optional) <code>reasoning_trace</code> str LLM reasoning chain"},{"location":"metric-registry/shared/slack/feedback/#failed-step-categories","title":"Failed Step Categories","text":"Step Description <code>classification_failure</code> AI classification or recommendation was wrong <code>data_integrity_failure</code> Third-party data (e.g., Magic Dust) was inaccurate <code>rule_engine_failure</code> Rule engine applied rules incorrectly <code>system_tooling_failure</code> Platform tooling (Socotra, SFX) issues <code>chat_interface</code> Slack formatting or UX problems <code>unknown</code> Cannot determine root cause"},{"location":"metric-registry/shared/slack/feedback/#conditional-execution","title":"Conditional Execution","text":"<p>The feedback analyzer is designed to run only when friction is detected. In the composite pipeline, it fires when any of these conditions are true:</p> <ul> <li><code>has_intervention</code> is <code>True</code></li> <li><code>is_escalated</code> is <code>True</code></li> <li><code>frustration_score &gt; 0.5</code></li> </ul> <p>This prevents unnecessary LLM calls on positive interactions.</p>"},{"location":"metric-registry/shared/slack/feedback/#analysis-context","title":"Analysis Context","text":"<p>The analyzer receives context from upstream analyses:</p> <ul> <li><code>sentiment_score</code> \u2014 Numeric sentiment (0-1)</li> <li><code>frustration_score</code> \u2014 Frustration level</li> <li><code>frustration_cause</code> \u2014 Root cause from subjective analysis</li> <li><code>has_intervention</code> \u2014 Whether human intervened</li> <li><code>intervention_type</code> \u2014 Type of intervention</li> </ul>"},{"location":"metric-registry/shared/slack/feedback/#sub-metrics","title":"Sub-Metrics","text":"cls failed_step Pipeline stage attribution cls attribution_confidence Confidence in attribution <p>Only emitted when <code>has_negative_feedback</code> is <code>true</code>.</p>"},{"location":"metric-registry/shared/slack/feedback/#usage","title":"Usage","text":"<pre><code>from eval_workbench.shared.metrics.slack.feedback import SlackFeedbackAttributionAnalyzer\n\nanalyzer = SlackFeedbackAttributionAnalyzer(\n    analysis_context={\n        \"sentiment_score\": 0.3,\n        \"frustration_score\": 0.7,\n        \"frustration_cause\": \"ai_error\",\n        \"has_intervention\": True,\n        \"intervention_type\": \"correction\",\n    }\n)\nresult = await analyzer.execute(dataset_item)\n\nsignals = result.signals\nif signals.has_negative_feedback:\n    print(f\"Failed step: {signals.failed_step}\")\n    print(f\"Evidence: {signals.failure_evidence}\")\n    print(f\"Fix: {signals.remediation_hint}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/feedback/#kpis-supported","title":"KPIs Supported","text":"<ul> <li>Failure attribution distribution across pipeline stages</li> <li>Root cause analysis for negative feedback trends</li> <li>Remediation tracking and prioritization</li> </ul>"},{"location":"metric-registry/shared/slack/heuristic/","title":"Heuristic Analyzer","text":"<p> Zero-cost pattern matching \u2014 no LLM required. Computes interaction metrics, engagement depth, recommendation detection, emoji reaction sentiment, and bot message repetition (stalemate). </p> At a Glance No LLM Class <code>SlackHeuristicAnalyzer</code> Base <code>BaseMetric</code> Source <code>shared/metrics/slack/heuristic.py</code> Temperature N/A Signals <code>InteractionSignals</code> \u00b7 <code>EngagementSignals</code> \u00b7 <code>RecommendationSignals</code> \u00b7 <code>ReactionSignals</code> \u00b7 <code>StalemateSignals</code>"},{"location":"metric-registry/shared/slack/heuristic/#constructor","title":"Constructor","text":"<pre><code>from eval_workbench.shared.metrics.slack.heuristic import SlackHeuristicAnalyzer\n\nanalyzer = SlackHeuristicAnalyzer(\n    config=None,                # Optional[AnalyzerConfig]\n    include_reactions=True,     # Analyze emoji reactions\n    include_stalemate=True,     # Detect repeated bot messages\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/heuristic/#signals","title":"Signals","text":""},{"location":"metric-registry/shared/slack/heuristic/#interactionsignals","title":"InteractionSignals","text":"Signal Type Description <code>ai_message_count</code> int Number of AI messages <code>human_message_count</code> int Number of human messages <code>total_turn_count</code> int Total messages in thread <code>reply_count</code> int Number of replies <code>is_ai_initiated</code> bool Whether AI sent first message <code>has_human_response</code> bool Whether humans responded to AI <code>is_interactive</code> bool Has both AI and human participation"},{"location":"metric-registry/shared/slack/heuristic/#engagementsignals","title":"EngagementSignals","text":"Signal Type Description <code>interaction_depth</code> int Number of back-and-forth exchanges <code>has_multiple_interactions</code> bool More than one human message <code>avg_human_response_length</code> float Average human message length (chars) <code>avg_ai_response_length</code> float Average AI message length (chars) <code>question_count</code> int Total questions asked <code>mention_count</code> int Total @mentions <code>unique_participants</code> int Count of unique human participants"},{"location":"metric-registry/shared/slack/heuristic/#recommendationsignals","title":"RecommendationSignals","text":"Signal Type Description <code>has_recommendation</code> bool Whether AI made a recommendation <code>recommendation_type</code> str Type: approve, decline, review, hold, none <code>recommendation_turn_index</code> int Turn where recommendation was made <code>recommendation_confidence</code> float Extracted confidence level (0-1) <code>case_id</code> str Case identifier (e.g., MGT-BOP-123456) <code>case_priority</code> int Priority/base score (0-100)"},{"location":"metric-registry/shared/slack/heuristic/#reactionsignals-optional","title":"ReactionSignals (optional)","text":"<p>Computed when <code>include_reactions=True</code>. Analyzes emoji reactions on messages.</p>"},{"location":"metric-registry/shared/slack/heuristic/#stalematesignals-optional","title":"StalemateSignals (optional)","text":"<p>Computed when <code>include_stalemate=True</code>. Detects repeated/identical bot messages that may indicate a stuck conversation loop.</p>"},{"location":"metric-registry/shared/slack/heuristic/#sub-metrics","title":"Sub-Metrics","text":"0\u20131 interaction Interaction score from message counts 0\u20131 engagement Engagement depth score cls recommendation_type Detected recommendation type 0\u20131 reaction_sentiment Reaction sentiment (if enabled) cls is_stalemate Bot repeating (if enabled)"},{"location":"metric-registry/shared/slack/heuristic/#usage","title":"Usage","text":"<pre><code>from eval_workbench.shared.metrics.slack.heuristic import SlackHeuristicAnalyzer\n\nanalyzer = SlackHeuristicAnalyzer()\nresult = await analyzer.execute(dataset_item)\n\n# Access signals\nsignals = result.signals\nprint(f\"Interactive: {signals.interaction.is_interactive}\")\nprint(f\"Depth: {signals.engagement.interaction_depth}\")\nprint(f\"Recommendation: {signals.recommendation.recommendation_type}\")\n\n# Get sub-metrics for reporting\nsub_metrics = analyzer.get_sub_metrics(result)\n</code></pre>"},{"location":"metric-registry/shared/slack/heuristic/#kpis-supported","title":"KPIs Supported","text":"<ul> <li><code>interaction_rate</code> \u2014 Interactive threads / Total eligible cases</li> <li><code>MAU</code> \u2014 Unique senders in 30 days</li> <li><code>engagement_rate</code> \u2014 Avg interactions per case or % with multiple interactions</li> <li><code>acceptance_rate</code> / <code>override_rate</code> \u2014 Via recommendation detection (foundation for downstream analyzers)</li> </ul>"},{"location":"metric-registry/shared/slack/objective/","title":"Objective Analyzer","text":"<p> LLM-powered factual analysis at temperature 0.0. Classifies escalations, human interventions, and resolution status. Distinguishes data/logic errors from system issues. Falls back to heuristic on LLM failure. </p> At a Glance LLM \u00b7 temp 0.0 Class <code>SlackObjectiveAnalyzer</code> Base <code>BaseMetric</code> Source <code>shared/metrics/slack/objective.py</code> Temperature 0.0 (deterministic) Signals <code>EscalationSignals</code> \u00b7 <code>InterventionSignals</code> \u00b7 <code>ResolutionSignals</code>"},{"location":"metric-registry/shared/slack/objective/#constructor","title":"Constructor","text":"<pre><code>from eval_workbench.shared.metrics.slack.objective import SlackObjectiveAnalyzer\n\nanalyzer = SlackObjectiveAnalyzer(\n    config=None,                # Optional[AnalyzerConfig]\n    truncation_config=None,     # Optional[TruncationConfig] \u2014 conversation truncation\n)\n</code></pre> <p>Temperature is forced to <code>0.0</code> for deterministic, reproducible classification.</p>"},{"location":"metric-registry/shared/slack/objective/#signals","title":"Signals","text":""},{"location":"metric-registry/shared/slack/objective/#escalationsignals","title":"EscalationSignals","text":"Signal Type Description <code>is_escalated</code> bool Whether conversation was escalated <code>escalation_type</code> str Type: no_escalation, team_mention, explicit_handoff, error_escalation, complexity_escalation <code>escalation_turn_index</code> int Turn where escalation occurred <code>escalation_targets</code> list @mentioned users during escalation <code>escalation_reason</code> str Reason for escalation"},{"location":"metric-registry/shared/slack/objective/#interventionsignals","title":"InterventionSignals","text":"Signal Type Description <code>has_intervention</code> bool Human intervened in the thread <code>intervention_type</code> str Category of intervention (correction, missing context, approval, etc.) <code>intervention_escalation</code> str Escalation class: hard / soft / authority / none <code>is_stp</code> bool Straight-through processing (no intervention/escalation) <code>intervention_summary</code> str Summary of what happened <code>friction_point</code> str Concept causing friction (optional) <code>issue_details</code> str Technical details (optional)"},{"location":"metric-registry/shared/slack/objective/#resolutionsignals","title":"ResolutionSignals","text":"Signal Type Description <code>final_status</code> str approved / declined / blocked / needs_info / stalemate / pending <code>is_resolved</code> bool Whether the thread reached a resolution <code>resolution_type</code> str How it was resolved <code>is_stalemate</code> bool Inactive beyond threshold <code>time_to_resolution_seconds</code> float Time from first to last message (optional)"},{"location":"metric-registry/shared/slack/objective/#internal-architecture","title":"Internal Architecture","text":"<p>The analyzer uses a two-stage pattern:</p> <ol> <li>Outer class (<code>SlackObjectiveAnalyzer</code>) handles orchestration, transcript building, and heuristic fallback</li> <li>Inner LLM class (<code>_ObjectiveLLMAnalyzer</code>) handles the actual LLM call with structured input/output models</li> </ol> <p>The LLM receives an <code>ObjectiveAnalysisInput</code> (transcript, recommendation context, mentions, human count) and returns an <code>ObjectiveAnalysisOutput</code> with escalation, intervention, and resolution fields.</p>"},{"location":"metric-registry/shared/slack/objective/#sub-metrics","title":"Sub-Metrics","text":"cls escalation_type Escalation type detected cls intervention_type Intervention category cls resolution_status Final thread status"},{"location":"metric-registry/shared/slack/objective/#usage","title":"Usage","text":"<pre><code>from eval_workbench.shared.metrics.slack.objective import SlackObjectiveAnalyzer\n\nanalyzer = SlackObjectiveAnalyzer()\nresult = await analyzer.execute(dataset_item)\n\nsignals = result.signals\nprint(f\"Escalated: {signals.escalation.is_escalated}\")\nprint(f\"STP: {signals.intervention.is_stp}\")\nprint(f\"Status: {signals.resolution.final_status}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/objective/#kpis-supported","title":"KPIs Supported","text":"<ul> <li><code>escalation_rate</code> \u2014 Escalated cases / Total AI cases</li> <li><code>intervention_rate</code> \u2014 Threads with intervention / Total threads</li> <li><code>stp_rate</code> \u2014 Threads with no intervention / Total threads</li> <li><code>resolution_rate</code> \u2014 Resolved / Total</li> <li><code>stalemate_rate</code> \u2014 Stalemates / Total</li> </ul>"},{"location":"metric-registry/shared/slack/product/","title":"Product Analyzer","text":"<p> Actionable product insights extraction via LLM. Identifies UX issues, accuracy problems, workflow friction, feature requests, and rule configuration needs from Slack conversations. </p> At a Glance LLM \u00b7 temp 0.3 Class <code>SlackProductAnalyzer</code> Base <code>BaseMetric</code> Source <code>shared/metrics/slack/product.py</code> Temperature 0.3 Signals <code>ProductSignalsOutput</code> \u2014 learnings, feature requests, priority"},{"location":"metric-registry/shared/slack/product/#constructor","title":"Constructor","text":"<pre><code>from eval_workbench.shared.metrics.slack.product import SlackProductAnalyzer\n\nanalyzer = SlackProductAnalyzer(\n    config=None,                # Optional[AnalyzerConfig]\n    analysis_context=None,      # Optional[Dict[str, Any]] \u2014 from other analyses\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/product/#signals","title":"Signals","text":""},{"location":"metric-registry/shared/slack/product/#productsignalsoutput","title":"ProductSignalsOutput","text":"Signal Type Description <code>learnings</code> list[str] Actionable insights extracted from conversation <code>learning_categories</code> list[str] Categories: ux, accuracy, coverage, speed, workflow, rules, guardrails, other <code>feature_requests</code> list[str] Explicit or implicit feature requests <code>has_actionable_feedback</code> bool Whether actionable feedback was found <code>priority_level</code> str high / medium / low / none <code>suggested_action</code> str Recommended next step (optional) <code>reasoning_trace</code> str LLM reasoning chain"},{"location":"metric-registry/shared/slack/product/#learning-categories","title":"Learning Categories","text":"Category Description <code>ux</code> User interface / experience issues <code>accuracy</code> Incorrect or imprecise outputs <code>coverage</code> Missing knowledge or capabilities <code>speed</code> Performance / response time concerns <code>workflow</code> Process or workflow friction <code>rules</code> Rule configuration needs <code>guardrails</code> Safety or boundary issues <code>other</code> Uncategorized feedback"},{"location":"metric-registry/shared/slack/product/#analysis-context","title":"Analysis Context","text":"<p>The product analyzer can receive context from other analyses to enrich its extraction:</p> <ul> <li><code>has_intervention</code> \u2014 Whether human intervention occurred</li> <li><code>intervention_type</code> \u2014 Type of intervention</li> <li><code>is_frustrated</code> \u2014 Whether frustration was detected</li> <li><code>sentiment</code> \u2014 User sentiment label</li> </ul>"},{"location":"metric-registry/shared/slack/product/#sub-metrics","title":"Sub-Metrics","text":"0\u20131 learnings Number of learnings extracted 0\u20131 feature_requests Feature requests identified cls actionable_feedback Actionable feedback exists cls priority_level Priority classification"},{"location":"metric-registry/shared/slack/product/#usage","title":"Usage","text":"<pre><code>from eval_workbench.shared.metrics.slack.product import SlackProductAnalyzer\n\nanalyzer = SlackProductAnalyzer()\nresult = await analyzer.execute(dataset_item)\n\nsignals = result.signals\nprint(f\"Learnings: {signals.learnings}\")\nprint(f\"Priority: {signals.priority_level}\")\nprint(f\"Feature requests: {signals.feature_requests}\")\n\n# With context from other analyses\nanalyzer = SlackProductAnalyzer(\n    analysis_context={\n        \"has_intervention\": True,\n        \"intervention_type\": \"correction\",\n        \"is_frustrated\": False,\n        \"sentiment\": \"neutral\",\n    }\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/product/#kpis-supported","title":"KPIs Supported","text":"<ul> <li>Product insight extraction for daily/weekly reports</li> <li>Feature request tracking and prioritization</li> <li>Friction point identification across conversations</li> </ul>"},{"location":"metric-registry/shared/slack/subjective/","title":"Subjective Analyzer","text":"<p> LLM-powered sentiment and quality assessment at temperature 0.3. Nuanced analysis of user sentiment, frustration, acceptance, override decisions, and satisfaction. Receives objective context to inform assessment. </p> At a Glance LLM \u00b7 temp 0.3 Class <code>SlackSubjectiveAnalyzer</code> Base <code>BaseMetric</code> Source <code>shared/metrics/slack/subjective.py</code> Depends on Objective Analyzer Signals <code>SubjectiveAnalysisOutput</code> \u2014 sentiment, frustration, acceptance, override, satisfaction"},{"location":"metric-registry/shared/slack/subjective/#constructor","title":"Constructor","text":"<pre><code>from eval_workbench.shared.metrics.slack.subjective import SlackSubjectiveAnalyzer\n\nanalyzer = SlackSubjectiveAnalyzer(\n    config=None,                # Optional[AnalyzerConfig]\n    objective_context=None,     # Optional[Dict[str, Any]] \u2014 from objective analysis\n)\n</code></pre> <p>Temperature is forced to <code>0.3</code> for nuanced but semi-consistent outputs.</p>"},{"location":"metric-registry/shared/slack/subjective/#signals","title":"Signals","text":""},{"location":"metric-registry/shared/slack/subjective/#sentiment","title":"Sentiment","text":"Signal Type Description <code>sentiment</code> str positive / neutral / frustrated / confused <code>sentiment_trajectory</code> str improving / stable / worsening <code>sentiment_indicators</code> list Detected sentiment signals"},{"location":"metric-registry/shared/slack/subjective/#frustration","title":"Frustration","text":"Signal Type Description <code>frustration_cause</code> str ai_error / data_quality / tooling_friction / rule_rigidity / slow_response / other <code>frustration_indicators</code> list Specific frustration signals <code>peak_frustration_turn</code> int Turn with highest frustration"},{"location":"metric-registry/shared/slack/subjective/#acceptance","title":"Acceptance","text":"Signal Type Description <code>acceptance_status</code> str accepted / accepted_with_discussion / pending / rejected / modified <code>is_accepted</code> bool Whether recommendation was accepted <code>acceptance_turn_index</code> int Turn where decision was made <code>decision_maker</code> str Who made the final decision"},{"location":"metric-registry/shared/slack/subjective/#override","title":"Override","text":"Signal Type Description <code>is_overridden</code> bool Whether recommendation was overridden <code>override_type</code> str no_override / full_override / partial_override / pending_override <code>final_decision</code> str What was actually decided <code>override_reason</code> str Stated reason for override <code>override_reason_category</code> str additional_info / risk_assessment / policy_exception / experience_judgment / etc."},{"location":"metric-registry/shared/slack/subjective/#satisfaction","title":"Satisfaction","text":"Signal Type Description <code>satisfaction_score</code> float Quality score (0.0 - 1.0) <code>has_clear_reason</code> bool Has stated reason for override <code>has_supporting_evidence</code> bool Cites specific information <code>is_actionable</code> bool Provides actionable feedback <code>improvement_suggestions</code> list Suggestions for AI improvement"},{"location":"metric-registry/shared/slack/subjective/#objective-context","title":"Objective Context","text":"<p>The subjective analyzer receives context from the objective analysis to inform its assessment. These fields are passed as input to the LLM:</p> <ul> <li><code>objective_is_escalated</code> \u2014 Whether escalation was detected</li> <li><code>objective_has_intervention</code> \u2014 Whether human intervention occurred</li> <li><code>objective_intervention_type</code> \u2014 Type of intervention</li> <li><code>objective_final_status</code> \u2014 Resolution status</li> </ul> <p>This prevents the subjective analyzer from contradicting factual findings.</p>"},{"location":"metric-registry/shared/slack/subjective/#sub-metrics","title":"Sub-Metrics","text":"cls sentiment_category Detected sentiment label cls frustration_cause Root cause of frustration cls acceptance_status Recommendation acceptance cls override_type Override classification 0\u20131 satisfaction_score Override explanation quality"},{"location":"metric-registry/shared/slack/subjective/#usage","title":"Usage","text":"<pre><code>from eval_workbench.shared.metrics.slack.subjective import SlackSubjectiveAnalyzer\n\n# Standalone usage\nanalyzer = SlackSubjectiveAnalyzer()\nresult = await analyzer.execute(dataset_item)\n\n# With objective context (typical in composite pipeline)\nanalyzer = SlackSubjectiveAnalyzer(\n    objective_context={\n        \"is_escalated\": False,\n        \"has_intervention\": True,\n        \"intervention_type\": \"correction\",\n        \"final_status\": \"approved\",\n    }\n)\nresult = await analyzer.execute(dataset_item)\n\nsignals = result.signals\nprint(f\"Sentiment: {signals.sentiment} ({signals.sentiment_trajectory})\")\nprint(f\"Override: {signals.override_type}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/subjective/#kpis-supported","title":"KPIs Supported","text":"<ul> <li><code>frustration_rate</code> \u2014 Frustrated interactions / Total interactions</li> <li><code>acceptance_rate</code> \u2014 Recommendations accepted / Total recommendations</li> <li><code>override_rate</code> \u2014 Recommendations overridden / Total recommendations</li> <li><code>override_satisfaction</code> \u2014 Satisfactory overrides / Total overrides</li> <li>Sentiment distribution (positive / neutral / frustrated / confused)</li> </ul>"}]}