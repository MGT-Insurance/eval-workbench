{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Eval Workbench \u2013 MGT AI Evaluation","text":"<p>This repository contains specific implementations built on top of the Axion or any other Evaluation Module. This architecture separates the core evaluation framework from individual evaluation implementations, enabling better tracking, ability to create custom sharable tooling, easier sharing, and a clear separation of concerns.</p>"},{"location":"#architecture","title":"architecture","text":"<p>Get the full repo-level architecture overview, system diagram, and runtime flows: Architecture overview</p>"},{"location":"#environment-variables","title":"environment variables","text":"<p>Environment variables are loaded with Pydantic Settings in two layers, with later files overriding earlier ones:</p> <ul> <li>1) Repo root <code>.env</code> provides global defaults.</li> <li>2) <code>implementations/&lt;name&gt;/.env</code> provides per-implementation overrides.</li> </ul> <p>Use <code>.env.example</code> files as the canonical templates for what each layer supports. When you add a new setting, define it on a settings class that inherits <code>shared.settings.RepoSettingsBase</code> and sets <code>model_config</code> via <code>build_settings_config(from_path=Path(__file__))</code>, then document the value in the relevant <code>.env.example</code>.</p>"},{"location":"#langfuse-settings","title":"langfuse settings","text":"<p>Prompt management uses these Langfuse settings: <pre><code>LANGFUSE_PUBLIC_KEY=\"\"\nLANGFUSE_SECRET_KEY=\"\"\nLANGFUSE_HOST=\"\"\nLANGFUSE_DEFAULT_LABEL=\"production\"\nLANGFUSE_DEFAULT_CACHE_TTL_SECONDS=60\n</code></pre></p>"},{"location":"#database-settings","title":"database settings","text":"<p>The Neon/Postgres helper reads these variables (all optional unless your app needs a DB connection): <pre><code>DATABASE_URL=\"\"\nDB_POOL_MIN_SIZE=0\nDB_POOL_MAX_SIZE=20\nDB_CONNECT_TIMEOUT_SECONDS=10\nDB_STATEMENT_TIMEOUT_MS=60000\nDB_USE_STARTUP_STATEMENT_TIMEOUT=false\nDB_APPLICATION_NAME=\"\"\nDB_UPLOAD_CHUNK_SIZE=1000\n</code></pre></p>"},{"location":"#notebook-imports","title":"notebook imports","text":"<p>For notebooks under any <code>src/implementations/&lt;name&gt;/notebooks/</code>, install this repo in editable mode so <code>shared</code> and <code>implementations.&lt;name&gt;</code> imports work without <code>sys.path</code> hacks: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"#prompt-patterns","title":"prompt patterns","text":"<p>Prompt extraction uses an explicit strategy passed to <code>Trace</code> or <code>TraceCollection</code>. For Athena: <pre><code>from eval_workbench.implementations.athena.langfuse.prompt_patterns import (\n    ChatPromptPatterns,\n    WorkflowPromptPatterns,\n)\nfrom eval_workbench.shared.langfuse.trace import TraceCollection\n\nchat_traces = TraceCollection(data, prompt_patterns=ChatPromptPatterns)\nrecommendation_traces = TraceCollection(data, prompt_patterns=WorkflowPromptPatterns)\n</code></pre></p>"},{"location":"#langfuse-webhooks","title":"langfuse webhooks","text":"<p>Use a webhook to invalidate prompt cache on updates: <pre><code>export LANGFUSE_PUBLIC_KEY=\"...\"\nexport LANGFUSE_SECRET_KEY=\"...\"\nexport LANGFUSE_WEBHOOK_SECRET=\"whsec_...\"\nexport LANGFUSE_WEBHOOK_NOTIFY_URL=\"https://your-app/notify\"\nexport LANGFUSE_SLACK_CHANNEL_ID=\"C0123456789\"\nexport LANGFUSE_SLACK_REQUEST_TIMEOUT_SECONDS=\"10\"\nexport LANGFUSE_SLACK_RETRY_MAX_ATTEMPTS=\"3\"\nexport LANGFUSE_SLACK_RETRY_BACKOFF_SECONDS=\"0.5\"\nexport LANGFUSE_SLACK_RETRY_MAX_BACKOFF_SECONDS=\"4.0\"\nexport SLACK_ATHENA_TOKEN=\"xoxb-...\"\nexport SLACK_AIMEE_TOKEN=\"xoxb-...\"\nexport SLACK_CANARY_TOKEN=\"xoxb-...\"\nexport SLACK_PROMETHEUS_TOKEN=\"xoxb-...\"\nexport SLACK_QUILL_TOKEN=\"xoxb-...\"\n\nuvicorn shared.langfuse.webhook:app --host 0.0.0.0 --port 5001\n</code></pre></p> <p>Configure Langfuse to POST to <code>https://your-host/webhooks/langfuse</code> for <code>prompt.created</code>, <code>prompt.updated</code>, and <code>prompt.deleted</code> events.</p> <p>Test: update a prompt in Langfuse and confirm a message posts to the Slack channel. Slack posting happens in a background task and includes retry/backoff with rate-limit handling.</p>"},{"location":"#pre-commit","title":"pre-commit","text":"<p>Formatting is managed via pre-commit hooks. <pre><code># Run on all files\npre-commit run --all-files\n\n# Install to run after every commit\npre-commit install\n</code></pre></p>"},{"location":"deep-dives/","title":"Deep Dives","text":"<p>Long-form explanations and architecture notes.</p> <ul> <li>Architecture</li> <li>Database</li> </ul>"},{"location":"deep-dives/architecture/","title":"Architecture","text":"<p>This repo is a thin, implementation-focused layer built on top of Axion (an external evaluation framework) plus a set of shared utilities for:</p> <ul> <li>Online monitoring (scheduled \u201cproduction-ish\u201d evaluation runs against live data sources)</li> <li>Offline analysis (loading traces/datasets locally, running metrics, exporting results)</li> <li>Prompt management + cache invalidation (Langfuse + webhook + Slack notifications)</li> <li>Optional persistence (Neon/Postgres for normalized results)</li> </ul> <p>Key directories:</p> <ul> <li><code>src/shared/</code>: cross-implementation primitives (monitoring pipeline, Langfuse helpers, Slack helpers, DB helpers, shared metrics)</li> <li><code>src/implementations/athena/</code>: a concrete implementation (extractors, metrics, Langfuse prompt patterns, YAML configs)</li> <li><code>scripts/</code>: runnable entrypoints (notably <code>scripts/run_monitoring.py</code>)</li> <li><code>.github/workflows/</code>: CI + scheduled monitoring automation</li> <li><code>data/</code>: local + CI-cached dedup state (<code>data/scored_items.csv</code>) and other artifacts</li> </ul>"},{"location":"deep-dives/architecture/#repo-wide-system-diagram-ascii","title":"Repo-wide system diagram (ASCII)","text":"<pre><code>                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502                 GitHub Actions               \u2502\n                         \u2502                                              \u2502\n                         \u2502  CI: lint/test (push/PR)                      \u2502\n                         \u2502  Scheduled Monitoring: cron (hourly)          \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                                         \u2502 runs\n                                         \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502          scripts/run_monitoring.py         \u2502\n                          \u2502  - parse args/env                           \u2502\n                          \u2502  - load YAML config                          \u2502\n                          \u2502  - set up ScoredItemsStore (optional)        \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                           \u2502\n                                           \u2502 constructs from YAML\n                                           \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502       shared.monitoring.OnlineMonitor      \u2502\n                          \u2502  Responsibilities:                           \u2502\n                          \u2502  - load config (YAML + overrides)            \u2502\n                          \u2502  - fetch from source                          \u2502\n                          \u2502  - deduplicate via ScoredItemsStore           \u2502\n                          \u2502  - sample items (strategy)                    \u2502\n                          \u2502  - run Axion evaluation_runner                \u2502\n                          \u2502  - publish results (Langfuse / Neon)          \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502               \u2502\n                          fetch items \u2502               \u2502 publish\n                                      \u2502               \u2502\n                                      \u25bc               \u25bc\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 shared.monitoring.sources   \u2502   \u2502        Publishing Sinks       \u2502\n                \u2502                             \u2502   \u2502                              \u2502\n                \u2502  DataSource (interface)     \u2502   \u2502  Langfuse (experiments/obs)   \u2502\n                \u2502   \u251c\u2500 LangfuseDataSource     \u2502   \u2502   - results.publish_as_...     \u2502\n                \u2502   \u2514\u2500 SlackDataSource        \u2502   \u2502                              \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                              \u2502\n                           \u2502           \u2502          \u2502  Neon/Postgres (optional)     \u2502\n                           \u2502           \u2502          \u2502   - shared.database.neon      \u2502\n                           \u2502           \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 Langfuse  \u2502           \u2502 Slack\n                  traces   \u2502           \u2502 messages/threads\n                           \u2502           \u2502\n                           \u25bc           \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  Langfuse API / Axion   \u2502   \u2502            Slack API            \u2502\n          \u2502  - LangfuseTraceLoader  \u2502   \u2502  - shared.slack.* helpers       \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u2502 per-trace extraction\n                      \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502             Implementations (example: Athena)                       \u2502\n     \u2502                                                                    \u2502\n     \u2502  src/implementations/athena/                                        \u2502\n     \u2502   - extractors/*: Trace -&gt; Axion DatasetItem                         \u2502\n     \u2502   - metrics/*: scoring logic (LLM + heuristics)                      \u2502\n     \u2502   - langfuse/prompt_patterns.py: regex extraction patterns           \u2502\n     \u2502   - config/*.yaml: monitor definitions (source, sampling, publish)  \u2502\n     \u2502                                                                    \u2502\n     \u2502  Metric registry (import-time registration)                          \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u2502 calls into\n                                \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502            Axion (external)        \u2502\n                     \u2502  - DatasetItem                     \u2502\n                     \u2502  - evaluation_runner               \u2502\n                     \u2502  - tracing integration             \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n  Prompt lifecycle / cache invalidation side-channel:\n\n        Langfuse Prompt UI/API \u2500\u2500(prompt.updated webhook)\u2500\u2500\u25ba FastAPI app\n                                                     shared.langfuse.webhook:app\n                                                           \u2502\n                                                           \u251c\u2500 invalidate local prompt cache\n                                                           \u251c\u2500 optional notify URL callback\n                                                           \u2514\u2500 Slack alert (async task)\n</code></pre>"},{"location":"deep-dives/architecture/#core-runtime-flows","title":"Core runtime flows","text":""},{"location":"deep-dives/architecture/#1-scheduled-online-monitoring-production-path","title":"1) Scheduled online monitoring (production path)","text":"<p>Trigger: <code>.github/workflows/monitoring.yml</code> runs hourly (cron) or via manual dispatch.</p> <p>Flow:</p> <ol> <li>GitHub Actions runs <code>python scripts/run_monitoring.py &lt;config&gt;.yaml</code>.</li> <li><code>scripts/run_monitoring.py</code>:</li> <li>loads <code>src/implementations/athena/config/&lt;config&gt;.yaml</code></li> <li>optionally enables dedup via <code>data/scored_items.csv</code> (cached across workflow runs)</li> <li><code>OnlineMonitor.from_yaml(...)</code> builds:</li> <li>a <code>DataSource</code> (<code>LangfuseDataSource</code> or <code>SlackDataSource</code>)</li> <li>a <code>SamplingStrategy</code> (e.g. random (n=10))</li> <li>a metric configuration mapping to registered metrics</li> <li><code>OnlineMonitor.run_async(...)</code>:</li> <li>fetches items from the source</li> <li>filters out already-scored items (<code>ScoredItemsStore</code>)</li> <li>samples the remainder (strategy)</li> <li>runs Axion <code>evaluation_runner(...)</code></li> <li>records dedup IDs</li> <li>If configured, publishing happens (via <code>publish=True</code> on <code>run_async(...)</code> / <code>run(...)</code>):</li> <li>to Langfuse as an experiment and/or observability events</li> <li>optionally to Neon/Postgres in normalized tables</li> </ol> <p>Note: the scripts currently call a convenience method named <code>run_and_publish(...)</code>, but the underlying monitor implementation publishes through the <code>publish</code> flag on <code>run_async(...)</code>.</p> <p>Why this works well:</p> <ul> <li>The pipeline is idempotent (dedup) and safe to run repeatedly on a schedule.</li> <li>The inputs are pluggable (source/extractor) and the scoring is configurable (metrics_config).</li> </ul>"},{"location":"deep-dives/architecture/#2-localdev-monitoring-continuous-runner","title":"2) Local/dev monitoring (continuous runner)","text":"<p>Trigger: <code>shared.monitoring.scheduler.MonitoringScheduler</code> (APScheduler) for local environments.</p> <p>Flow:</p> <ul> <li>You add one or more YAML configs, each with either:</li> <li><code>interval_minutes</code>, or</li> <li><code>cron</code> (crontab string)</li> <li>The scheduler calls <code>monitor.run_async(publish=True)</code> on schedule.</li> </ul> <p>This mirrors the GitHub Actions cron path but runs in a long-lived process.</p>"},{"location":"deep-dives/architecture/#3-offline-trace-analysis-notebooks-scripts","title":"3) Offline trace analysis (notebooks / scripts)","text":"<p>The trace wrapper layer (<code>shared.langfuse.trace</code>) supports a \u201cdomain-friendly\u201d view of Langfuse traces:</p> <ul> <li><code>TraceCollection</code> wraps a list of trace payloads</li> <li><code>Trace</code> groups observations into named \u201csteps\u201d</li> <li><code>PromptPatternsBase</code> (and implementation-specific subclasses) provides regex-based extraction so you can do:</li> <li><code>trace.recommendation.variables.some_field</code> (via dot-access wrappers)</li> </ul> <p>This is used to build <code>DatasetItem</code> objects for evaluation and/or exploratory analysis.</p>"},{"location":"deep-dives/architecture/#4-prompt-management-and-cache-invalidation-langfuse-app","title":"4) Prompt management and cache invalidation (Langfuse \u2192 app)","text":"<p><code>shared.langfuse.prompt.LangfusePromptManager</code> caches prompt fetches (TTL) and supports explicit invalidation.</p> <p>Flow:</p> <ol> <li>A prompt changes in Langfuse (create/update/delete).</li> <li>Langfuse POSTs to the FastAPI endpoint <code>shared.langfuse.webhook:app</code> at <code>/webhooks/langfuse</code>.</li> <li>The webhook verifies a signature and then:</li> <li>marks the prompt stale in <code>LangfusePromptManager</code></li> <li>optionally notifies an external listener URL</li> <li>posts a Slack alert asynchronously</li> </ol> <p>This keeps \u201cprompt-as-code\u201d consumers consistent with the latest prompt versions without restarting long-running processes.</p>"},{"location":"deep-dives/architecture/#architectural-boundaries-how-things-fit","title":"Architectural boundaries (how things fit)","text":""},{"location":"deep-dives/architecture/#shared-core-vs-implementation","title":"\u201cShared core\u201d vs \u201cImplementation\u201d","text":"<ul> <li><code>src/shared/</code> is the platform: stable abstractions (monitoring pipeline, data source interface, settings, DB clients).</li> <li><code>src/implementations/&lt;name&gt;/</code> is product-specific glue:</li> <li>how to convert source data into evaluation inputs (extractors)</li> <li>which metrics exist and how they\u2019re parameterized</li> <li>what a \u201cmonitor run\u201d means for that product (YAML configs)</li> </ul> <p>This separation makes it easy to add a new implementation without rewriting the monitoring machinery.</p>"},{"location":"deep-dives/architecture/#design-patterns-used-explicitly","title":"Design patterns used (explicitly)","text":""},{"location":"deep-dives/architecture/#strategy-pattern","title":"Strategy pattern","text":"<ul> <li>Data acquisition: <code>DataSource</code> with <code>LangfuseDataSource</code> and <code>SlackDataSource</code></li> <li>Sampling: <code>SamplingStrategy</code> implementations selected from config (e.g., all/random/most_recent/oldest)</li> <li>Prompt variable extraction: <code>PromptPatternsBase</code> subclasses per implementation (e.g., workflow vs chat patterns)</li> </ul> <p>Why: swapping behavior without changing the <code>OnlineMonitor</code> orchestration logic.</p>"},{"location":"deep-dives/architecture/#factory-configuration-as-code","title":"Factory + configuration-as-code","text":"<ul> <li><code>OnlineMonitor.from_yaml(...)</code> acts as a factory that translates YAML into concrete objects:</li> <li>loads dotted-path extractors/classes</li> <li>chooses source type and sampling strategy</li> <li>threads raw config through for publishing metadata (agent/channel/component)</li> </ul> <p>Why: the runtime topology is controlled by config, not code edits.</p>"},{"location":"deep-dives/architecture/#pluginregistry-pattern-metrics","title":"Plugin/registry pattern (metrics)","text":"<p>Implementation metrics are registered in a module-level registry (e.g. Athena recommendation metrics), and the monitor imports the registry to ensure it\u2019s populated before evaluation.</p> <p>Why: the evaluation runner can refer to metrics by name/config while the implementation decides what exists.</p>"},{"location":"deep-dives/architecture/#adapter-ports-and-adapters-lightweight-hexagonal","title":"Adapter / ports-and-adapters (lightweight hexagonal)","text":"<p>You can view the monitor as a \u201cuse-case orchestrator\u201d with ports:</p> <ul> <li>Input ports: <code>DataSource.fetch_items()</code> returning Axion <code>DatasetItem</code>s</li> <li>Output ports: publish-to-Langfuse and push-to-DB</li> </ul> <p>Concrete adapters:</p> <ul> <li>Langfuse API (trace loader, experiments, prompt API)</li> <li>Slack API (exporting messages, posting notifications)</li> <li>Neon/Postgres (connection pool + dataframe upload)</li> </ul> <p>Why: keeps the orchestration stable even as integrations evolve.</p>"},{"location":"deep-dives/architecture/#idempotency-dedup-stateful-edge","title":"Idempotency + dedup (stateful edge)","text":"<ul> <li><code>ScoredItemsStore</code> is a simple append-only CSV store keyed by <code>(source_key, monitor_key, item_id)</code>.</li> <li>GitHub Actions caches it across runs, which makes hourly monitoring \u201cpick up where it left off\u201d.</li> </ul> <p>Why: online monitoring should be repeatable and safe under retries.</p>"},{"location":"deep-dives/architecture/#settings-layering-12-factor-style","title":"Settings layering (12-factor style)","text":"<p>Settings are loaded via Pydantic Settings in two layers (documented in <code>README.md</code>):</p> <ul> <li>repo root <code>.env</code> (global defaults)</li> <li>per-implementation <code>.env</code> overrides</li> </ul> <p>Why: one codebase supports multiple deployments/implementations with minimal drift.</p>"},{"location":"deep-dives/architecture/#where-to-look-cheat-sheet","title":"\u201cWhere to look\u201d cheat-sheet","text":"<ul> <li>Online monitoring orchestrator: <code>src/shared/monitoring/monitor.py</code></li> <li>Data sources: <code>src/shared/monitoring/sources.py</code></li> <li>Dedup store: <code>src/shared/monitoring/scored_items.py</code></li> <li>Local scheduler: <code>src/shared/monitoring/scheduler.py</code></li> <li>Athena monitor config: <code>src/implementations/athena/config/monitoring.yaml</code></li> <li>Langfuse trace wrappers: <code>src/shared/langfuse/trace.py</code></li> <li>Langfuse prompts + caching: <code>src/shared/langfuse/prompt.py</code></li> <li>Langfuse webhook (invalidate + Slack notify): <code>src/shared/langfuse/webhook.py</code></li> <li>Neon/Postgres helper: <code>src/shared/database/neon.py</code></li> <li>Scheduled run automation: <code>.github/workflows/monitoring.yml</code></li> </ul>"},{"location":"deep-dives/database/","title":"Database Integration (Neon/PostgreSQL)","text":"<p>The shared database module provides synchronous and asynchronous interfaces for PostgreSQL/Neon with connection pooling, DataFrame I/O, and concurrent task execution.</p>"},{"location":"deep-dives/database/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     NeonSettings                                \u2502\n\u2502  Configuration from environment variables                       \u2502\n\u2502  \u2022 DATABASE_URL, DB_POOL_*, DB_STATEMENT_TIMEOUT_MS            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                                   \u2502\n            \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  NeonConnection       \u2502         \u2502  AsyncNeonConnection \u2502\n\u2502  (Synchronous)        \u2502         \u2502  (Asynchronous)           \u2502\n\u2502                       \u2502         \u2502                           \u2502\n\u2502  \u2022 fetch_all()        \u2502         \u2502  \u2022 await fetch_all()      \u2502\n\u2502  \u2022 fetch_one()        \u2502         \u2502  \u2022 await fetch_one()      \u2502\n\u2502  \u2022 execute_commit()   \u2502         \u2502  \u2022 await execute_commit() \u2502\n\u2502  \u2022 fetch_dataframe()  \u2502         \u2502  \u2022 await fetch_dataframe()\u2502\n\u2502  \u2022 upload_dataframe() \u2502         \u2502  \u2022 await upload_dataframe()\u2502\n\u2502  \u2022 create_table()     \u2502         \u2502  \u2022 await create_table()   \u2502\n\u2502  \u2022 check_health()     \u2502         \u2502  \u2022 await check_health()   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                                   \u2502\n            \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   ConnectionPool      \u2502         \u2502   AsyncConnectionPool     \u2502\n\u2502   (psycopg3)          \u2502         \u2502   (psycopg3)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     QueueExecutor                               \u2502\n\u2502  Concurrent task execution with asyncio.Queue                   \u2502\n\u2502  \u2022 Worker pool pattern                                          \u2502\n\u2502  \u2022 Supports sync and async functions                            \u2502\n\u2502  \u2022 Configurable backpressure                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-dives/database/#configuration","title":"Configuration","text":""},{"location":"deep-dives/database/#neonsettings","title":"NeonSettings","text":"<pre><code>class NeonSettings(RepoSettingsBase):\n    database_url: str | None              # PostgreSQL connection URL\n    db_pool_min_size: int                 # Minimum pool connections\n    db_pool_max_size: int                 # Maximum pool connections\n    db_connect_timeout_seconds: int       # Connection timeout\n    db_statement_timeout_ms: int          # Query timeout (0 = disabled)\n    db_use_startup_statement_timeout: bool\n    db_application_name: str | None       # For PostgreSQL monitoring\n    db_upload_chunk_size: int             # Rows per batch for uploads\n</code></pre>"},{"location":"deep-dives/database/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>DATABASE_URL</code> str Required Postgres connection URL <code>DB_POOL_MIN_SIZE</code> int <code>0</code> Minimum connections in pool <code>DB_POOL_MAX_SIZE</code> int <code>20</code> Maximum connections in pool <code>DB_CONNECT_TIMEOUT_SECONDS</code> int <code>10</code> Connection timeout <code>DB_STATEMENT_TIMEOUT_MS</code> int <code>60000</code> Statement timeout (0 = disabled) <code>DB_USE_STARTUP_STATEMENT_TIMEOUT</code> bool <code>False</code> Use startup options (unsupported by Neon pooler) <code>DB_APPLICATION_NAME</code> str None PostgreSQL application_name <code>DB_UPLOAD_CHUNK_SIZE</code> int <code>1000</code> Rows per batch for DataFrame uploads"},{"location":"deep-dives/database/#settings-access","title":"Settings Access","text":"<pre><code>from eval_workbench.shared.database.neon import get_neon_settings, reset_neon_settings_cache\n\n# Get cached settings\nsettings = get_neon_settings()\n\n# Clear cache when environment changes\nreset_neon_settings_cache()\n</code></pre>"},{"location":"deep-dives/database/#neonconnection-synchronous","title":"NeonConnection (Synchronous)","text":"<p>Synchronous database manager for scripts, data analysis, and standard web apps.</p>"},{"location":"deep-dives/database/#initialization","title":"Initialization","text":"<pre><code>from eval_workbench.shared.database.neon import NeonConnection\n\n# Using environment variable\ndb = NeonConnection()\n\n# Using explicit connection string\ndb = NeonConnection(connection_string=\"postgresql://user:pass@host/db\")\n</code></pre>"},{"location":"deep-dives/database/#context-manager-recommended","title":"Context Manager (Recommended)","text":"<pre><code>with NeonConnection() as db:\n    users = db.fetch_all(\"SELECT * FROM users\")\n    # Pool automatically closed on exit\n</code></pre>"},{"location":"deep-dives/database/#query-methods","title":"Query Methods","text":""},{"location":"deep-dives/database/#fetch_all-retrieve-all-rows","title":"fetch_all() - Retrieve All Rows","text":"<pre><code>def fetch_all(\n    self,\n    query: str,\n    params: Optional[Union[tuple, dict]] = None\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Example: <pre><code># Positional parameters\nresults = db.fetch_all(\n    \"SELECT * FROM users WHERE age &gt; %s\",\n    (18,)\n)\n\n# Named parameters\nresults = db.fetch_all(\n    \"SELECT * FROM users WHERE email = %(email)s\",\n    {\"email\": \"user@example.com\"}\n)\n</code></pre></p>"},{"location":"deep-dives/database/#fetch_one-retrieve-single-row","title":"fetch_one() - Retrieve Single Row","text":"<pre><code>def fetch_one(\n    self,\n    query: str,\n    params: Optional[Union[tuple, dict]] = None\n) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Example: <pre><code>user = db.fetch_one(\"SELECT * FROM users WHERE id = %s\", (1,))\nif user:\n    print(user['name'])  # Access as dictionary\n</code></pre></p>"},{"location":"deep-dives/database/#execute_commit-execute-and-commit","title":"execute_commit() - Execute and Commit","text":"<pre><code>def execute_commit(\n    self,\n    query: str,\n    params: Optional[Union[tuple, dict]] = None\n) -&gt; None\n</code></pre> <p>Example: <pre><code>db.execute_commit(\n    \"INSERT INTO users (name, email) VALUES (%s, %s)\",\n    (\"Alice\", \"alice@example.com\")\n)\n\ndb.execute_commit(\n    \"UPDATE users SET status = %s WHERE id = %s\",\n    (\"active\", 42)\n)\n</code></pre></p>"},{"location":"deep-dives/database/#dataframe-utilities","title":"DataFrame Utilities","text":""},{"location":"deep-dives/database/#fetch_dataframe-query-to-dataframe","title":"fetch_dataframe() - Query to DataFrame","text":"<pre><code>def fetch_dataframe(\n    self,\n    query: str,\n    params: Optional[Union[tuple, dict]] = None\n) -&gt; pd.DataFrame\n</code></pre> <p>Example: <pre><code>df = db.fetch_dataframe(\n    \"SELECT id, name, score FROM results WHERE score &gt; %s\",\n    (75,)\n)\nprint(df.describe())\n</code></pre></p>"},{"location":"deep-dives/database/#upload_dataframe-insert-dataframe","title":"upload_dataframe() - Insert DataFrame","text":"<pre><code>def upload_dataframe(\n    self,\n    df: pd.DataFrame,\n    table_name: str\n) -&gt; None\n</code></pre> <p>Behavior: - Converts NaN/None to NULL - Batches inserts by <code>db_upload_chunk_size</code> (default: 1000 rows) - Logs row count on success</p> <p>Example: <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'id': [1, 2, 3],\n    'value': [100.5, 200.3, None],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\ndb.upload_dataframe(df, 'measurements')\n# Logs: \"Successfully uploaded 3 rows to 'measurements'.\"\n</code></pre></p>"},{"location":"deep-dives/database/#table-management","title":"Table Management","text":""},{"location":"deep-dives/database/#create_table-safe-table-creation","title":"create_table() - Safe Table Creation","text":"<pre><code>def create_table(\n    self,\n    table_name: str,\n    schema: Optional[str] = None,\n    columns: Optional[List[Tuple[str, str]]] = None\n) -&gt; None\n</code></pre> <p>Using columns list (recommended): <pre><code>db.create_table(\n    table_name='users',\n    columns=[\n        ('id', 'SERIAL PRIMARY KEY'),\n        ('name', 'VARCHAR(255) NOT NULL'),\n        ('email', 'VARCHAR(255) UNIQUE'),\n        ('created_at', 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'),\n    ]\n)\n</code></pre></p> <p>Using raw schema: <pre><code>db.create_table(\n    table_name='users',\n    schema='id SERIAL PRIMARY KEY, name VARCHAR(255), email VARCHAR(255) UNIQUE'\n)\n</code></pre></p> <p>SQL Type Validation: - Only allows: letters, digits, underscore, space, parentheses, comma, period - Prevents SQL injection attacks</p>"},{"location":"deep-dives/database/#health-check","title":"Health Check","text":"<pre><code>def check_health(self) -&gt; bool:\n    \"\"\"Test database connectivity. Returns True if successful.\"\"\"\n\nif db.check_health():\n    print(\"Database is healthy\")\n</code></pre>"},{"location":"deep-dives/database/#cleanup","title":"Cleanup","text":"<pre><code>db.close()  # Close the connection pool\n</code></pre>"},{"location":"deep-dives/database/#asyncneonconnection-asynchronous","title":"AsyncNeonConnection (Asynchronous)","text":"<p>Async database manager for FastAPI, high-concurrency workloads, and AI agents. Identical API but fully async.</p>"},{"location":"deep-dives/database/#initialization_1","title":"Initialization","text":"<pre><code>from eval_workbench.shared.database.neon import AsyncNeonConnection\n\ndb = AsyncNeonConnection()  # Pool opens lazily\n</code></pre>"},{"location":"deep-dives/database/#async-context-manager","title":"Async Context Manager","text":"<pre><code>async with AsyncNeonConnection() as db:\n    users = await db.fetch_all(\"SELECT * FROM users\")\n    # Pool automatically closed on exit\n</code></pre>"},{"location":"deep-dives/database/#async-methods","title":"Async Methods","text":"<p>All methods parallel the sync API but are async:</p> <pre><code>async def fetch_all(self, query, params=None) -&gt; List[Dict]: ...\nasync def fetch_one(self, query, params=None) -&gt; Optional[Dict]: ...\nasync def execute_commit(self, query, params=None) -&gt; None: ...\nasync def fetch_dataframe(self, query, params=None) -&gt; pd.DataFrame: ...\nasync def upload_dataframe(self, df, table_name) -&gt; None: ...\nasync def create_table(self, table_name, schema=None, columns=None) -&gt; None: ...\nasync def check_health(self) -&gt; bool: ...\nasync def close(self) -&gt; None: ...\n</code></pre>"},{"location":"deep-dives/database/#concurrent-queries","title":"Concurrent Queries","text":"<pre><code>async with AsyncNeonConnection() as db:\n    # Execute multiple queries concurrently\n    users, orders, products = await asyncio.gather(\n        db.fetch_all(\"SELECT * FROM users\"),\n        db.fetch_all(\"SELECT * FROM orders\"),\n        db.fetch_all(\"SELECT * FROM products\"),\n    )\n</code></pre>"},{"location":"deep-dives/database/#queueexecutor","title":"QueueExecutor","text":"<p>Worker pool pattern using <code>asyncio.Queue</code> for concurrent task execution.</p>"},{"location":"deep-dives/database/#initialization_2","title":"Initialization","text":"<pre><code>from eval_workbench.shared.database.neon import QueueExecutor\n\nexecutor = QueueExecutor(\n    num_workers=5,      # Number of worker coroutines\n    maxsize=0,          # Queue size limit (0 = unlimited)\n    executor=None,      # Optional thread pool for sync functions\n)\n</code></pre>"},{"location":"deep-dives/database/#context-manager-usage","title":"Context Manager Usage","text":"<pre><code>async def process_item(item_id: int):\n    await asyncio.sleep(1)\n    return f\"Processed {item_id}\"\n\nasync def main():\n    async with QueueExecutor(num_workers=10) as executor:\n        # Submit tasks\n        futures = [\n            executor.submit(process_item, i)\n            for i in range(100)\n        ]\n\n        # Gather results\n        results = await asyncio.gather(*futures)\n        return results\n\nresults = asyncio.run(main())\n</code></pre>"},{"location":"deep-dives/database/#methods","title":"Methods","text":""},{"location":"deep-dives/database/#start-activate-workers","title":"start() - Activate Workers","text":"<pre><code>async def start(self):\n    \"\"\"Start worker coroutines. Must call before submitting tasks.\"\"\"\n</code></pre>"},{"location":"deep-dives/database/#stop-graceful-shutdown","title":"stop() - Graceful Shutdown","text":"<pre><code>async def stop(self, timeout: Optional[float] = None):\n    \"\"\"Stop worker pool gracefully with optional timeout.\"\"\"\n</code></pre>"},{"location":"deep-dives/database/#submit-queue-task","title":"submit() - Queue Task","text":"<pre><code>def submit(\n    self,\n    func: Callable[..., Any],\n    *args: Any,\n    **kwargs: Any\n) -&gt; asyncio.Future:\n    \"\"\"\n    Submit function to be executed by worker pool.\n    Supports both sync and async functions.\n    \"\"\"\n</code></pre>"},{"location":"deep-dives/database/#backpressure-control","title":"Backpressure Control","text":"<pre><code># With queue size limit - blocks when full\nasync with QueueExecutor(num_workers=5, maxsize=100) as executor:\n    # If queue has 100 tasks and workers are slow,\n    # submit() will await until queue has space\n    future = executor.submit(slow_function)\n</code></pre>"},{"location":"deep-dives/database/#error-handling","title":"Error Handling","text":"<p>All database methods follow consistent error handling:</p> <pre><code>try:\n    result = db.fetch_all(query, params)\nexcept psycopg.Error as e:\n    # Database-specific errors\n    logger.error(f\"Database query failed: {e}\")\n    raise\n</code></pre> <p>Error Types: - <code>psycopg.Error</code>: Base exception for all psycopg errors - <code>psycopg.DatabaseError</code>: Database-specific errors - <code>psycopg.IntegrityError</code>: Constraint violations - <code>psycopg.OperationalError</code>: Connection/operational issues - <code>ValueError</code>: Invalid configuration or parameters</p>"},{"location":"deep-dives/database/#code-examples","title":"Code Examples","text":""},{"location":"deep-dives/database/#pattern-1-simple-synchronous-queries","title":"Pattern 1: Simple Synchronous Queries","text":"<pre><code>from eval_workbench.shared.database.neon import NeonConnection\n\ndb = NeonConnection()\n\nuser = db.fetch_one(\"SELECT * FROM users WHERE id = %s\", (1,))\nusers = db.fetch_all(\"SELECT * FROM users WHERE active = true\")\n\ndb.close()\n</code></pre>"},{"location":"deep-dives/database/#pattern-2-dataframe-workflow","title":"Pattern 2: DataFrame Workflow","text":"<pre><code>with NeonConnection() as db:\n    # Query to DataFrame\n    df = db.fetch_dataframe(\"SELECT * FROM sales WHERE year = %s\", (2024,))\n\n    # Transform\n    df['adjusted'] = df['value'] * 1.1\n\n    # Upload back\n    db.upload_dataframe(df, 'sales_adjusted')\n</code></pre>"},{"location":"deep-dives/database/#pattern-3-async-with-fastapi","title":"Pattern 3: Async with FastAPI","text":"<pre><code>from fastapi import FastAPI\nfrom eval_workbench.shared.database.neon import AsyncNeonConnection\n\napp = FastAPI()\ndb = AsyncNeonConnection()\n\n@app.on_event(\"startup\")\nasync def startup():\n    await db.pool.open()\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    await db.close()\n\n@app.get(\"/users\")\nasync def get_users():\n    return await db.fetch_all(\"SELECT * FROM users\")\n</code></pre>"},{"location":"deep-dives/database/#pattern-4-batch-operations","title":"Pattern 4: Batch Operations","text":"<pre><code>with NeonConnection() as db:\n    # Create table\n    db.create_table('results', columns=[\n        ('id', 'SERIAL PRIMARY KEY'),\n        ('value', 'FLOAT'),\n        ('name', 'VARCHAR(255)'),\n    ])\n\n    # Upload large DataFrame in batches\n    import pandas as pd\n    large_df = pd.read_csv('data.csv')  # 1M rows\n    db.upload_dataframe(large_df, 'results')\n    # Internally batches as: [0:1000], [1000:2000], ...\n</code></pre>"},{"location":"deep-dives/database/#pattern-5-parallel-processing-with-queueexecutor","title":"Pattern 5: Parallel Processing with QueueExecutor","text":"<pre><code>from eval_workbench.shared.database.neon import QueueExecutor\n\nasync def fetch_and_process(user_id: int):\n    # Could be CPU-intensive or I/O work\n    return user_id * 2\n\nasync def main():\n    async with QueueExecutor(num_workers=10) as executor:\n        futures = [\n            executor.submit(fetch_and_process, i)\n            for i in range(1000)\n        ]\n        results = await asyncio.gather(*futures)\n        return results\n\nresults = asyncio.run(main())\n</code></pre>"},{"location":"deep-dives/database/#pattern-6-integration-with-langfuse-joiner","title":"Pattern 6: Integration with Langfuse Joiner","text":"<pre><code>from eval_workbench.shared.database.neon import NeonConnection\nfrom eval_workbench.implementations.athena.langfuse.join import AthenaNeonLangfuseJoiner\n\ndb = NeonConnection()\njoiner = AthenaNeonLangfuseJoiner(neon_db=db, trace_loader=loader)\n\n# Fetch cases using the database manager\ncases = joiner.fetch_cases(\n    limit=100,\n    where=\"status = 'completed'\",\n)\n\ndb.close()\n</code></pre>"},{"location":"deep-dives/database/#connection-pool-management","title":"Connection Pool Management","text":""},{"location":"deep-dives/database/#synchronous-pool","title":"Synchronous Pool","text":"<pre><code>self.pool = ConnectionPool(\n    conninfo=self.dsn,\n    min_size=0,          # Pre-allocated connections\n    max_size=20,         # Maximum concurrent connections\n    kwargs={\n        \"row_factory\": dict_row,\n        \"connect_timeout\": 10,\n    },\n)\n</code></pre>"},{"location":"deep-dives/database/#asynchronous-pool","title":"Asynchronous Pool","text":"<pre><code>self.pool = AsyncConnectionPool(\n    conninfo=self.dsn,\n    min_size=0,\n    max_size=20,\n    open=False,          # Lazy initialization\n)\n</code></pre>"},{"location":"deep-dives/database/#statement-timeout","title":"Statement Timeout","text":"<p>Applied per-connection when not using startup options:</p> <pre><code># If db_statement_timeout_ms &gt; 0 and not using startup options:\nSET statement_timeout = 60000;  # 60 seconds\n</code></pre>"},{"location":"deep-dives/database/#dependencies","title":"Dependencies","text":"<pre><code>psycopg[binary]&gt;=3.0         # PostgreSQL adapter\npsycopg-pool&gt;=3.0            # Connection pooling\npandas&gt;=1.0                  # DataFrame support\npydantic&gt;=2.0                # Settings validation\npydantic-settings&gt;=2.0       # Environment loading\n</code></pre>"},{"location":"deep-dives/database/#exports","title":"Exports","text":"<pre><code>from eval_workbench.shared.database.neon import (\n    # Configuration\n    NeonSettings,\n    get_neon_settings,\n    reset_neon_settings_cache,\n\n    # Database Managers\n    NeonConnection,\n    AsyncNeonConnection,\n\n    # Task Execution\n    QueueExecutor,\n)\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This site hosts documentation for MGT Eval Workbench and its evaluation metrics.</p>"},{"location":"getting-started/#install-editable","title":"Install (editable)","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"guides/","title":"Guides","text":"<p>Guides for Langfuse integrations, monitoring, and Slack tooling.</p>"},{"location":"guides/#monitoring","title":"Monitoring","text":"<ul> <li>Monitoring Configuration (YAML)</li> </ul>"},{"location":"guides/athena-langfuse/","title":"Langfuse Prompt Management (Athena)","text":"<p>The Athena Langfuse integration provides a join system that bridges Neon database records with Langfuse distributed traces, enabling rich analysis of AI workflows through combined case and trace data.</p>"},{"location":"guides/athena-langfuse/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AthenaNeonLangfuseJoiner   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                                          \u2502\n    \u25bc                                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 NeonDatabaseMgr  \u2502              \u2502 LangfuseTraceLoader  \u2502\n\u2502  fetch_dataframe \u2502              \u2502   fetch_traces()     \u2502\n\u2502  fetch_cases()   \u2502              \u2502  fetch_trace_by_id() \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                   \u2502\n         \u25bc                                   \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 cases   \u2502                    \u2502 TraceCollection  \u2502\n    \u2502DataFrame\u2502                    \u2502  List[Trace]     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                \u2502\n         \u2502         join_cases_with_traces()\n         \u2502                 \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                      \u25bc\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502 Joined DataFrame     \u2502\n           \u2502 - case columns       \u2502\n           \u2502 + langfuse_trace col \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/athena-langfuse/#trace-access-smartaccess","title":"Trace Access (SmartAccess)","text":"<pre><code>Trace.recommendation\n   \u2514\u2500 TraceStep\n      \u251c\u2500 .generation (GENERATION observation)\n      \u251c\u2500 .context (SPAN observation)\n      \u2514\u2500 .variables (extracted from patterns)\n         \u251c\u2500 caseAssessment\n         \u251c\u2500 contextData\n         \u251c\u2500 underwritingFlags\n         \u2514\u2500 swallowDebugData\n</code></pre>"},{"location":"guides/athena-langfuse/#joinsettings-configuration","title":"JoinSettings Configuration","text":"<p>Frozen dataclass for configuring the join operation.</p> <pre><code>@dataclass(frozen=True)\nclass JoinSettings:\n    case_table: str = \"athena_cases\"\n    case_columns: tuple[str, ...] = (\n        \"id\",\n        \"workflow_id\",\n        \"quote_locator\",\n        \"slack_thread_ts\",\n        \"slack_channel_id\",\n        \"langfuse_trace_id\",\n    )\n    trace_name: str = \"athena\"\n    trace_tags: tuple[str, ...] = (\"production\",)\n</code></pre> Attribute Type Default Purpose <code>case_table</code> <code>str</code> <code>\"athena_cases\"</code> Target table in Neon database <code>case_columns</code> <code>tuple[str, ...]</code> See above Columns to fetch from case_table <code>trace_name</code> <code>str</code> <code>\"athena\"</code> Langfuse trace name filter <code>trace_tags</code> <code>tuple[str, ...]</code> <code>(\"production\",)</code> Langfuse trace tags to filter by <p>Usage: <pre><code># Default settings\nsettings = JoinSettings()\n\n# Custom settings\nsettings = JoinSettings(\n    case_table=\"custom_cases\",\n    case_columns=(\"id\", \"workflow_id\", \"status\"),\n    trace_name=\"custom_workflow\",\n    trace_tags=(\"staging\", \"v2\"),\n)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#athenaneonlangfusejoiner","title":"AthenaNeonLangfuseJoiner","text":"<p>Main helper class to coordinate fetching and joining case data from Neon with trace data from Langfuse.</p>"},{"location":"guides/athena-langfuse/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    neon_db: NeonConnection,\n    trace_loader: LangfuseTraceLoader,\n    *,\n    settings: JoinSettings | None = None,\n    prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n) -&gt; None\n</code></pre> Parameter Type Default Description <code>neon_db</code> <code>NeonConnection</code> Required Database connection manager <code>trace_loader</code> <code>LangfuseTraceLoader</code> Required Langfuse trace fetching client <code>settings</code> <code>JoinSettings \\| None</code> <code>JoinSettings()</code> Configuration for table/column/trace names <code>prompt_patterns</code> <code>PromptPatternsBase</code> <code>WorkflowPromptPatterns</code> Pattern registry for prompt extraction"},{"location":"guides/athena-langfuse/#methods","title":"Methods","text":""},{"location":"guides/athena-langfuse/#fetch_cases","title":"fetch_cases()","text":"<p>Fetch case records from Neon database.</p> <pre><code>def fetch_cases(\n    self,\n    *,\n    limit: Optional[int] = None,\n    where: Optional[str] = None,\n    columns: Optional[Sequence[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Example: <pre><code>cases = joiner.fetch_cases(\n    limit=50,\n    where=\"status = 'active'\",\n    columns=(\"id\", \"workflow_id\", \"quote_locator\"),\n)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#fetch_traces","title":"fetch_traces()","text":"<p>Fetch traces from Langfuse by name and tags.</p> <pre><code>def fetch_traces(\n    self,\n    *,\n    limit: int = 200,\n    name: Optional[str] = None,\n    tags: Optional[Sequence[str]] = None,\n    fetch_full_traces: bool = True,\n    show_progress: bool = False,\n) -&gt; TraceCollection\n</code></pre> <p>Example: <pre><code>traces = joiner.fetch_traces(\n    limit=100,\n    name=\"custom_workflow\",\n    tags=[\"staging\"],\n    fetch_full_traces=True,\n)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#fetch_traces_by_ids","title":"fetch_traces_by_ids()","text":"<p>Fetch specific traces by their IDs.</p> <pre><code>def fetch_traces_by_ids(\n    self,\n    trace_ids: Sequence[str],\n    *,\n    fetch_full_traces: bool = True,\n    show_progress: bool = False,\n    trace_fetcher: Callable[[str], object] | None = None,\n) -&gt; TraceCollection\n</code></pre> <p>Example: <pre><code>trace_ids = [\"trace-001\", \"trace-002\", \"trace-003\"]\ntraces = joiner.fetch_traces_by_ids(trace_ids, fetch_full_traces=True)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#join_cases_with_traces","title":"join_cases_with_traces()","text":"<p>Join case records with corresponding trace objects via trace ID.</p> <pre><code>def join_cases_with_traces(\n    self,\n    cases: pd.DataFrame,\n    traces: TraceCollection,\n    *,\n    trace_id_column: str = \"langfuse_trace_id\",\n    trace_output_column: str = \"langfuse_trace\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Example: <pre><code>joined = joiner.join_cases_with_traces(\n    cases,\n    traces,\n    trace_id_column=\"langfuse_trace_id\",\n    trace_output_column=\"trace_data\",\n)\n# Result: cases DataFrame with new 'trace_data' column containing Trace objects\n</code></pre></p>"},{"location":"guides/athena-langfuse/#prompt-extraction-patterns","title":"Prompt Extraction Patterns","text":"<p>Pattern classes define regex patterns for extracting structured data from LLM prompts. They inherit from <code>PromptPatternsBase</code>.</p>"},{"location":"guides/athena-langfuse/#promptpatternsbase","title":"PromptPatternsBase","text":"<p>Base registry for regex extraction patterns.</p> <pre><code>class PromptPatternsBase:\n    @classmethod\n    def get_for(cls, step_name: str) -&gt; Dict[str, str]:\n        \"\"\"\n        Dynamically looks for methods named _patterns_{step_name}().\n        Returns dict of {field_name: regex_pattern}.\n        \"\"\"\n</code></pre>"},{"location":"guides/athena-langfuse/#workflowpromptpatterns","title":"WorkflowPromptPatterns","text":"<p>Extract structured fields from \"recommendation\" workflow prompts.</p> <pre><code>class WorkflowPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_recommendation() -&gt; Dict[str, str]:\n        \"\"\"Returns patterns for recommendation step.\"\"\"\n</code></pre> <p>Extracted Fields:</p> Field Header Delimiter Description <code>caseAssessment</code> \"CASE ASSESSMENT...\" \u2192 \"FULL CONTEXT DATA\" Previous case analysis <code>contextData</code> \"FULL CONTEXT DATA\" \u2192 \"UNDERWRITING FLAGS...\" Complete context info <code>underwritingFlags</code> \"UNDERWRITING FLAGS...\" \u2192 \"SWALLOW DEBUG DATA...\" Active underwriting rules <code>swallowDebugData</code> \"SWALLOW DEBUG DATA...\" \u2192 \"PREVIOUS RECOMMENDATIONS...\" or EOF Debug trace data"},{"location":"guides/athena-langfuse/#chatpromptpatterns","title":"ChatPromptPatterns","text":"<p>Extract structured fields from chat-based prompts.</p> <pre><code>class ChatPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_chat() -&gt; Dict[str, str]:\n        \"\"\"Returns patterns for chat step.\"\"\"\n</code></pre> <p>Extracted Fields:</p> Field Delimiter Pattern Description <code>conversationHistory</code> <code>**Conversation History:**</code> \u2192 <code>**Quote Context:**</code> Chat message history <code>quoteContext</code> <code>**Quote Context:**</code> \u2192 <code>**Current User Message:**</code> Quote/reference data <code>currentUserMessage</code> <code>**Current User Message:**</code> \u2192 <code>**Context:**</code> Latest user input <code>context</code> <code>**Context:**</code> \u2192 <code>**Instructions:**</code> System/background context <code>instructions</code> <code>**Instructions:**</code> \u2192 EOF System instructions"},{"location":"guides/athena-langfuse/#tracecollection-and-smartaccess-utilities","title":"TraceCollection and SmartAccess Utilities","text":""},{"location":"guides/athena-langfuse/#tracecollection","title":"TraceCollection","text":"<p>Wraps a list of trace data items, providing collection-level operations.</p> <pre><code>class TraceCollection:\n    def __init__(\n        self,\n        data: List[Any],\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    ): ...\n\n    def __getitem__(self, index: int) -&gt; Trace: ...\n    def __iter__(self) -&gt; Iterator[Trace]: ...\n    def __len__(self) -&gt; int: ...\n    def filter_by(self, **kwargs) -&gt; TraceCollection: ...\n</code></pre> <p>Example: <pre><code>traces = TraceCollection(trace_data_list, prompt_patterns=WorkflowPromptPatterns)\n\n# Access individual traces\nfirst_trace = traces[0]\n\n# Iterate\nfor trace in traces:\n    print(trace.id)\n\n# Filter\nproduction_traces = traces.filter_by(name=\"athena\")\n\n# Get count\ntotal = len(traces)\n</code></pre></p>"},{"location":"guides/athena-langfuse/#smartaccess","title":"SmartAccess","text":"<p>Base class enabling intelligent dot-notation access with fuzzy matching.</p> <p>Features: - Dot notation access: <code>trace.recommendation</code> - Bracket access: <code>trace['recommendation']</code> - Case-insensitive matching: <code>trace.product_type</code> matches <code>productType</code> - Recursive wrapping: nested objects maintain SmartAccess</p> <p>Fuzzy Matching: <pre><code>def _normalize_key(key: str) -&gt; str:\n    \"\"\"'product_type' matches 'productType'\"\"\"\n    return key.lower().replace(\"_\", \"\")\n</code></pre></p>"},{"location":"guides/athena-langfuse/#trace","title":"Trace","text":"<p>Main wrapper providing access to steps and trace-level attributes.</p> <pre><code>class Trace(SmartAccess):\n    def __init__(\n        self,\n        trace_data: Any,\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    ): ...\n</code></pre> <p>Access Patterns:</p> Access Returns Example <code>trace.{step_name}</code> <code>TraceStep</code> <code>trace.recommendation</code> <code>trace.{trace_attr}</code> Any <code>trace.id</code>, <code>trace.latency</code>"},{"location":"guides/athena-langfuse/#tracestep","title":"TraceStep","text":"<p>Represents a specific named step (e.g., 'recommendation').</p> <pre><code>class TraceStep(SmartAccess):\n    name: str                              # Step name\n    observations: List[ObservationsView]   # Observations in step\n    prompt_patterns: PromptPatternsBase    # Pattern registry\n</code></pre> <p>Properties:</p> Access Returns Description <code>step.variables</code> <code>Dict[str, str]</code> Lazily extracted prompt variables <code>step.generation</code> <code>ObservationsView</code> First GENERATION observation <code>step.context</code> <code>ObservationsView</code> SPAN observation (context) <code>step.span</code> <code>ObservationsView</code> SPAN observation"},{"location":"guides/athena-langfuse/#integration-with-sharedlangfusetracepy","title":"Integration with shared/langfuse/trace.py","text":"<p>The Athena implementation builds on the shared Langfuse trace module:</p> <ol> <li>PromptPatternsBase Inheritance - <code>WorkflowPromptPatterns</code> and    <code>ChatPromptPatterns</code> extend base</li> <li>TraceCollection Usage - <code>fetch_traces()</code> returns <code>TraceCollection</code></li> <li>Trace Smart Access - All Trace objects use SmartAccess for dot notation</li> <li>Pattern Resolution - Helper converts class to instance for all wrapped    traces</li> </ol>"},{"location":"guides/athena-langfuse/#code-examples","title":"Code Examples","text":""},{"location":"guides/athena-langfuse/#basic-usage-fetch-and-join","title":"Basic Usage: Fetch and Join","text":"<pre><code>from eval_workbench.implementations.athena.langfuse.join import AthenaNeonLangfuseJoiner, JoinSettings\nfrom eval_workbench.implementations.athena.langfuse.prompt_patterns import WorkflowPromptPatterns\nfrom eval_workbench.shared.database.neon import NeonConnection\nfrom axion.tracing import LangfuseTraceLoader\n\n# Initialize components\nneon_db = NeonConnection()\ntrace_loader = LangfuseTraceLoader(api_key=\"YOUR_API_KEY\")\n\n# Create joiner\njoiner = AthenaNeonLangfuseJoiner(\n    neon_db,\n    trace_loader,\n    prompt_patterns=WorkflowPromptPatterns,\n)\n\n# Fetch cases and traces\ncases = joiner.fetch_cases(limit=100, where=\"status = 'completed'\")\ntrace_ids = cases['langfuse_trace_id'].dropna().unique()\ntraces = joiner.fetch_traces_by_ids(trace_ids, fetch_full_traces=True)\n\n# Join\njoined = joiner.join_cases_with_traces(cases, traces)\nprint(f\"Joined {len(joined)} cases with traces\")\n</code></pre>"},{"location":"guides/athena-langfuse/#analyzing-traces","title":"Analyzing Traces","text":"<pre><code># Access joined data\nfor idx, row in joined.iterrows():\n    case_id = row['id']\n    trace = row['langfuse_trace']\n\n    if trace is None:\n        print(f\"Case {case_id}: No trace found\")\n        continue\n\n    # Access trace metadata\n    print(f\"Case {case_id}:\")\n    print(f\"  Trace ID: {trace.id}\")\n    print(f\"  Latency: {trace.latency}ms\")\n\n    # Access recommendation step\n    rec_step = trace.recommendation\n\n    # Extract variables from prompt (lazy evaluation)\n    variables = rec_step.variables\n    print(f\"  Case Assessment: {variables.get('caseAssessment', 'N/A')[:100]}...\")\n    print(f\"  Context Data: {variables.get('contextData', 'N/A')[:100]}...\")\n\n    # Access generation observation\n    gen = rec_step.generation\n    print(f\"  Generation model: {gen.model}\")\n</code></pre>"},{"location":"guides/athena-langfuse/#custom-settings","title":"Custom Settings","text":"<pre><code># Configure for staging environment\nsettings = JoinSettings(\n    case_table=\"athena_cases_staging\",\n    case_columns=(\"id\", \"workflow_id\", \"langfuse_trace_id\", \"status\"),\n    trace_name=\"athena-staging\",\n    trace_tags=(\"staging\", \"v2\"),\n)\n\njoiner = AthenaNeonLangfuseJoiner(\n    neon_db,\n    trace_loader,\n    settings=settings,\n    prompt_patterns=WorkflowPromptPatterns,\n)\n</code></pre>"},{"location":"guides/athena-langfuse/#custom-prompt-patterns","title":"Custom Prompt Patterns","text":"<pre><code>from eval_workbench.shared.langfuse.trace import PromptPatternsBase, create_extraction_pattern\n\nclass CustomPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_my_step() -&gt; Dict[str, str]:\n        return {\n            \"fieldA\": create_extraction_pattern(\n                \"FIELD A HEADER\",\n                \"FIELD B HEADER\"\n            ),\n            \"fieldB\": create_extraction_pattern(\n                \"FIELD B HEADER\",\n                \"(?:$)\"  # End of text\n            ),\n        }\n\n# Use custom patterns\njoiner = AthenaNeonLangfuseJoiner(\n    neon_db,\n    trace_loader,\n    prompt_patterns=CustomPromptPatterns,\n)\n</code></pre>"},{"location":"guides/athena-langfuse/#working-with-tracecollection","title":"Working with TraceCollection","text":"<pre><code># Fetch traces\ntraces = joiner.fetch_traces(limit=200)\n\n# Filter traces\nrecommendation_traces = traces.filter_by(name=\"recommendation\")\n\n# Iterate and analyze\nfor trace in traces:\n    # Access via dot notation (case insensitive)\n    trace_id = trace.id\n\n    # Access steps\n    if hasattr(trace, 'recommendation'):\n        rec = trace.recommendation\n        print(f\"Trace {trace_id}: has recommendation step\")\n\n        # Lazy variable extraction\n        vars = rec.variables\n        if 'caseAssessment' in vars:\n            print(f\"  Assessment: {vars['caseAssessment'][:50]}...\")\n</code></pre>"},{"location":"guides/athena-langfuse/#exports","title":"Exports","text":"<pre><code>from eval_workbench.implementations.athena.langfuse.join import (\n    JoinSettings,\n    AthenaNeonLangfuseJoiner,\n)\n\nfrom eval_workbench.implementations.athena.langfuse.prompt_patterns import (\n    WorkflowPromptPatterns,\n    ChatPromptPatterns,\n)\n\nfrom eval_workbench.shared.langfuse.trace import (\n    PromptPatternsBase,\n    SmartAccess,\n    SmartDict,\n    SmartObject,\n    Trace,\n    TraceStep,\n    TraceCollection,\n    create_extraction_pattern,\n)\n</code></pre>"},{"location":"guides/monitoring-config/","title":"Monitoring Configuration (YAML)","text":"<p>This guide explains the <code>OnlineMonitor</code> YAML configuration format. Start from <code>src/shared/monitoring/monitoring.yaml.example</code> and customize it for your implementation.</p>"},{"location":"guides/monitoring-config/#1-top-level-fields","title":"1) Top-Level Fields","text":"<pre><code>version: \"1.0\"\nname: \"my_monitor\"\n</code></pre> <ul> <li><code>version</code>: config schema version</li> <li><code>name</code>: monitor name (used for dedup + experiment naming)</li> </ul>"},{"location":"guides/monitoring-config/#optional-scored-items-store","title":"Optional scored items store","text":"<pre><code>scored_store:\n  # type: none | csv | db\n  type: csv\n  # file_path: \"data/scored_items.csv\"\n  # connection_string: \"postgresql://user:pass@host:5432/dbname\"\n</code></pre> <ul> <li><code>type</code>: <code>csv</code> uses file-based dedup, <code>db</code> uses <code>evaluation_dataset</code>, <code>none</code> disables</li> <li><code>file_path</code>: only for <code>csv</code></li> <li><code>connection_string</code>: only for <code>db</code></li> </ul>"},{"location":"guides/monitoring-config/#optional-schedule-for-monitoringscheduler","title":"Optional schedule (for MonitoringScheduler)","text":"<pre><code>schedule:\n  # Use ONE of these\n  interval_minutes: 10\n  # cron: \"*/10 * * * *\"\n</code></pre> <ul> <li><code>interval_minutes</code>: run every N minutes</li> <li><code>cron</code>: cron expression for aligned schedules</li> </ul>"},{"location":"guides/monitoring-config/#2-source-configuration","title":"2) Source Configuration","text":""},{"location":"guides/monitoring-config/#langfuse-source","title":"Langfuse source","text":"<pre><code>source:\n  type: langfuse\n  name: \"my_agent\"\n  component: \"my_component\"\n  extractor: \"implementations.my_agent.extractors.extract_trace\"\n  # prompt_patterns: \"implementations.my_agent.langfuse.prompt_patterns.MyPromptPatterns\"\n  limit: 100\n  days_back: 7\n  tags: [\"production\"]\n  timeout: 60\n  fetch_full_traces: true\n  show_progress: true\n</code></pre> <p>Key fields:</p> <ul> <li><code>extractor</code>: python path to a <code>(Trace) -&gt; DatasetItem</code> function</li> <li><code>limit</code>: max traces to fetch</li> <li><code>days_back</code> / <code>hours_back</code> / <code>minutes_back</code>: time window (days_back wins if both set; hours_back wins over minutes_back)</li> <li><code>tags</code>: filter by Langfuse tags</li> <li><code>fetch_full_traces</code>: include observations/scores (slower but richer)</li> </ul>"},{"location":"guides/monitoring-config/#slack-source-alternative","title":"Slack source (alternative)","text":"<pre><code>source:\n  type: slack\n  name: \"my_slack_channels\"\n  channel_ids:\n    - \"C0XXXXXXXXX\"\n  limit: 50\n  scrape_threads: true\n  bot_name: \"MyBot\"\n  workspace_domain: \"myworkspace\"\n  drop_if_first_is_user: true\n  drop_if_all_ai: true\n  max_concurrent: 2\n  # filter_sender: \"U1234567890\"\n</code></pre>"},{"location":"guides/monitoring-config/#3-sampling-configuration","title":"3) Sampling Configuration","text":"<pre><code>sampling:\n  strategy: random\n  n: 10\n  # seed: 42\n</code></pre> <p>Strategies:</p> <ul> <li><code>all</code> (default)</li> <li><code>random</code></li> <li><code>most_recent</code></li> <li><code>oldest</code></li> </ul> <p>Best practice: set <code>source.limit</code> much larger than <code>sampling.n</code>.</p>"},{"location":"guides/monitoring-config/#4-metrics-configuration","title":"4) Metrics Configuration","text":"<pre><code>metrics_config:\n  MyMetric:\n    class: \"my_metric_class\"\n    llm_provider: \"openai\"\n    model_name: \"gpt-4o\"\n</code></pre> <p>Each metric key is a label in results; <code>class</code> must match a registered metric.</p>"},{"location":"guides/monitoring-config/#5-publishing-configuration-optional","title":"5) Publishing Configuration (Optional)","text":"<pre><code>publishing:\n  push_to_db: false\n  push_to_langfuse: false\n  trace_experiment: false\n  # metric_names:\n  #   - \"MyMetric\"\n\n  database:\n    on_conflict: do_nothing\n    chunk_size: 1000\n\n  experiment:\n    enabled: true\n    dataset_name: \"development\"\n    run_name: \"online-${ENVIRONMENT:-preview}\"\n    link_to_traces: true\n    # run_metadata: {}\n    # tags: [\"monitoring\", \"athena\"]\n    # flush: true\n    # score_on_runtime_traces: false\n    # metrics:  # legacy: prefer publishing.metric_names\n    #   - \"MyMetric\"\n</code></pre>"},{"location":"guides/monitoring-config/#usage-examples","title":"Usage Examples","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, ScoredItemsStore\n\n# Basic usage\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\")\nresults = monitor.run()\n\n# With dedup store\nstore = ScoredItemsStore(\"data/scored_items.csv\")\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\", scored_store=store)\nresults = monitor.run(deduplicate=True)\n\n# With overrides\nmonitor = OnlineMonitor.from_yaml(\n    \"config/monitoring.yaml\",\n    overrides={\n        \"source.limit\": 50,\n        \"sampling.strategy\": \"most_recent\",\n        \"sampling.n\": 5,\n    },\n)\n</code></pre>"},{"location":"guides/monitoring/","title":"Online Monitoring","text":"<p>Online monitoring module with pluggable data sources and deduplication. Supports multiple data sources (Langfuse, Slack, etc.) and tracks scored items to avoid re-processing. Can be scheduled via APScheduler (local/dev) or GitHub Actions cron (production).</p>"},{"location":"guides/monitoring/#scored-items-store","title":"Scored Items Store","text":"<p>The <code>ScoredItemsStore</code> abstract base class provides deduplication to avoid re-processing items that have already been evaluated. Two implementations are available:</p> <ul> <li>CSVScoredItemsStore: File-based storage using CSV (pandas for reads, raw CSV for appends)</li> <li>DBScoredItemsStore: Database-backed storage using the <code>evaluation_dataset</code> table</li> </ul>"},{"location":"guides/monitoring/#csv-backed-deduplication","title":"CSV-backed deduplication","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, CSVScoredItemsStore\n\nstore = CSVScoredItemsStore(\"data/scored_items.csv\")\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\", scored_store=store)\nresults = monitor.run(publish=True)\n</code></pre> <p>You can also configure a scored store directly in YAML:</p> <pre><code>scored_store:\n  type: csv\n  file_path: \"data/scored_items.csv\"\n</code></pre>"},{"location":"guides/monitoring/#database-backed-deduplication","title":"Database-backed deduplication","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, DBScoredItemsStore\n\n# Uses DATABASE_URL environment variable\nstore = DBScoredItemsStore()\n\n# Or with explicit connection string\nstore = DBScoredItemsStore(connection_string=\"postgresql://...\")\n\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\", scored_store=store)\nresults = monitor.run(publish=True)\n</code></pre> <p>YAML version:</p> <pre><code>scored_store:\n  type: db\n  connection_string: \"postgresql://...\"\n</code></pre>"},{"location":"guides/monitoring/#scheduling-via-yaml","title":"Scheduling via YAML","text":"<p>If you use <code>MonitoringScheduler</code>, you can define the interval or cron in YAML:</p> <pre><code>schedule:\n  interval_minutes: 10\n  # cron: \"*/10 * * * *\"\n</code></pre>"},{"location":"guides/monitoring/#no-deduplication","title":"No deduplication","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor\n\nmonitor = OnlineMonitor.from_yaml(\"config/monitoring.yaml\")  # scored_store=None\nresults = monitor.run(deduplicate=False, publish=True)\n</code></pre>"},{"location":"guides/monitoring/#example-programmatic-setup","title":"Example - Programmatic setup","text":"<pre><code>from eval_workbench.shared.monitoring import OnlineMonitor, LangfuseDataSource, DBScoredItemsStore\n\nsource = LangfuseDataSource(\n    name=\"athena\",\n    extractor=extract_fn,\n    limit=100,\n    hours_back=2,\n)\nstore = DBScoredItemsStore()\nmonitor = OnlineMonitor(\n    name=\"athena_monitor\",\n    source=source,\n    metrics_config={\"Metric\": {\"class\": \"metric_class\"}},\n    scored_store=store,\n)\nresults = monitor.run()\n</code></pre>"},{"location":"guides/monitoring/#example-scheduled-monitoring","title":"Example - Scheduled monitoring","text":"<pre><code>from eval_workbench.shared.monitoring import MonitoringScheduler, DBScoredItemsStore\n\nstore = DBScoredItemsStore()\nscheduler = MonitoringScheduler(scored_store=store)\n\nscheduler.add_monitor(\"config/monitoring.yaml\", interval_minutes=60)\nscheduler.start()\n</code></pre>"},{"location":"guides/monitoring/#example-config-access","title":"Example - Config access","text":"<pre><code>from shared import config\n\nconfig.load(\"config/monitoring.yaml\")\nlimit = config.get(\"trace_loader.limit\")\n\n# Temporary config overrides\nwith config.set({\"trace_loader.limit\": 10}):\n    pass\n</code></pre>"},{"location":"guides/shared-langfuse/","title":"Langfuse Integration (Shared)","text":"<p>The shared Langfuse module provides prompt management, trace handling, and webhook integration for LLM observability. It enables intelligent access to prompts and traces with smart dot-notation access patterns.</p>"},{"location":"guides/shared-langfuse/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   LangfusePromptManager                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 \u2022 create_or_update_prompt()  [\u2192 PromptClient]           \u2502   \u2502\n\u2502  \u2502 \u2022 get_prompt()               [\u2192 PromptClient + caching] \u2502   \u2502\n\u2502  \u2502 \u2022 get_compiled_prompt()      [\u2192 str|List with variables]\u2502   \u2502\n\u2502  \u2502 \u2022 get_template()             [\u2192 raw template]           \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u2022 mark_prompt_as_stale(name)                                    \u2502\n\u2502  \u2022 notify_prompt_change(name, payload)                           \u2502\n\u2502  \u2022 on_prompt_change(listener) [registers callback]               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2195\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Webhook Handler                              \u2502\n\u2502  POST /webhooks/langfuse                                        \u2502\n\u2502  \u2022 verify_signature()  [HMAC-SHA256]                            \u2502\n\u2502  \u2022 Handles: prompt.{created,updated,deleted}                    \u2502\n\u2502  \u2022 Marks stale \u2192 Notifies listeners \u2192 Posts Slack alerts        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2195\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Trace Access &amp; Analysis (trace.py)                 \u2502\n\u2502                                                                 \u2502\n\u2502  Trace(trace_data, prompt_patterns)                             \u2502\n\u2502   \u251c\u2500 trace.recommendation  \u2192 TraceStep                          \u2502\n\u2502   \u2502   \u251c\u2500 step.generation   \u2192 ObservationsView                   \u2502\n\u2502   \u2502   \u251c\u2500 step.context      \u2192 ObservationsView                   \u2502\n\u2502   \u2502   \u2514\u2500 step.variables    \u2192 Dict[extracted variables]          \u2502\n\u2502   \u251c\u2500 trace.id              \u2192 trace attribute                    \u2502\n\u2502   \u2514\u2500 trace.latency         \u2192 trace attribute                    \u2502\n\u2502                                                                 \u2502\n\u2502  SmartAccess Framework (dot-notation + fuzzy matching)          \u2502\n\u2502   \u251c\u2500 SmartDict (dict wrapper)                                   \u2502\n\u2502   \u251c\u2500 SmartObject (object wrapper)                               \u2502\n\u2502   \u2514\u2500 Recursive wrapping of nested structures                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/shared-langfuse/#configuration","title":"Configuration","text":""},{"location":"guides/shared-langfuse/#langfusesettings","title":"LangfuseSettings","text":"<pre><code>class LangfuseSettings(RepoSettingsBase):\n    # API Credentials\n    langfuse_public_key: str | None = None\n    langfuse_secret_key: str | None = None\n    langfuse_host: str | None = None\n\n    # Prompt Management\n    langfuse_default_label: str = \"production\"\n    langfuse_default_cache_ttl_seconds: int = 60\n\n    # Webhook Configuration\n    langfuse_webhook_secret: str | None = None\n    langfuse_webhook_notify_url: str | None = None\n\n    # Slack Alerts\n    langfuse_slack_channel_id: str | None = None\n    langfuse_slack_request_timeout_seconds: float = 10\n    langfuse_slack_retry_max_attempts: int = 3\n    langfuse_slack_retry_backoff_seconds: float = 0.5\n    langfuse_slack_retry_max_backoff_seconds: float = 4.0\n</code></pre> Setting Default Description <code>langfuse_default_label</code> <code>\"production\"</code> Default prompt label <code>langfuse_default_cache_ttl_seconds</code> <code>60</code> Local cache duration <code>langfuse_slack_request_timeout_seconds</code> <code>10</code> Slack API timeout <code>langfuse_slack_retry_max_attempts</code> <code>3</code> Retry count for Slack"},{"location":"guides/shared-langfuse/#langfusepromptmanager","title":"LangfusePromptManager","text":"<p>Central manager for all Langfuse prompt operations including CRUD, caching, invalidation, and change notifications.</p>"},{"location":"guides/shared-langfuse/#initialization","title":"Initialization","text":"<pre><code>from eval_workbench.shared.langfuse.prompt import LangfusePromptManager\n\nmanager = LangfusePromptManager(\n    public_key=\"pk-...\",      # Optional, uses env if not provided\n    secret_key=\"sk-...\",      # Optional, uses env if not provided\n    host=\"https://...\",       # Optional, uses env if not provided\n)\n</code></pre>"},{"location":"guides/shared-langfuse/#creating-and-updating-prompts","title":"Creating and Updating Prompts","text":"<pre><code>def create_or_update_prompt(\n    self,\n    name: str,\n    prompt_content: Union[str, List[Dict[str, str]]],\n    prompt_type: str = \"text\",           # \"text\" or \"chat\"\n    labels: Optional[Sequence[str]] = None,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; PromptClient\n</code></pre> <p>Example: <pre><code># Text prompt\nmanager.create_or_update_prompt(\n    name=\"recommendation\",\n    prompt_content=\"Analyze this case: {{ case_data }}\",\n    prompt_type=\"text\",\n    labels=[\"production\", \"v2\"],\n    config={\"model\": \"gpt-4\", \"temperature\": 0.7},\n)\n\n# Chat prompt\nmanager.create_or_update_prompt(\n    name=\"chat_assistant\",\n    prompt_content=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"{{ user_message }}\"},\n    ],\n    prompt_type=\"chat\",\n    labels=[\"production\"],\n)\n</code></pre></p>"},{"location":"guides/shared-langfuse/#fetching-prompts","title":"Fetching Prompts","text":"<pre><code>def get_prompt(\n    self,\n    name: str,\n    version: Optional[int] = None,\n    label: Optional[str] = None,\n    cache_ttl_seconds: Optional[int] = None,\n    retry_count: int = 0,\n    retry_backoff_seconds: float = 0.2,\n) -&gt; PromptClient\n</code></pre> <p>Behavior: - Defaults to \"production\" label if version/label omitted - Checks <code>_stale_prompts</code> and bypasses cache if flagged - Implements exponential backoff retry logic</p>"},{"location":"guides/shared-langfuse/#compiling-prompts-with-variables","title":"Compiling Prompts with Variables","text":"<pre><code>def get_compiled_prompt(\n    self,\n    name: str,\n    variables: Dict[str, Any],\n    *,\n    fallback: Optional[Union[str, List[Dict[str, Any]]]] = None,\n    strict: bool = True,\n    required_variables: Optional[Sequence[str]] = None,\n    strict_mode: Literal[\"template\", \"required\", \"none\"] = \"template\",\n    fallback_on_compile_error: bool = True,\n    render_with: Optional[Callable] = None,\n    **kwargs,\n) -&gt; Union[str, List[Dict]]\n</code></pre> <p>Strict Mode Options: - <code>\"template\"</code>: Check for unrendered placeholders after compilation - <code>\"required\"</code>: Verify required_variables are present before compilation - <code>\"none\"</code>: Skip all strict checks</p> <p>Example: <pre><code>compiled = manager.get_compiled_prompt(\n    \"recommendation\",\n    variables={\"case_id\": \"123\", \"assessment\": \"high-risk\"},\n    strict_mode=\"template\",\n    fallback=\"Default prompt text\",\n)\n</code></pre></p>"},{"location":"guides/shared-langfuse/#cache-invalidation","title":"Cache Invalidation","text":"<pre><code># Mark prompt for immediate refresh on next fetch\nmanager.mark_prompt_as_stale(\"recommendation\")\n\n# Promote specific version to labels\nmanager.promote_version(\"recommendation\", version=5, labels=[\"production\"])\n</code></pre>"},{"location":"guides/shared-langfuse/#change-notifications","title":"Change Notifications","text":"<pre><code># Register listener\ndef on_update(name: str, payload: Dict[str, Any] | None):\n    print(f\"Prompt {name} updated: {payload}\")\n\nmanager.on_prompt_change(on_update)\n\n# Manually trigger notifications\nmanager.notify_prompt_change(\"recommendation\", payload={\"version\": 6})\n</code></pre>"},{"location":"guides/shared-langfuse/#smartaccess-framework","title":"SmartAccess Framework","text":"<p>The SmartAccess system enables intelligent dot-notation access with fuzzy matching across trace data.</p>"},{"location":"guides/shared-langfuse/#smartaccess-base-class","title":"SmartAccess (Base Class)","text":"<pre><code>class SmartAccess:\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Dot-notation with 2-step fallback:\n        1. Exact match via _lookup(key)\n        2. Fuzzy match via _lookup_insensitive(key)\n        \"\"\"\n\n    def _wrap(self, val: Any) -&gt; Any:\n        \"\"\"Recursively wrap results:\n        - dict \u2192 SmartDict\n        - list \u2192 list of wrapped items\n        - object \u2192 SmartObject\n        \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#fuzzy-matching","title":"Fuzzy Matching","text":"<p>Keys are normalized for case/separator-insensitive matching:</p> <pre><code>def _normalize_key(key: str) -&gt; str:\n    return key.lower().replace(\"_\", \"\")\n\n# Examples:\n# \"product_type\" matches \"productType\"\n# \"CaseAssessment\" matches \"caseassessment\"\n</code></pre>"},{"location":"guides/shared-langfuse/#smartdict","title":"SmartDict","text":"<p>Dictionary wrapper with dot-notation and fuzzy key matching.</p> <pre><code>smart_dict = SmartDict({\"productType\": \"insurance\", \"caseId\": \"123\"})\nsmart_dict.product_type  # \"insurance\" (fuzzy match)\nsmart_dict.case_id       # \"123\" (fuzzy match)\nsmart_dict.to_dict()     # Returns raw dictionary\n</code></pre>"},{"location":"guides/shared-langfuse/#smartobject","title":"SmartObject","text":"<p>Generic object wrapper ensuring attributes return Smart wrappers.</p> <pre><code>smart_obj = SmartObject(some_object)\nsmart_obj.nested_attr.deep_value  # Recursive wrapping\n</code></pre>"},{"location":"guides/shared-langfuse/#trace-classes","title":"Trace Classes","text":""},{"location":"guides/shared-langfuse/#trace","title":"Trace","text":"<p>Main entry point for accessing trace data with smart dot-notation navigation.</p> <pre><code>class Trace(SmartAccess):\n    def __init__(\n        self,\n        trace_data: Any,\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    )\n</code></pre> <p>Access Patterns:</p> Access Returns Description <code>trace.{step_name}</code> <code>TraceStep</code> Named workflow step <code>trace.id</code> <code>str</code> Trace identifier <code>trace.latency</code> <code>int</code> Execution time <code>trace.name</code> <code>str</code> Trace name"},{"location":"guides/shared-langfuse/#tracestep","title":"TraceStep","text":"<p>Represents a named workflow step (e.g., \"recommendation\") with observations.</p> <pre><code>class TraceStep(SmartAccess):\n    name: str                              # Step name\n    observations: List[ObservationsView]   # Observations in step\n    prompt_patterns: PromptPatternsBase    # Pattern registry\n</code></pre> <p>Properties:</p> Access Returns Description <code>step.variables</code> <code>Dict[str, str]</code> Lazily extracted prompt variables <code>step.generation</code> <code>ObservationsView</code> GENERATION observation <code>step.context</code> <code>ObservationsView</code> SPAN observation (alias) <code>step.span</code> <code>ObservationsView</code> SPAN observation"},{"location":"guides/shared-langfuse/#tracecollection","title":"TraceCollection","text":"<p>Container for multiple traces with filtering and iteration.</p> <pre><code>class TraceCollection:\n    def __init__(\n        self,\n        data: List[Any],\n        prompt_patterns: PromptPatternsBase | type[PromptPatternsBase] | None = None,\n    )\n\n    def __getitem__(self, index: int) -&gt; Trace: ...\n    def __iter__(self) -&gt; Iterator[Trace]: ...\n    def __len__(self) -&gt; int: ...\n    def filter_by(self, **kwargs) -&gt; TraceCollection: ...\n</code></pre> <p>Example: <pre><code>traces = TraceCollection(trace_list, prompt_patterns=WorkflowPromptPatterns)\n\n# Filter\nfiltered = traces.filter_by(name=\"athena\", status=\"completed\")\n\n# Iterate\nfor trace in traces:\n    print(trace.id, trace.recommendation.variables)\n</code></pre></p>"},{"location":"guides/shared-langfuse/#promptpatternsbase","title":"PromptPatternsBase","text":"<p>Registry for regex extraction patterns. Enables custom pattern definitions per workflow.</p> <pre><code>class PromptPatternsBase:\n    @classmethod\n    def get_for(cls, step_name: str) -&gt; Dict[str, str]:\n        \"\"\"\n        Looks for method: _patterns_{step_name_lowercase}()\n        Returns: Dict[pattern_name: pattern_regex]\n        \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#creating-custom-patterns","title":"Creating Custom Patterns","text":"<pre><code>from eval_workbench.shared.langfuse.trace import PromptPatternsBase, create_extraction_pattern\n\nclass CustomPromptPatterns(PromptPatternsBase):\n    @staticmethod\n    def _patterns_recommendation() -&gt; Dict[str, str]:\n        return {\n            \"caseAssessment\": create_extraction_pattern(\n                \"CASE ASSESSMENT\",\n                \"CONTEXT DATA\"\n            ),\n            \"contextData\": create_extraction_pattern(\n                \"CONTEXT DATA\",\n                \"(?:$)\"  # End of text\n            ),\n        }\n</code></pre>"},{"location":"guides/shared-langfuse/#helper-function","title":"Helper Function","text":"<pre><code>def create_extraction_pattern(start_text: str, end_pattern: str) -&gt; str:\n    \"\"\"\n    Creates regex: escaped(Start) \u2192 (Content) \u2192 End\n\n    Example:\n        create_extraction_pattern(\"CONTEXT:\", \"FLAGS:\")\n        \u2192 r\"CONTEXT:\\s*(.*?)\\s*(?:FLAGS:)\"\n    \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#webhook-handler","title":"Webhook Handler","text":"<p>The webhook handler processes Langfuse prompt update events.</p>"},{"location":"guides/shared-langfuse/#endpoint","title":"Endpoint","text":"<pre><code>POST /webhooks/langfuse\nHeader: X-Langfuse-Signature: t=&lt;timestamp&gt;,v1=&lt;signature&gt;\n</code></pre>"},{"location":"guides/shared-langfuse/#signature-verification","title":"Signature Verification","text":"<pre><code>def verify_signature(\n    payload: bytes,\n    signature_header: str | None,\n    secret: str\n) -&gt; None:\n    \"\"\"\n    Verifies HMAC-SHA256 signature.\n    Supports both timestamped and non-timestamped formats.\n    \"\"\"\n</code></pre>"},{"location":"guides/shared-langfuse/#event-handling","title":"Event Handling","text":"<p>Supported Events: - <code>prompt.created</code> - New prompt created - <code>prompt.updated</code> - Existing prompt modified - <code>prompt.deleted</code> - Prompt removed</p> <p>Processing Flow: 1. Verify HMAC signature 2. Parse JSON payload 3. Extract prompt name 4. Mark prompt as stale in manager 5. Notify registered listeners 6. Notify external services (if configured) 7. Post Slack alert (async, non-blocking)</p>"},{"location":"guides/shared-langfuse/#slack-alerts","title":"Slack Alerts","text":"<p>When configured, posts formatted alerts to Slack on prompt changes:</p> <pre><code># Requires environment variables:\n# LANGFUSE_SLACK_CHANNEL_ID - Target Slack channel\n# SLACK_ATHENA_TOKEN - Bot token for posting\n</code></pre>"},{"location":"guides/shared-langfuse/#utility-functions","title":"Utility Functions","text":""},{"location":"guides/shared-langfuse/#parse_chat_transcript","title":"parse_chat_transcript","text":"<p>Parses raw string chat transcript into structured messages.</p> <pre><code>from eval_workbench.shared.langfuse.utils import parse_chat_transcript\n\ntranscript = \"\"\"\nAthena: Hello, how can I help?\nUser: I have a question about my policy.\nAthena: I'd be happy to help with that.\n\"\"\"\n\nmessages = parse_chat_transcript(transcript, agent_name=\"Athena\")\n# [\n#     {\"role\": \"assistant\", \"content\": \"Hello, how can I help?\"},\n#     {\"role\": \"user\", \"content\": \"I have a question about my policy.\"},\n#     {\"role\": \"assistant\", \"content\": \"I'd be happy to help with that.\"},\n# ]\n</code></pre>"},{"location":"guides/shared-langfuse/#code-examples","title":"Code Examples","text":""},{"location":"guides/shared-langfuse/#prompt-management","title":"Prompt Management","text":"<pre><code>from eval_workbench.shared.langfuse.prompt import LangfusePromptManager\n\nmanager = LangfusePromptManager()\n\n# Fetch and compile with variables\ncompiled = manager.get_compiled_prompt(\n    \"recommendation\",\n    variables={\"case_id\": \"123\", \"assessment\": \"high-risk\"},\n    strict_mode=\"template\",\n    fallback=\"Default prompt text\",\n)\n\n# Register for updates\ndef on_update(name, payload):\n    print(f\"Prompt {name} updated: {payload}\")\n\nmanager.on_prompt_change(on_update)\n</code></pre>"},{"location":"guides/shared-langfuse/#trace-analysis","title":"Trace Analysis","text":"<pre><code>from eval_workbench.shared.langfuse.trace import Trace, TraceCollection\nfrom eval_workbench.implementations.athena.langfuse.prompt_patterns import WorkflowPromptPatterns\n\n# Single trace\ntrace = Trace(trace_data, prompt_patterns=WorkflowPromptPatterns)\nassessment = trace.recommendation.variables[\"caseAssessment\"]\ngeneration_output = trace.recommendation.generation.output\n\n# Multiple traces\ntraces = TraceCollection(trace_list, prompt_patterns=WorkflowPromptPatterns)\nfor trace in traces:\n    if trace.id in important_ids:\n        print(trace.recommendation.generation.output)\n</code></pre>"},{"location":"guides/shared-langfuse/#deep-trace-navigation","title":"Deep Trace Navigation","text":"<pre><code>trace = Trace(trace_data, prompt_patterns=WorkflowPromptPatterns)\n\n# Access step\nrec = trace.recommendation\n\n# Get generation observation\ngen = rec.generation\nprint(gen.input)   # Prompt input\nprint(gen.output)  # Model output\nprint(gen.model)   # Model used\n\n# Extract variables (lazy evaluation)\nvars = rec.variables\nprint(vars[\"caseAssessment\"])\nprint(vars[\"contextData\"])\n\n# Access trace metadata\nprint(trace.id)\nprint(trace.latency)\nprint(trace.name)\n</code></pre>"},{"location":"guides/shared-langfuse/#exports","title":"Exports","text":"<pre><code>from eval_workbench.shared.langfuse.prompt import (\n    LangfuseSettings,\n    get_langfuse_settings,\n    LangfusePromptManager,\n)\n\nfrom eval_workbench.shared.langfuse.trace import (\n    # Enums\n    ModelUsageUnit,\n    ObservationLevel,\n\n    # Dataclasses\n    Usage,\n\n    # SmartAccess Framework\n    SmartAccess,\n    SmartDict,\n    SmartObject,\n\n    # Trace Views\n    TraceView,\n    ObservationsView,\n\n    # Pattern Extraction\n    create_extraction_pattern,\n    PromptPatternsBase,\n\n    # Main Classes\n    TraceStep,\n    Trace,\n    TraceCollection,\n)\n\nfrom eval_workbench.shared.langfuse.webhook import (\n    verify_signature,\n    langfuse_webhook,\n)\n\nfrom eval_workbench.shared.langfuse.utils import (\n    parse_chat_transcript,\n)\n</code></pre>"},{"location":"guides/slack/","title":"Slack Integration","text":"<p>The Slack integration module provides a complete, production-ready interface for Slack interactions with async support, retry logic, message parsing, formatting validation, and subscription management.</p>"},{"location":"guides/slack/#architecture","title":"Architecture","text":"<pre><code>SlackConfig (Token Management)\n    \u2514\u2500\u2500 get_token(agent_id) -&gt; str\n\nSlackHttpClient (Low-level Async)\n    \u251c\u2500\u2500 request() -&gt; with retry logic\n    \u2514\u2500\u2500 _get_session()\n\nSlackService (High-level API)\n    \u251c\u2500\u2500 post_message()\n    \u251c\u2500\u2500 update_message()\n    \u251c\u2500\u2500 post_threaded_message()\n    \u251c\u2500\u2500 get_thread_replies()\n    \u251c\u2500\u2500 get_channel_history()\n    \u251c\u2500\u2500 open_modal()\n    \u251c\u2500\u2500 publish_app_home()\n    \u251c\u2500\u2500 download_file()\n    \u251c\u2500\u2500 post_to_subscribed_channels()\n    \u2514\u2500\u2500 post_to_all_threads()\n\nSlackScraper (Message Parsing)\n    \u251c\u2500\u2500 extract_text_from_blocks()\n    \u251c\u2500\u2500 extract_text_from_attachments()\n    \u251c\u2500\u2500 simplify_message()\n    \u251c\u2500\u2500 simplify_message_extended()\n    \u2514\u2500\u2500 extract_thread_metadata()\n\nSlackBlockBuilder (Message Formatting)\n    \u251c\u2500\u2500 format_prompt_change_alert()\n    \u251c\u2500\u2500 format_chat_response()\n    \u2514\u2500\u2500 create_error_message()\n</code></pre>"},{"location":"guides/slack/#slackconfig-token-management","title":"SlackConfig (Token Management)","text":"<p>Centralized management of Slack bot tokens for multiple agents.</p> <pre><code>class SlackConfig:\n    ATHENA_TOKEN = os.getenv(\"SLACK_ATHENA_TOKEN\")\n    AIMEE_TOKEN = os.getenv(\"SLACK_AIMEE_TOKEN\")\n    CANARY_TOKEN = os.getenv(\"SLACK_CANARY_TOKEN\")\n    PROMETHEUS_TOKEN = os.getenv(\"SLACK_PROMETHEUS_TOKEN\")\n    QUILL_TOKEN = os.getenv(\"SLACK_QUILL_TOKEN\")\n\n    @classmethod\n    def get_token(\n        cls,\n        agent_id: Optional[str] = None,\n        override_token: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"\n        Get token by priority:\n        1. override_token if provided\n        2. agent_map[agent_id] if agent_id provided\n        3. ATHENA_TOKEN as fallback\n        \"\"\"\n</code></pre> <p>Supported Agents: <code>\"athena\"</code>, <code>\"aimee\"</code>, <code>\"canary\"</code>, <code>\"quill\"</code>, <code>\"prometheus\"</code></p>"},{"location":"guides/slack/#slackhttpclient-async-client-with-retry","title":"SlackHttpClient (Async Client with Retry)","text":"<p>Low-level async HTTP client with exponential backoff retry strategy.</p> <pre><code>class SlackHttpClient:\n    def __init__(\n        self,\n        *,\n        timeout_seconds: float = 10,\n        max_attempts: int = 3,\n        backoff_seconds: float = 0.5,\n        max_backoff_seconds: float = 4.0,\n        jitter_seconds: float = 0.1,\n    ): ...\n\n    async def request(\n        self,\n        *,\n        method: str,\n        url: str,\n        token: str,\n        json_data: Any = None,\n        data: Any = None,\n        headers: Dict | None = None,\n        params: Dict | None = None,\n    ) -&gt; Dict[str, Any]: ...\n</code></pre> <p>Retry Strategy: 1. Detects rate limiting (HTTP 429 or \"ratelimited\" error) 2. Detects server errors (HTTP 5xx) 3. Exponential backoff: <code>delay = backoff_seconds * (2 ^ (attempt - 1))</code> 4. Adds random jitter to prevent thundering herd 5. Respects Slack's <code>Retry-After</code> header if provided</p>"},{"location":"guides/slack/#slackscraper-message-parsing","title":"SlackScraper (Message Parsing)","text":"<p>Extract and simplify raw Slack message data into readable formats.</p>"},{"location":"guides/slack/#extract_text_from_blocks","title":"extract_text_from_blocks()","text":"<pre><code>@staticmethod\ndef extract_text_from_blocks(blocks: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"\n    Extracts text from Slack Block Kit elements:\n    - section blocks: text.text\n    - rich_text blocks: rich_text_section elements\n    - context blocks: element text field\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#extract_text_from_attachments","title":"extract_text_from_attachments()","text":"<pre><code>@staticmethod\ndef extract_text_from_attachments(attachments: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"Handles legacy Slack attachments.\"\"\"\n</code></pre>"},{"location":"guides/slack/#simplify_message","title":"simplify_message()","text":"<pre><code>@staticmethod\ndef simplify_message(raw: Dict[str, Any]) -&gt; SimplifiedMessage:\n    \"\"\"\n    Returns SimplifiedMessage with:\n    - ts: Message timestamp\n    - sender: Human-readable sender name\n    - is_bot: Boolean flag\n    - content: Concatenated, deduplicated, normalized text\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#simplify_message_extended","title":"simplify_message_extended()","text":"<pre><code>@staticmethod\ndef simplify_message_extended(raw: Dict[str, Any]) -&gt; ExtendedSimplifiedMessage:\n    \"\"\"\n    Returns ExtendedSimplifiedMessage with additional fields:\n    - timestamp_utc: Parsed datetime\n    - user_id: Slack user ID\n    - reply_count: Number of thread replies\n    - message_url: Permalink to message\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#extract_thread_metadata","title":"extract_thread_metadata()","text":"<pre><code>@staticmethod\ndef extract_thread_metadata(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns:\n    - thread_created_at: Earliest message timestamp\n    - thread_last_activity_at: Latest message timestamp\n    - human_participants: List of non-bot user IDs\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#slackservice-readwrite-operations","title":"SlackService (Read/Write Operations)","text":"<p>High-level API for Slack interactions.</p> <pre><code>class SlackService:\n    def __init__(\n        self,\n        storage: SubscriptionStorage = None,  # Default: InMemorySubscriptionStorage\n        client: SlackHttpClient = None         # Default: shared global client\n    ): ...\n</code></pre>"},{"location":"guides/slack/#write-operations","title":"Write Operations","text":""},{"location":"guides/slack/#post_message","title":"post_message()","text":"<pre><code>async def post_message(\n    channel: str,\n    options: PostMessageOptions,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"\n    Posts a message to a channel.\n    - Validates token and message content\n    - Applies payload limits (text: 40000 chars, block text: 3000 chars)\n    - Posts to chat.postMessage endpoint\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#update_message","title":"update_message()","text":"<pre><code>async def update_message(\n    channel: str,\n    ts: str,\n    options: PostMessageOptions,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"Updates an existing message by timestamp.\"\"\"\n</code></pre>"},{"location":"guides/slack/#post_threaded_message","title":"post_threaded_message()","text":"<pre><code>async def post_threaded_message(\n    channel: str,\n    thread_ts: str,\n    options: PostMessageOptions,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"\n    Posts a reply in a thread.\n    - Validates thread exists\n    - Sets thread_ts in options\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#readscrape-operations","title":"Read/Scrape Operations","text":""},{"location":"guides/slack/#get_thread_replies","title":"get_thread_replies()","text":"<pre><code>async def get_thread_replies(\n    channel: str,\n    thread_ts: str,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"\n    Retrieves all replies in a thread with automatic pagination.\n    - Endpoint: conversations.replies\n    - Batch size: 200\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#get_channel_history","title":"get_channel_history()","text":"<pre><code>async def get_channel_history(\n    channel: str,\n    limit: int = 100,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"Fetches channel message history with pagination.\"\"\"\n</code></pre>"},{"location":"guides/slack/#interaction-operations","title":"Interaction Operations","text":""},{"location":"guides/slack/#open_modal","title":"open_modal()","text":"<pre><code>async def open_modal(\n    trigger_id: str,\n    view: Dict,\n    agent_id: Optional[str] = None\n) -&gt; SlackResponse:\n    \"\"\"Opens a modal view dialog.\"\"\"\n</code></pre>"},{"location":"guides/slack/#publish_app_home","title":"publish_app_home()","text":"<pre><code>async def publish_app_home(\n    agent_id: str,\n    user_id: str\n) -&gt; None:\n    \"\"\"Publishes the App Home tab view for a user.\"\"\"\n</code></pre>"},{"location":"guides/slack/#download_file","title":"download_file()","text":"<pre><code>async def download_file(\n    file: SlackFile,\n    agent_id: Optional[str] = None\n) -&gt; FileDownloadResponse:\n    \"\"\"\n    Downloads a file from Slack with redirect handling.\n    - Handles up to 5 redirects to preserve auth headers\n    - Validates content type\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#multi-channel-broadcasting","title":"Multi-Channel Broadcasting","text":""},{"location":"guides/slack/#post_to_subscribed_channels","title":"post_to_subscribed_channels()","text":"<pre><code>async def post_to_subscribed_channels(\n    agent_id: str,\n    event_type: str,\n    event_data: Dict[str, str],\n    message_options: PostMessageOptions\n) -&gt; MultiChannelPostResult:\n    \"\"\"Posts to all subscribed channels matching event type and filters.\"\"\"\n</code></pre>"},{"location":"guides/slack/#post_to_all_threads","title":"post_to_all_threads()","text":"<pre><code>async def post_to_all_threads(\n    threads: List[SlackThread],\n    message_options: PostMessageOptions,\n    agent_id: str\n) -&gt; None:\n    \"\"\"Broadcasts reply to multiple tracked threads.\"\"\"\n</code></pre>"},{"location":"guides/slack/#slackblockbuilder-block-kit-formatting","title":"SlackBlockBuilder (Block Kit Formatting)","text":"<p>Utility class for constructing Slack Block Kit formatted messages.</p>"},{"location":"guides/slack/#format_prompt_change_alert","title":"format_prompt_change_alert()","text":"<pre><code>@staticmethod\ndef format_prompt_change_alert(\n    prompt_name: str,\n    event_type: str,\n    prompt_version: Optional[Union[int, str]] = None,\n    prompt_id: Optional[str] = None,\n    prompt_url: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Creates notification for Langfuse prompt updates.\"\"\"\n</code></pre>"},{"location":"guides/slack/#format_chat_response","title":"format_chat_response()","text":"<pre><code>@staticmethod\ndef format_chat_response(\n    message: str,\n    citations: List[Dict] = None,\n    is_learning_worthy: bool = False,\n    is_feature_request: bool = False\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Creates AI chat response message with:\n    - Main message block\n    - Optional citations block\n    - Optional action buttons (Save as Learning, Create Feature Request)\n    \"\"\"\n</code></pre>"},{"location":"guides/slack/#create_error_message","title":"create_error_message()","text":"<pre><code>@staticmethod\ndef create_error_message(error: str) -&gt; Dict[str, Any]:\n    \"\"\"Creates error alert message with section block.\"\"\"\n</code></pre>"},{"location":"guides/slack/#typeddicts-and-dataclasses","title":"TypedDicts and Dataclasses","text":""},{"location":"guides/slack/#slackresponse","title":"SlackResponse","text":"<pre><code>class SlackResponse(TypedDict, total=False):\n    success: bool\n    ts: Optional[str]                        # Message timestamp\n    channel: Optional[str]                   # Channel ID\n    error: Optional[str]                     # Error message\n    status: Optional[int]                    # HTTP status code\n    messages: Optional[List[Dict[str, Any]]] # Array of message objects\n    user: Optional[Dict[str, Any]]\n</code></pre>"},{"location":"guides/slack/#simplifiedmessage","title":"SimplifiedMessage","text":"<pre><code>class SimplifiedMessage(TypedDict):\n    ts: str           # Message timestamp\n    sender: str       # Sender name/bot name\n    is_bot: bool      # Is from a bot\n    content: str      # Consolidated text content\n</code></pre>"},{"location":"guides/slack/#extendedsimplifiedmessage","title":"ExtendedSimplifiedMessage","text":"<pre><code>class ExtendedSimplifiedMessage(TypedDict):\n    ts: str\n    timestamp_utc: Optional[datetime]   # Parsed UTC datetime\n    sender: str\n    user_id: Optional[str]              # Slack user ID\n    is_bot: bool\n    content: str\n    reply_count: int                    # Number of thread replies\n    message_url: Optional[str]          # Message permalink\n</code></pre>"},{"location":"guides/slack/#postmessageoptions","title":"PostMessageOptions","text":"<pre><code>class PostMessageOptions(TypedDict, total=False):\n    text: Optional[str]\n    blocks: Optional[List[Dict[str, Any]]]\n    attachments: Optional[List[Dict[str, Any]]]\n    thread_ts: Optional[str]\n    reply_broadcast: Optional[bool]\n    unfurl_links: Optional[bool]\n    unfurl_media: Optional[bool]\n    slackToken: Optional[str]           # Override token\n</code></pre>"},{"location":"guides/slack/#slackfile","title":"SlackFile","text":"<pre><code>@dataclass\nclass SlackFile:\n    id: str\n    name: str\n    mimetype: str\n    size: int\n    url_private: str\n    url_private_download: Optional[str] = None\n</code></pre>"},{"location":"guides/slack/#filedownloadresponse","title":"FileDownloadResponse","text":"<pre><code>@dataclass\nclass FileDownloadResponse:\n    success: bool\n    buffer: Optional[bytes] = None\n    filename: Optional[str] = None\n    mimetype: Optional[str] = None\n    error: Optional[str] = None\n</code></pre>"},{"location":"guides/slack/#slacksubscription","title":"SlackSubscription","text":"<pre><code>@dataclass\nclass SlackSubscription:\n    id: str\n    agent_id: str                 # Agent receiving events\n    event_type: str               # Type of event\n    channel_id: str               # Where to post\n    filters: Dict[str, str]       # Event filter conditions\n    created_by_slack_user_id: str\n    created_by_username: Optional[str]\n    active: bool\n    created_at: str\n    updated_at: str\n</code></pre>"},{"location":"guides/slack/#slackthread","title":"SlackThread","text":"<pre><code>@dataclass\nclass SlackThread:\n    channel_id: str\n    thread_ts: str       # Thread message timestamp\n    posted_at: str       # ISO format timestamp\n</code></pre>"},{"location":"guides/slack/#multichannelpostresult","title":"MultiChannelPostResult","text":"<pre><code>@dataclass\nclass MultiChannelPostResult:\n    threads: List[SlackThread]\n    errors: List[Dict[str, str]]\n</code></pre>"},{"location":"guides/slack/#slackformattingcompliance-metric","title":"SlackFormattingCompliance Metric","text":"<p>Located at <code>src/shared/metrics/slack_compliance.py</code>, this metric validates AI outputs comply with Slack formatting rules.</p> <pre><code>class SlackFormattingCompliance(BaseMetric):\n    key = \"slack_compliance\"\n    required_fields = [\"actual_output\"]\n    default_threshold = 1.0\n</code></pre> <p>Validation Checks: 1. Double Asterisks: Detects <code>**text**</code> (should be <code>*text*</code> in Slack) 2. Headers: Detects <code># Header</code> markdown (forbidden in Slack) 3. Unwrapped Data: Detects <code>$500</code> or <code>50%</code> not in backticks</p> <p>Scoring: Deducts 0.25 per issue type found, minimum 0.0</p>"},{"location":"guides/slack/#formatting-rules-and-compliance","title":"Formatting Rules and Compliance","text":""},{"location":"guides/slack/#general-slack-formatting-rules","title":"General Slack Formatting Rules","text":"<pre><code>SLACK_FORMATTING_RULES = \"\"\"\n- NO using # for headers (Slack doesn't render them)\n- Use *single asterisks* for bold (NOT **double**)\n- Use backticks for variables, numbers, and technical terms\n\"\"\"\n</code></pre>"},{"location":"guides/slack/#chat-specific-formatting-rules","title":"Chat-Specific Formatting Rules","text":"<pre><code>SLACK_CHAT_FORMATTING_RULES = \"\"\"\n- Same as above, plus:\n- Reserve backticks for citations and technical terms only\n\"\"\"\n</code></pre>"},{"location":"guides/slack/#valid-event-types","title":"Valid Event Types","text":"<pre><code>VALID_EVENT_TYPES = {\n    \"athena\": [\"referrals\"],\n    \"quill\": [\"binds\"],\n    \"canary\": [\n        \"evaluations\", \"openprs\", \"weekly-summary\",\n        \"release-train\", \"releases\", \"new-users\",\n        \"new-repo\", \"new-swallow-projects\"\n    ],\n    \"aimee\": [\"inquiries\"],\n    \"prometheus\": [\"dust-audit\"],\n}\n</code></pre>"},{"location":"guides/slack/#code-examples","title":"Code Examples","text":""},{"location":"guides/slack/#basic-message-posting","title":"Basic Message Posting","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, PostMessageOptions\n\nslack = SlackService()\n\n# Post simple message\nresult = await slack.post_message(\n    channel=\"C1234567890\",\n    options=PostMessageOptions(\n        text=\"Hello from Athena!\",\n    ),\n    agent_id=\"athena\",\n)\n\nif result[\"success\"]:\n    print(f\"Message posted: {result['ts']}\")\nelse:\n    print(f\"Error: {result['error']}\")\n</code></pre>"},{"location":"guides/slack/#posting-with-block-kit","title":"Posting with Block Kit","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, SlackBlockBuilder\n\nslack = SlackService()\n\n# Create formatted response\noptions = SlackBlockBuilder.format_chat_response(\n    message=\"Here's your analysis...\",\n    citations=[\n        {\"url\": \"https://example.com\", \"source\": \"Example Doc\"},\n    ],\n    is_learning_worthy=True,\n)\n\nresult = await slack.post_message(\n    channel=\"C1234567890\",\n    options=options,\n    agent_id=\"athena\",\n)\n</code></pre>"},{"location":"guides/slack/#reading-thread-replies","title":"Reading Thread Replies","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, SlackScraper\n\nslack = SlackService()\n\n# Get thread replies\nresult = await slack.get_thread_replies(\n    channel=\"C1234567890\",\n    thread_ts=\"1700000000.000000\",\n    agent_id=\"athena\",\n)\n\nif result[\"success\"]:\n    messages = result[\"messages\"]\n\n    # Simplify messages for processing\n    simplified = [\n        SlackScraper.simplify_message_extended(msg)\n        for msg in messages\n    ]\n\n    # Extract thread metadata\n    metadata = SlackScraper.extract_thread_metadata(messages)\n    print(f\"Thread created: {metadata['thread_created_at']}\")\n    print(f\"Human participants: {metadata['human_participants']}\")\n</code></pre>"},{"location":"guides/slack/#multi-channel-broadcasting_1","title":"Multi-Channel Broadcasting","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, PostMessageOptions\n\nslack = SlackService()\n\nresult = await slack.post_to_subscribed_channels(\n    agent_id=\"canary\",\n    event_type=\"releases\",\n    event_data={\"repo\": \"swallow\"},\n    message_options=PostMessageOptions(\n        text=\"New release deployed!\",\n    ),\n)\n\nprint(f\"Posted to {len(result.threads)} channels\")\nprint(f\"Errors: {len(result.errors)}\")\n</code></pre>"},{"location":"guides/slack/#downloading-files","title":"Downloading Files","text":"<pre><code>from eval_workbench.shared.slack.service import SlackService, SlackFile\n\nslack = SlackService()\n\nfile = SlackFile(\n    id=\"F1234567890\",\n    name=\"document.pdf\",\n    mimetype=\"application/pdf\",\n    size=1024000,\n    url_private=\"https://files.slack.com/...\",\n)\n\nresponse = await slack.download_file(file, agent_id=\"athena\")\n\nif response.success:\n    with open(response.filename, \"wb\") as f:\n        f.write(response.buffer)\nelse:\n    print(f\"Download failed: {response.error}\")\n</code></pre>"},{"location":"guides/slack/#exports","title":"Exports","text":"<pre><code>from eval_workbench.shared.slack.service import (\n    # Configuration\n    SlackConfig,\n    SLACK_FORMATTING_RULES,\n    SLACK_CHAT_FORMATTING_RULES,\n    VALID_EVENT_TYPES,\n    AGENT_INFO,\n\n    # TypedDicts\n    SlackResponse,\n    SimplifiedMessage,\n    ExtendedSimplifiedMessage,\n    PostMessageOptions,\n\n    # Dataclasses\n    SlackFile,\n    FileDownloadResponse,\n    SlackSubscription,\n    SlackThread,\n    MultiChannelPostResult,\n\n    # HTTP Client\n    SlackHttpClient,\n    get_shared_slack_client,\n\n    # Storage\n    SubscriptionStorage,\n    InMemorySubscriptionStorage,\n\n    # Main Classes\n    SlackScraper,\n    SlackService,\n    SlackBlockBuilder,\n)\n\nfrom eval_workbench.shared.metrics.slack_compliance import (\n    SlackFormattingCompliance,\n)\n</code></pre>"},{"location":"metric-registry/","title":"Metric Registry","text":"Evaluation metrics organized by scope and implementation Shared + Athena Axion-style registry <p>Metrics are grouped by Shared (cross-implementation) and Athena (implementation-specific).</p> Shared Metrics <p>Cross-implementation metrics</p> <code>conversation</code> <code>actual_output</code> Athena Metrics <p>Implementation-specific underwriting evaluation</p> <code>actual_output</code> <code>expected_output</code>"},{"location":"metric-registry/athena/","title":"Athena Metrics","text":"Athena underwriting metrics and analysis 9 Metrics Implementation-specific <p>Athena metrics cover recommendation quality, underwriting completeness, and citation accuracy.</p> Recommendation Metrics <p>Citations, decision quality, referral reasons, underwriting checks</p> <code>actual_output</code> <code>expected_output</code>"},{"location":"metric-registry/athena/recommendation/","title":"Recommendation Metrics","text":"<p>Metrics used for Athena recommendation evaluation.</p>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/","title":"Citation Accuracy","text":"Validate numeric citations against reference data Rule-Based Verification Athena"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Validity ratio \u26a1 Default Threshold <code>1.0</code> All citations must be valid \ud83d\udccb Required Inputs <code>actual_reference</code> Optional: actual_output, additional_output, additional_input <p>What It Measures</p> <p>Citation Accuracy validates numeric citations like <code>[1]</code>, <code>[2]</code> in AI-generated text against the <code>actual_reference</code> data. It ensures every citation points to a real reference entry and optionally verifies that referenced fields exist in the input data.</p> Score Interpretation 1.0  All citations reference valid entries 0.8+  Most citations valid, minor issues 0.5  Half the citations are invalid &lt; 0.5  Significant citation errors \u2705 Use When <ul> <li>AI output contains numbered citations</li> <li>You have reference data to validate against</li> <li>Traceability of claims is important</li> <li>Verifying underwriting recommendations</li> </ul> \u274c Don't Use When <ul> <li>Citations use bracket-path format (use Citation Fidelity)</li> <li>No reference data available</li> <li>Output doesn't contain citations</li> <li>Free-form text without structured references</li> </ul> <p>See Also: Citation Fidelity</p> <p>Citation Accuracy validates numeric citations like <code>[1]</code> against reference lists. Citation Fidelity validates bracket-path citations like <code>[quote.field]</code> against JSON data.</p> <p>Use Accuracy for numbered references; use Fidelity for JSON path citations.</p> How It Works  Computation Scoring System <p>The metric extracts numeric citations from the output text and validates each against the reference data.</p> <p> \u2705 VALID 1 Citation matches a reference entry and field exists (if checked). </p> <p> \u274c INVALID 0 Citation doesn't match any reference or field doesn't exist. </p> <p>Score Formula</p> <pre><code>score = valid_citations / scorable_citations\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Output Text]\n        B[Reference Data]\n        C[Input Data - Optional]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Citation Extraction\"]\n        D[\"Find [1], [2], etc.\"]\n        E[\"Citation List\"]\n    end\n\n    subgraph VALIDATE[\"\u2696\ufe0f Step 2: Validation\"]\n        F[Match to Reference Entry]\n        G[Optionally Check Input Fields]\n        H[\"Valid / Invalid\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        I[\"Count Valid Citations\"]\n        J[\"Calculate Ratio\"]\n        K[\"Final Score\"]\n    end\n\n    A --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    C --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style VALIDATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style K fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>validation_mode</code> <code>str</code> <code>ref_only</code> <code>ref_only</code> or <code>ref_plus_input</code> <code>output_key</code> <code>str</code> <code>None</code> Key in <code>additional_output</code> to analyze (fallback to <code>actual_output</code>) <p>Validation Modes</p> <ul> <li>ref_only: Only check that citations match reference entries</li> <li>ref_plus_input: Also verify referenced fields exist in <code>additional_input</code></li> </ul>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#code-examples","title":"Code Examples","text":"Basic Usage With Input Validation <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_accuracy import CitationAccuracy\n\nmetric = CitationAccuracy(validation_mode=\"ref_only\")\n\nitem = DatasetItem(\n    actual_output=\"The policy was approved based on the roof age [1] and revenue [2].\",\n    actual_reference=[\"[1] - quote.roof_age\", \"[2] - quote.revenue\"],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (2 of 2 citations valid)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_accuracy import CitationAccuracy\n\nmetric = CitationAccuracy(validation_mode=\"ref_plus_input\")\n\nitem = DatasetItem(\n    actual_output=\"Premium is $1,200 [1].\",\n    actual_reference=[\"[1] - quote.premium\"],\n    additional_input={\"quote\": {\"premium\": 1200}},\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.score}\")\nprint(f\"Valid: {result.signals.valid_citations}/{result.signals.total_citations}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationAccuracyResult Structure <pre><code>CitationAccuracyResult(\n{\n    \"score\": 1.0,\n    \"total_citations\": 2,\n    \"scorable_citations\": 2,\n    \"valid_citations\": 2,\n    \"verdicts\": [\n        {\n            \"citation\": \"[1]\",\n            \"reference_match\": \"[1] - quote.roof_age\",\n            \"status\": \"valid\",\n            \"reason\": \"Reference entry found\"\n        },\n        {\n            \"citation\": \"[2]\",\n            \"reference_match\": \"[2] - quote.revenue\",\n            \"status\": \"valid\",\n            \"reason\": \"Reference entry found\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Overall accuracy score <code>total_citations</code> <code>int</code> Total citations found in output <code>scorable_citations</code> <code>int</code> Citations that could be validated <code>valid_citations</code> <code>int</code> Citations that passed validation <code>verdicts</code> <code>List</code> Per-citation validation details"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: All Valid (Score: 1.0) <p>All Citations Match References</p> <p>Output:</p> <p>\"The property qualifies for approval based on the building age [1] and claims history [2].\"</p> <p>Reference Data: <pre><code>[\"[1] - property.building_age\", \"[2] - property.claims_count\"]\n</code></pre></p> <p>Analysis:</p> Citation Reference Match Status <code>[1]</code> <code>[1] - property.building_age</code> \u2705 Valid <code>[2]</code> <code>[2] - property.claims_count</code> \u2705 Valid <p>Final Score: <code>2 / 2 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Missing Reference (Score: 0.5) <p>Some Citations Invalid</p> <p>Output:</p> <p>\"Coverage approved per [1]. Additional review needed per [3].\"</p> <p>Reference Data: <pre><code>[\"[1] - quote.coverage\", \"[2] - quote.premium\"]\n</code></pre></p> <p>Analysis:</p> Citation Reference Match Status <code>[1]</code> <code>[1] - quote.coverage</code> \u2705 Valid <code>[3]</code> Not found \u274c Invalid <p>Final Score: <code>1 / 2 = 0.5</code> </p>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd17 Traceability <p>Ensures every claim in AI output can be traced back to source data.</p> \u2713 Compliance <p>Critical for regulatory requirements where decisions must be documented.</p> \ud83d\udee1\ufe0f Trust <p>Builds confidence that AI recommendations are grounded in real data.</p>"},{"location":"metric-registry/athena/recommendation/citation_accuracy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Accuracy = Do numeric citations <code>[1]</code>, <code>[2]</code> point to valid reference entries?</p> <ul> <li>Use it when: AI output has numbered citations and you have reference data</li> <li>Score interpretation: Higher = more citations are valid</li> <li>Key difference: Validates reference existence, not content accuracy</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Citation Fidelity \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/","title":"Citation Fidelity","text":"Verify bracket-path citations resolve to valid JSON values Rule-Based Verification Athena"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Validity ratio \u26a1 Default Threshold <code>1.0</code> All paths must resolve \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> JSON data required <p>What It Measures</p> <p>Citation Fidelity validates bracket-path citations like <code>[quote.premium]</code> or <code>[property.address]</code> against JSON data in <code>expected_output</code>. It ensures every cited path resolves to a real value and optionally verifies the cited value appears in the surrounding text.</p> Score Interpretation 1.0  All citation paths resolve correctly 0.8+  Most paths valid, minor issues 0.5  Half the citations are invalid &lt; 0.5  Many paths don't resolve \u2705 Use When <ul> <li>Citations use JSON path format</li> <li>You need to verify data accuracy</li> <li>Checking that cited values match the text</li> <li>Validating structured data references</li> </ul> \u274c Don't Use When <ul> <li>Citations use numeric format (use Citation Accuracy)</li> <li>No JSON data available</li> <li>Output doesn't contain bracket citations</li> <li>Free-form references without paths</li> </ul> <p>See Also: Citation Accuracy</p> <p>Citation Fidelity validates path citations like <code>[quote.field]</code> against JSON. Citation Accuracy validates numeric citations like <code>[1]</code> against reference lists.</p> <p>Use Fidelity for JSON paths; use Accuracy for numbered references.</p> How It Works  Computation Scoring System <p>The metric parses bracket-path citations and resolves each against the JSON structure.</p> <p> \u2705 VALID 1 Path resolves to a value; value appears in text (if check enabled). </p> <p> \u274c INVALID 0 Path doesn't exist or value not found in text. </p> <p>Score Formula</p> <pre><code>score = valid_citations / total_citations\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Output Text]\n        B[Expected Output JSON]\n    end\n\n    subgraph PARSE[\"\ud83d\udd0d Step 1: Parse Citations\"]\n        C[\"Find [path.to.field]\"]\n        D[\"Citation Path List\"]\n    end\n\n    subgraph RESOLVE[\"\u2696\ufe0f Step 2: Path Resolution\"]\n        E[Navigate JSON Structure]\n        F[Optionally Check Value in Text]\n        G[\"Resolved / Not Found\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Valid Paths\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; E\n    B --&gt; E\n    E --&gt; F\n    A --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style PARSE stroke:#3b82f6,stroke-width:2px\n    style RESOLVE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>check_values</code> <code>bool</code> <code>False</code> Verify cited values appear in text <code>window_chars</code> <code>int</code> <code>200</code> Characters to search for value <code>min_shared_tokens</code> <code>int</code> <code>2</code> Min tokens to match for text values <code>fuzzy_threshold</code> <code>float</code> <code>0.8</code> Fuzzy match threshold for strings <code>numeric_tolerance</code> <code>float</code> <code>0.01</code> Tolerance for numeric comparisons <p>Value Checking</p> <p>When <code>check_values=True</code>, the metric verifies that the JSON value actually appears in the text near the citation, preventing citations that point to valid paths but misrepresent the data.</p>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#code-examples","title":"Code Examples","text":"Basic Usage With Value Checking <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_fidelity import CitationFidelity\n\nmetric = CitationFidelity()\n\nitem = DatasetItem(\n    actual_output=\"The premium is $1,200 [quote.premium].\",\n    expected_output={\"quote\": {\"premium\": 1200}},\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (path resolves correctly)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.citation_fidelity import CitationFidelity\n\nmetric = CitationFidelity(check_values=True, window_chars=100)\n\nitem = DatasetItem(\n    actual_output=\"Premium is $1,200 [quote.premium]. Coverage limit $500,000 [quote.coverage].\",\n    expected_output={\n        \"quote\": {\n            \"premium\": 1200,\n            \"coverage\": 500000\n        }\n    },\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.score}\")\nprint(f\"Valid: {result.signals.valid_citations}/{result.signals.total_citations}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationFidelityResult Structure <pre><code>CitationFidelityResult(\n{\n    \"score\": 1.0,\n    \"total_citations\": 2,\n    \"valid_citations\": 2,\n    \"verdicts\": [\n        {\n            \"citation\": \"[quote.premium]\",\n            \"path\": \"quote.premium\",\n            \"resolved_value\": 1200,\n            \"status\": \"valid\",\n            \"reason\": \"Path resolved successfully\"\n        },\n        {\n            \"citation\": \"[quote.coverage]\",\n            \"path\": \"quote.coverage\",\n            \"resolved_value\": 500000,\n            \"status\": \"valid\",\n            \"reason\": \"Path resolved successfully\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Overall fidelity score <code>total_citations</code> <code>int</code> Total path citations found <code>valid_citations</code> <code>int</code> Citations that resolved <code>verdicts</code> <code>List</code> Per-citation resolution details"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: All Paths Resolve (Score: 1.0) <p>Valid JSON Path Citations</p> <p>Output:</p> <p>\"Building age is 15 years [property.building_age]. Revenue: $2.5M [financials.revenue].\"</p> <p>Expected Output (JSON): <pre><code>{\n    \"property\": {\"building_age\": 15},\n    \"financials\": {\"revenue\": 2500000}\n}\n</code></pre></p> <p>Analysis:</p> Citation Path Resolved Value Status <code>[property.building_age]</code> <code>property.building_age</code> <code>15</code> \u2705 Valid <code>[financials.revenue]</code> <code>financials.revenue</code> <code>2500000</code> \u2705 Valid <p>Final Score: <code>2 / 2 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Invalid Path (Score: 0.5) <p>Some Paths Don't Resolve</p> <p>Output:</p> <p>\"Premium is $1,200 [quote.premium]. Deductible: $500 [quote.deductible].\"</p> <p>Expected Output (JSON): <pre><code>{\n    \"quote\": {\"premium\": 1200}\n}\n</code></pre></p> <p>Analysis:</p> Citation Path Resolved Value Status <code>[quote.premium]</code> <code>quote.premium</code> <code>1200</code> \u2705 Valid <code>[quote.deductible]</code> <code>quote.deductible</code> Not found \u274c Invalid <p>Final Score: <code>1 / 2 = 0.5</code> </p>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Data Accuracy <p>Ensures AI-cited values actually exist in the source data structure.</p> \ud83d\udd0d Value Verification <p>Optional check that cited values match what's stated in the text.</p> \ud83d\udcca Structured Tracing <p>Enables precise traceability through JSON path references.</p>"},{"location":"metric-registry/athena/recommendation/citation_fidelity/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Fidelity = Do bracket-path citations like <code>[quote.field]</code> resolve to valid JSON values?</p> <ul> <li>Use it when: AI output uses JSON path citations and you have structured data</li> <li>Score interpretation: Higher = more paths resolve correctly</li> <li>Key difference: Validates path resolution; optionally checks value accuracy</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Citation Accuracy \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/decision_quality/","title":"Decision Quality","text":"Evaluate AI decision accuracy and reasoning alignment LLM-Powered Quality Athena"},{"location":"metric-registry/athena/recommendation/decision_quality/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Weighted quality score \u26a1 Default Threshold <code>\u2014</code> Uses score_range only \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Human decision + AI recommendation <p>What It Measures</p> <p>Decision Quality evaluates whether the AI made the correct underwriting decision (approve/decline/refer) and whether its reasoning aligns with the human underwriter's notes. It combines decision match scoring with reasoning coverage analysis.</p> Score Interpretation 1.0  Correct decision + complete reasoning 0.7+  Correct decision, reasoning mostly aligned 0.5  Decision match but reasoning gaps 0.0  Wrong decision (hard fail enabled) \u2705 Use When <ul> <li>Comparing AI to human decisions</li> <li>Both decision and reasoning matter</li> <li>Evaluating underwriting recommendations</li> <li>Training data has human annotations</li> </ul> \u274c Don't Use When <ul> <li>No ground truth decision available</li> <li>Only measuring completeness</li> <li>Decisions are always exploratory</li> <li>No reasoning comparison needed</li> </ul> How It Works  Computation Scoring System <p>The metric extracts decisions from both human and AI outputs, scores the match, and analyzes reasoning coverage.</p> <p> \u2705 DECISION MATCH 1.0 AI decision matches human decision exactly. </p> <p> \u274c DECISION MISMATCH 0.0 AI decision differs from human decision. </p> <p>Score Formula</p> <pre><code>overall_score = (outcome_weight \u00d7 outcome_score) + (reasoning_weight \u00d7 reasoning_score)\n\n# If hard_fail_on_outcome_mismatch=True and decisions differ:\noverall_score = 0.0\n</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Recommendation]\n        B[Human Decision/Notes]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Decision Extraction\"]\n        C[Detect Human Decision]\n        D[Detect AI Decision]\n        E[\"Approve / Decline / Refer\"]\n    end\n\n    subgraph MATCH[\"\u2696\ufe0f Step 2: Decision Match\"]\n        F[Compare Decisions]\n        G[\"outcome_score\"]\n    end\n\n    subgraph REASON[\"\ud83d\udcdd Step 3: Reasoning Coverage\"]\n        H[Extract Risk Factors]\n        I[Check Coverage in AI Output]\n        J[\"reasoning_score\"]\n    end\n\n    subgraph COMBINE[\"\ud83d\udcca Step 4: Final Score\"]\n        K[Apply Weights]\n        L[Hard Fail Check]\n        M[\"overall_score\"]\n    end\n\n    A --&gt; D\n    B --&gt; C\n    C &amp; D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    B --&gt; H\n    A --&gt; I\n    H --&gt; I\n    I --&gt; J\n    G &amp; J --&gt; K\n    K --&gt; L\n    L --&gt; M\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style MATCH stroke:#f59e0b,stroke-width:2px\n    style REASON stroke:#8b5cf6,stroke-width:2px\n    style COMBINE stroke:#10b981,stroke-width:2px\n    style M fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>outcome_weight</code> <code>float</code> <code>0.6</code> Weight for decision match component <code>reasoning_weight</code> <code>float</code> <code>0.4</code> Weight for reasoning coverage <code>hard_fail_on_outcome_mismatch</code> <code>bool</code> <code>True</code> Force score to 0.0 if decisions differ <code>recommendation_column_name</code> <code>str</code> <code>None</code> Additional output field to analyze <p>Hard Fail Mode</p> <p>When <code>hard_fail_on_outcome_mismatch=True</code> (default), any decision mismatch results in a score of 0.0, regardless of reasoning quality. Disable this for softer evaluation.</p>"},{"location":"metric-registry/athena/recommendation/decision_quality/#code-examples","title":"Code Examples","text":"Basic Usage Custom Weights <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.decision_quality import DecisionQuality\n\nmetric = DecisionQuality()\n\nitem = DatasetItem(\n    actual_output=\"Recommend Decline due to building age exceeding 30 years.\",\n    expected_output=\"Decline - roof age and building condition are concerns.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Overall: 0.85 (decision match + partial reasoning coverage)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.decision_quality import DecisionQuality\n\n# Prioritize reasoning over outcome\nmetric = DecisionQuality(\n    outcome_weight=0.4,\n    reasoning_weight=0.6,\n    hard_fail_on_outcome_mismatch=False,\n)\n\nitem = DatasetItem(\n    actual_output=\"Approve with conditions for roof repair.\",\n    expected_output=\"Approve - good risk profile, minor roof concerns noted.\",\n)\n\nresult = await metric.execute(item)\nprint(f\"Overall: {result.signals.overall_score}\")\nprint(f\"Outcome Match: {result.signals.outcome_match}\")\nprint(f\"Reasoning: {result.signals.reasoning_score}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca DecisionQualityResult Structure <pre><code>DecisionQualityResult(\n{\n    \"overall_score\": 0.85,\n    \"outcome_match\": true,\n    \"outcome_score\": 1.0,\n    \"human_decision_detected\": \"decline\",\n    \"ai_decision_detected\": \"decline\",\n    \"reasoning_score\": 0.625,\n    \"matched_concepts\": [\"building age\", \"condition\"],\n    \"missing_concepts\": [\"roof age\"]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/decision_quality/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> Combined weighted score <code>outcome_match</code> <code>bool</code> Whether decisions matched <code>outcome_score</code> <code>float</code> Decision match score (0 or 1) <code>human_decision_detected</code> <code>str</code> Extracted human decision <code>ai_decision_detected</code> <code>str</code> Extracted AI decision <code>reasoning_score</code> <code>float</code> Reasoning coverage score <code>matched_concepts</code> <code>List[str]</code> Risk factors mentioned by AI <code>missing_concepts</code> <code>List[str]</code> Risk factors AI missed"},{"location":"metric-registry/athena/recommendation/decision_quality/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>Decision + Reasoning Aligned</p> <p>Human Decision:</p> <p>\"Decline - prior claims history and high BPP value are concerns.\"</p> <p>AI Recommendation:</p> <p>\"Recommend Decline. The applicant has prior claims on record and the BPP coverage requested exceeds typical thresholds.\"</p> <p>Analysis:</p> Component Score Details Decision Match 1.0 Both: Decline Reasoning Coverage 1.0 All factors mentioned <p>Final Score: <code>(0.6 \u00d7 1.0) + (0.4 \u00d7 1.0) = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Decision Match, Incomplete Reasoning (Score: 0.7) <p>Correct Decision, Missing Factors</p> <p>Human Decision:</p> <p>\"Approve - good claims history, reasonable BPP, building in good condition.\"</p> <p>AI Recommendation:</p> <p>\"Recommend Approve based on clean claims history.\"</p> <p>Analysis:</p> Component Score Details Decision Match 1.0 Both: Approve Reasoning Coverage 0.33 Only 1 of 3 factors mentioned <p>Final Score: <code>(0.6 \u00d7 1.0) + (0.4 \u00d7 0.33) = 0.73</code> </p> \u274c Scenario 3: Decision Mismatch (Score: 0.0) <p>Wrong Decision (Hard Fail)</p> <p>Human Decision:</p> <p>\"Decline - too many risk factors.\"</p> <p>AI Recommendation:</p> <p>\"Recommend Approve based on revenue metrics.\"</p> <p>Analysis:</p> Component Score Details Decision Match 0.0 Human: Decline, AI: Approve Hard Fail Triggered Score forced to 0.0 <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/decision_quality/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Decision Accuracy <p>Measures whether AI reaches the same conclusions as human experts.</p> \ud83d\udcdd Reasoning Quality <p>Ensures AI considers the same risk factors as human underwriters.</p> \ud83d\udd04 Calibration <p>Helps identify where AI and human judgment diverge for retraining.</p>"},{"location":"metric-registry/athena/recommendation/decision_quality/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Decision Quality = Does AI make the right decision for the right reasons?</p> <ul> <li>Use it when: You have ground truth decisions and want to evaluate both outcome and reasoning</li> <li>Score interpretation: Higher = better decision + reasoning alignment</li> <li>Key feature: Configurable hard-fail on decision mismatch</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Underwriting Completeness \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/refer_reason/","title":"Refer Reason","text":"Extract and categorize reasons for referral/decline outcomes LLM-Powered Analysis Athena"},{"location":"metric-registry/athena/recommendation/refer_reason/#at-a-glance","title":"At a Glance","text":"\ud83d\udcca Score Range <code>\u2014</code> Analysis metric (no score) \u26a1 Default Threshold <code>\u2014</code> Not applicable \ud83d\udccb Required Inputs <code>actual_output</code> AI recommendation text <p>What It Measures</p> <p>Refer Reason is an analysis metric that extracts and categorizes the reasons behind referral or decline outcomes. It identifies the primary reason category, all contributing reasons, and classifies the actionable type (market, system, or policy).</p> Output Description <code>primary_category</code> Main reason category (e.g., Claims History, BPP Value) <code>all_reasons</code> Complete list of extracted reasons <code>actionable_type</code> Classification: market, system, or policy \u2705 Use When <ul> <li>Understanding referral patterns</li> <li>Building analytics dashboards</li> <li>Categorizing decline reasons</li> <li>Training data analysis</li> </ul> \u274c Don't Use When <ul> <li>Evaluating quality (use Decision Quality)</li> <li>Checking rule compliance (use Underwriting Rules)</li> <li>Approval outcomes</li> <li>Scoring is required</li> </ul> How It Works  Computation Reason Categories <p>The metric detects negative outcomes, then uses LLM extraction to identify and categorize all reasons.</p> <pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[AI Recommendation]\n    end\n\n    subgraph DETECT[\"\ud83d\udd0d Step 1: Outcome Detection\"]\n        B[Check for Referral/Decline]\n        C{Negative Outcome?}\n    end\n\n    subgraph EXTRACT[\"\ud83d\udcdd Step 2: Reason Extraction\"]\n        D[LLM Analysis]\n        E[\"Extract All Reasons\"]\n    end\n\n    subgraph CATEGORIZE[\"\ud83c\udff7\ufe0f Step 3: Categorization\"]\n        F[Assign Categories]\n        G[Determine Primary]\n        H[Classify Actionable Type]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Output\"]\n        I[\"Structured Analysis\"]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt;|Yes| D\n    C --&gt;|No| I\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style DETECT stroke:#3b82f6,stroke-width:2px\n    style EXTRACT stroke:#f59e0b,stroke-width:2px\n    style CATEGORIZE stroke:#8b5cf6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre> <p> Claims History Prior claims or loss history issues </p> <p> BPP Value Business personal property limits </p> <p> New Business Organization age concerns </p> <p> Building Coverage Non-owned building or coverage issues </p> <p> Employee Count High employee count concerns </p> <p> Other Classification, location, or other factors </p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>recommendation_column_name</code> <code>str</code> <code>brief_recommendation</code> Field in additional_output to analyze <code>max_source_lines</code> <code>int</code> <code>50</code> Max source lines for context"},{"location":"metric-registry/athena/recommendation/refer_reason/#code-examples","title":"Code Examples","text":"Basic Usage Full Analysis <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.refer_reason import ReferReason\n\nmetric = ReferReason()\n\nitem = DatasetItem(\n    actual_output=\"Decline due to prior claims history and high BPP coverage request.\"\n)\n\nresult = await metric.execute(item)\nprint(result.explanation)\n# \"Claims History\"\n\nprint(result.signals.primary_category)\n# ReasonCategory.CLAIMS_HISTORY\n\nprint(result.signals.all_reasons)\n# [{\"category\": \"Claims History\", ...}, {\"category\": \"BPP Value\", ...}]\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.refer_reason import ReferReason\n\nmetric = ReferReason()\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    Refer to underwriting team.\n\n    Reasons:\n    - Business established in 2023 (less than 3 years)\n    - BPP limit requested: $300,000 (exceeds threshold)\n    - Home-based business requesting contents coverage\n    \"\"\"\n)\n\nresult = await metric.execute(item)\n\nprint(f\"Outcome: {result.signals.outcome_label}\")\nprint(f\"Primary Reason: {result.signals.primary_category}\")\nprint(f\"Reason Count: {result.signals.reason_count}\")\nprint(f\"Actionable Type: {result.signals.actionable_type}\")\n\nfor reason in result.signals.all_reasons:\n    print(f\"  - {reason['category']}: {reason['reasoning']}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/refer_reason/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Access detailed analysis results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.explanation)   # Primary category as explanation\nresult.signals              # Full analysis breakdown\n</code></pre> \ud83d\udcca ReferReasonResult Structure <pre><code>ReferReasonResult(\n{\n    \"is_negative_outcome\": true,\n    \"outcome_label\": \"Referral\",\n    \"primary_reason\": {\n        \"category\": \"Claims History\",\n        \"reasoning\": \"Prior claims mentioned as primary concern\",\n        \"confidence\": 0.92\n    },\n    \"primary_category\": \"Claims History\",\n    \"all_reasons\": [\n        {\n            \"category\": \"Claims History\",\n            \"reasoning\": \"Prior claims mentioned\",\n            \"confidence\": 0.92\n        },\n        {\n            \"category\": \"BPP Value\",\n            \"reasoning\": \"High BPP coverage request\",\n            \"confidence\": 0.85\n        }\n    ],\n    \"reason_count\": 2,\n    \"actionable_type\": \"policy\"\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/refer_reason/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>is_negative_outcome</code> <code>bool</code> Whether outcome is referral/decline <code>outcome_label</code> <code>str</code> Referral, Decline, Approved, or Unknown <code>primary_reason</code> <code>dict</code> Most significant reason details <code>primary_category</code> <code>str</code> Category of primary reason <code>all_reasons</code> <code>List</code> All extracted reasons <code>reason_count</code> <code>int</code> Number of reasons detected <code>actionable_type</code> <code>str</code> market, system, or policy"},{"location":"metric-registry/athena/recommendation/refer_reason/#example-scenarios","title":"Example Scenarios","text":"\ud83d\udcca Scenario 1: Single Clear Reason <p>Claims History</p> <p>Recommendation:</p> <p>\"Decline - applicant has 3 claims in the past 2 years.\"</p> <p>Analysis:</p> Field Value Outcome Decline Primary Category Claims History Reason Count 1 Actionable Type policy <p>Explanation: <code>\"Claims History\"</code></p> \ud83d\udcca Scenario 2: Multiple Reasons <p>Multiple Factors</p> <p>Recommendation:</p> <p>\"Refer - new business (2024), high BPP ($350k), and 25 employees.\"</p> <p>Analysis:</p> Field Value Outcome Referral Primary Category New Business Reason Count 3 All Reasons New Business, BPP Value, Employee Count Actionable Type system <p>Explanation: <code>\"New Business\"</code></p> \ud83d\udcca Scenario 3: Approval (No Analysis) <p>Not Applicable</p> <p>Recommendation:</p> <p>\"Approve - all criteria within guidelines.\"</p> <p>Analysis:</p> Field Value Outcome Approved Primary Category None Reason Count 0 <p>Note: Analysis only runs for negative outcomes.</p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#actionable-types","title":"Actionable Types","text":"\ud83c\udfea Market <p>External market conditions or factors outside control.</p> \u2699\ufe0f System <p>Configurable thresholds or rules that could be adjusted.</p> \ud83d\udcdc Policy <p>Fixed policy requirements or guidelines.</p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcc8 Analytics <p>Enables aggregation and trending of referral reasons.</p> \ud83d\udd0d Root Cause <p>Identifies patterns in why applications are declined.</p> \ud83c\udfaf Process Improvement <p>Helps identify where guidelines could be refined.</p>"},{"location":"metric-registry/athena/recommendation/refer_reason/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Refer Reason = Why did the AI refer or decline this application?</p> <ul> <li>Use it when: You need to categorize and analyze referral/decline reasons</li> <li>Output type: Analysis (no score)</li> <li>Key feature: Structured extraction with category classification</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Underwriting Rules \u00b7 Decision Quality</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/","title":"Underwriting Completeness","text":"Ensure recommendations contain all required components LLM-Powered Completeness Athena"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Weighted criteria score \u26a1 Default Threshold <code>\u2014</code> No default threshold \ud83d\udccb Required Inputs <code>actual_output</code> AI recommendation text <p>What It Measures</p> <p>Underwriting Completeness evaluates whether a recommendation contains all required components of a complete underwriting decision: Decision, Rationale, Evidence, and Next Steps. Missing the decision component results in an automatic score of 0.0.</p> Score Interpretation 1.0  All four components present and strong 0.7+  Decision clear, some components weaker 0.5  Missing or weak supporting components 0.0  No clear decision (hard gate) \u2705 Use When <ul> <li>Evaluating recommendation structure</li> <li>Ensuring actionable outputs</li> <li>Training agents on proper format</li> <li>Quality assurance for underwriting</li> </ul> \u274c Don't Use When <ul> <li>Checking factual accuracy</li> <li>Comparing to ground truth</li> <li>Evaluating informal responses</li> <li>Non-recommendation content</li> </ul> How It Works  Computation Criteria Components <p>The metric runs four specialized LLM judges to evaluate each component of the recommendation.</p> <p> \ud83c\udfaf Decision (Required) Clear approve/decline/refer recommendation. Hard gate: missing = score 0.0 </p> <p> \ud83d\udcdd Rationale Explanation of why the decision was made. </p> <p> \ud83d\udcca Evidence Supporting data points and facts cited. </p> <p> \u27a1\ufe0f Next Steps Clear guidance on what happens next. </p> <p>Score Formula</p> <pre><code>overall_score = \u03a3 (weight[i] \u00d7 criterion_score[i])\n\n# If Decision score = 0:\noverall_score = 0.0  (hard gate)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[AI Recommendation]\n    end\n\n    subgraph JUDGES[\"\u2696\ufe0f Four Criteria Judges\"]\n        B[\"\ud83c\udfaf Decision Judge\"]\n        C[\"\ud83d\udcdd Rationale Judge\"]\n        D[\"\ud83d\udcca Evidence Judge\"]\n        E[\"\u27a1\ufe0f Next Step Judge\"]\n    end\n\n    subgraph SCORES[\"\ud83d\udcca Per-Criterion Scores\"]\n        F[\"Decision Score\"]\n        G[\"Rationale Score\"]\n        H[\"Evidence Score\"]\n        I[\"NextStep Score\"]\n    end\n\n    subgraph COMBINE[\"\ud83d\udd04 Final Score\"]\n        J[Apply Weights]\n        K[Check Decision Gate]\n        L[\"overall_score\"]\n    end\n\n    A --&gt; B &amp; C &amp; D &amp; E\n    B --&gt; F\n    C --&gt; G\n    D --&gt; H\n    E --&gt; I\n    F &amp; G &amp; H &amp; I --&gt; J\n    J --&gt; K\n    K --&gt; L\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style JUDGES stroke:#3b82f6,stroke-width:2px\n    style SCORES stroke:#f59e0b,stroke-width:2px\n    style COMBINE stroke:#10b981,stroke-width:2px\n    style L fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>weights</code> <code>dict</code> See below Per-criterion weights <p>Default Weights:</p> Criterion Default Weight Decision 0.4 Rationale 0.25 Evidence 0.2 NextStep 0.15 <p>Hard Gate</p> <p>If the Decision criterion scores 0.0 (no clear decision found), the overall score is forced to 0.0 regardless of other criteria.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#code-examples","title":"Code Examples","text":"Basic Usage Custom Weights <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_completeness import UnderwritingCompleteness\n\nmetric = UnderwritingCompleteness()\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    **Recommendation: Approve**\n\n    The applicant presents a low-risk profile based on:\n    - Building age: 5 years (excellent)\n    - Clean claims history (0 claims)\n    - Revenue: $1.2M annually\n\n    Next steps: Proceed to bind coverage. No additional documentation required.\n    \"\"\"\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.95 (all components present and strong)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_completeness import UnderwritingCompleteness\n\n# Prioritize evidence over next steps\nmetric = UnderwritingCompleteness(\n    weights={\n        \"Decision\": 0.35,\n        \"Rationale\": 0.25,\n        \"Evidence\": 0.30,\n        \"NextStep\": 0.10,\n    }\n)\n\nitem = DatasetItem(actual_output=\"Approve. Roof age 5 years. Revenue $1.2M. Next step: bind.\")\nresult = await metric.execute(item)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca UnderwritingCompletenessResult Structure <pre><code>UnderwritingCompletenessResult(\n{\n    \"overall_score\": 0.92,\n    \"criteria\": {\n        \"Decision\": {\n            \"score\": 1.0,\n            \"reasoning\": \"Clear 'Approve' recommendation stated\",\n            \"evidence\": \"Recommendation: Approve\"\n        },\n        \"Rationale\": {\n            \"score\": 0.9,\n            \"reasoning\": \"Strong explanation with multiple factors\",\n            \"evidence\": \"low-risk profile, building age, claims history\"\n        },\n        \"Evidence\": {\n            \"score\": 0.85,\n            \"reasoning\": \"Specific data points cited\",\n            \"evidence\": \"5 years, 0 claims, $1.2M\"\n        },\n        \"NextStep\": {\n            \"score\": 0.8,\n            \"reasoning\": \"Clear action specified\",\n            \"evidence\": \"Proceed to bind coverage\"\n        }\n    }\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> Weighted combination of criteria <code>criteria</code> <code>dict</code> Per-criterion score, reasoning, evidence"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Complete Recommendation (Score: 0.95) <p>All Components Present</p> <p>Recommendation:</p> <p>\"Approve this application. The business has excellent financials with $2.1M annual revenue, no prior claims in 5 years, and the building is well-maintained (constructed 2019). Proceed to bind the policy immediately.\"</p> <p>Analysis:</p> Criterion Score Finding Decision 1.0 Clear \"Approve\" Rationale 0.95 Multiple factors explained Evidence 0.90 Specific data cited NextStep 0.85 Clear action <p>Final Score: <code>0.95</code> </p> \u26a0\ufe0f Scenario 2: Weak Components (Score: 0.65) <p>Missing Elements</p> <p>Recommendation:</p> <p>\"Approve. Good risk.\"</p> <p>Analysis:</p> Criterion Score Finding Decision 1.0 Clear \"Approve\" Rationale 0.4 Vague \"good risk\" Evidence 0.2 No specific data NextStep 0.0 No next steps <p>Final Score: <code>0.65</code> </p> \u274c Scenario 3: No Decision (Score: 0.0) <p>Hard Gate Triggered</p> <p>Recommendation:</p> <p>\"The building is 10 years old with $500k revenue. There have been 2 claims in the past 3 years.\"</p> <p>Analysis:</p> Criterion Score Finding Decision 0.0 No decision stated Hard Gate Triggered Score forced to 0.0 <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udccb Actionable Output <p>Ensures AI recommendations can be acted upon by underwriters.</p> \ud83d\udcdd Documentation <p>Complete recommendations create an audit trail for compliance.</p> \ud83c\udf93 Training Signal <p>Helps identify where AI outputs need structural improvement.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_completeness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Underwriting Completeness = Does the recommendation have all required parts?</p> <ul> <li>Use it when: Evaluating the structure of AI recommendations</li> <li>Score interpretation: Higher = more complete recommendation</li> <li>Key feature: Hard gate on missing decision</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Decision Quality \u00b7 Underwriting Faithfulness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/","title":"Underwriting Faithfulness","text":"Verify factual claims are supported by source data LLM-Powered Faithfulness Athena"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Support ratio \u26a1 Default Threshold <code>0.9</code> High bar for factual accuracy \ud83d\udccb Required Inputs <code>actual_output</code> Optional: additional_input (source data) <p>What It Measures</p> <p>Underwriting Faithfulness checks whether factual claims in the AI recommendation are supported by the source data. It extracts atomic claims from the recommendation, finds relevant evidence in the input data, and verifies each claim. Unsupported claims are flagged as potential hallucinations.</p> Score Interpretation 1.0  All claims verified against source data 0.9+  Nearly all claims supported 0.7  Some claims not verifiable &lt; 0.7  Significant hallucination risk \u2705 Use When <ul> <li>Detecting hallucinations</li> <li>Verifying factual accuracy</li> <li>Source data is available</li> <li>High-stakes recommendations</li> </ul> \u274c Don't Use When <ul> <li>No source data available</li> <li>Evaluating opinions/judgments</li> <li>Checking structural completeness</li> <li>Output is purely generative</li> </ul> How It Works  Computation Verification Modes <p>The metric extracts claims from the recommendation and verifies each against the source data.</p> <p> \ud83e\udd16 LLM Mode Uses LLM to semantically verify claims against evidence. Most accurate but slower. </p> <p> \u26a1 Heuristic Mode Uses pattern matching and fuzzy comparison. Faster but less nuanced. </p> <p> \ud83d\udd04 Heuristic-then-LLM Tries heuristic first, falls back to LLM for uncertain cases. </p> <p>Score Formula</p> <pre><code>overall_score = supported_claims / total_claims\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Recommendation]\n        B[Source Data JSON]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Claim Extraction\"]\n        C[Break into Atomic Claims]\n        D[\"Claim List\"]\n    end\n\n    subgraph EVIDENCE[\"\ud83d\udcca Step 2: Evidence Finding\"]\n        E[Search Source Data]\n        F[\"Relevant Evidence Lines\"]\n    end\n\n    subgraph VERIFY[\"\u2696\ufe0f Step 3: Verification\"]\n        G[LLM or Heuristic Check]\n        H[\"Supported / Unsupported\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcc8 Step 4: Scoring\"]\n        I[\"Count Supported Claims\"]\n        J[\"Calculate Ratio\"]\n        K[\"overall_score\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; E\n    B --&gt; E\n    E --&gt; F\n    D &amp; F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style EVIDENCE stroke:#f59e0b,stroke-width:2px\n    style VERIFY stroke:#8b5cf6,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style K fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>verification_mode</code> <code>str</code> <code>llm</code> <code>llm</code>, <code>heuristic</code>, or <code>heuristic_then_llm</code> <code>max_claims</code> <code>int</code> <code>50</code> Maximum claims to verify <code>max_concurrent</code> <code>int</code> <code>10</code> Concurrency limit for LLM verification <p>Performance Tuning</p> <p>For large recommendations, use <code>heuristic_then_llm</code> mode with appropriate <code>max_concurrent</code> to balance accuracy and speed.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#code-examples","title":"Code Examples","text":"Basic Usage LLM Verification <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_faithfulness import UnderwritingFaithfulness\n\nmetric = UnderwritingFaithfulness(verification_mode=\"heuristic\")\n\nitem = DatasetItem(\n    actual_output=\"The business has annual revenue of $1.2M and no prior claims.\",\n    additional_input={\n        \"financials\": {\"annual_revenue\": 1200000},\n        \"claims\": {\"count\": 0, \"history\": []}\n    }\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (both claims verified)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_faithfulness import UnderwritingFaithfulness\n\nmetric = UnderwritingFaithfulness(\n    verification_mode=\"llm\",\n    max_claims=30,\n    max_concurrent=5,\n)\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    Recommend approve based on:\n    - Strong financials: $2.1M revenue\n    - Clean 5-year claims history\n    - Building constructed in 2019\n    \"\"\",\n    additional_input={\n        \"financials\": {\"revenue\": 2100000},\n        \"claims\": {\"five_year_count\": 0},\n        \"property\": {\"year_built\": 2019}\n    }\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.signals.overall_score}\")\nprint(f\"Supported: {result.signals.supported_claims}/{result.signals.total_claims}\")\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca UnderwritingFaithfulnessResult Structure <pre><code>UnderwritingFaithfulnessResult(\n{\n    \"overall_score\": 0.85,\n    \"total_claims\": 4,\n    \"supported_claims\": 3,\n    \"hallucinations\": 1,\n    \"claim_details\": [\n        {\n            \"claim\": \"Revenue is $2.1M\",\n            \"status\": \"supported\",\n            \"evidence\": \"financials.revenue: 2100000\",\n            \"confidence\": 0.95\n        },\n        {\n            \"claim\": \"Building was constructed in 2019\",\n            \"status\": \"supported\",\n            \"evidence\": \"property.year_built: 2019\",\n            \"confidence\": 0.98\n        },\n        {\n            \"claim\": \"No claims in 5 years\",\n            \"status\": \"supported\",\n            \"evidence\": \"claims.five_year_count: 0\",\n            \"confidence\": 0.92\n        },\n        {\n            \"claim\": \"Premium is $1,200\",\n            \"status\": \"unsupported\",\n            \"evidence\": null,\n            \"confidence\": 0.1\n        }\n    ],\n    \"unverified_claims\": [\"Premium is $1,200\"]\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> Proportion of supported claims <code>total_claims</code> <code>int</code> Total atomic claims extracted <code>supported_claims</code> <code>int</code> Claims verified against source <code>hallucinations</code> <code>int</code> Claims not found in source <code>claim_details</code> <code>List</code> Per-claim verification details <code>unverified_claims</code> <code>List[str]</code> List of unsupported claims"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: All Claims Verified (Score: 1.0) <p>Fully Faithful Recommendation</p> <p>Recommendation:</p> <p>\"Approve. Revenue is $1.5M. Building age is 8 years. Zero claims.\"</p> <p>Source Data: <pre><code>{\n    \"financials\": {\"revenue\": 1500000},\n    \"property\": {\"building_age\": 8},\n    \"claims\": {\"count\": 0}\n}\n</code></pre></p> <p>Analysis:</p> Claim Evidence Status Revenue $1.5M <code>financials.revenue: 1500000</code> \u2705 Supported Building age 8 years <code>property.building_age: 8</code> \u2705 Supported Zero claims <code>claims.count: 0</code> \u2705 Supported <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Hallucination Detected (Score: 0.67) <p>Unsupported Claim Found</p> <p>Recommendation:</p> <p>\"Revenue is $1.5M. Building age is 8 years. Premium is $1,200.\"</p> <p>Source Data: <pre><code>{\n    \"financials\": {\"revenue\": 1500000},\n    \"property\": {\"building_age\": 8}\n}\n</code></pre></p> <p>Analysis:</p> Claim Evidence Status Revenue $1.5M <code>financials.revenue: 1500000</code> \u2705 Supported Building age 8 years <code>property.building_age: 8</code> \u2705 Supported Premium $1,200 Not found \u274c Hallucination <p>Final Score: <code>2 / 3 = 0.67</code> </p> \u274c Scenario 3: Significant Hallucinations (Score: 0.25) <p>Multiple Unsupported Claims</p> <p>Recommendation:</p> <p>\"Revenue is $5M. Building is brand new. Located in a low-risk zone. Premium is competitive.\"</p> <p>Source Data: <pre><code>{\n    \"financials\": {\"revenue\": 1000000}\n}\n</code></pre></p> <p>Analysis:</p> Claim Evidence Status Revenue $5M Contradicts source ($1M) \u274c False Building brand new Not found \u274c Hallucination Low-risk zone Not found \u274c Hallucination Competitive premium Not found \u274c Hallucination <p>Final Score: <code>0 / 4 = 0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Hallucination Detection <p>Catches AI claims that aren't grounded in actual data.</p> \u2713 Trust &amp; Compliance <p>Critical for regulated industries where false claims have consequences.</p> \ud83d\udee1\ufe0f Risk Mitigation <p>Prevents decisions based on fabricated information.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_faithfulness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Underwriting Faithfulness = Are the AI's factual claims actually in the source data?</p> <ul> <li>Use it when: You have source data and need to verify factual accuracy</li> <li>Score interpretation: Higher = more claims verified, fewer hallucinations</li> <li>Key feature: Detects fabricated facts not in source data</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Citation Accuracy \u00b7 Citation Fidelity \u00b7 Underwriting Completeness</p> </li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/","title":"Underwriting Rules","text":"Track referral triggers and validate outcome consistency Hybrid Rules Athena"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Consistency score \u26a1 Default Threshold <code>1.0</code> Outcome must match triggers \ud83d\udccb Required Inputs <code>actual_output</code> <code>additional_input</code> Recommendation + source data <p>What It Measures</p> <p>Underwriting Rules tracks referral triggers in both the source data and AI output, then validates whether the AI's outcome (approve/refer/decline) is consistent with those triggers. If a referral trigger is present, the AI should refer; if no triggers, it should approve.</p> Score Interpretation 1.0  Outcome matches trigger presence 0.0  Mismatch between triggers and outcome \u2705 Use When <ul> <li>Enforcing underwriting guidelines</li> <li>Tracking referral reasons</li> <li>Auditing decision consistency</li> <li>Validating rule compliance</li> </ul> \u274c Don't Use When <ul> <li>No structured input data</li> <li>Rules don't apply</li> <li>Evaluating reasoning quality</li> <li>Non-underwriting decisions</li> </ul> How It Works  Detection Pipeline Scoring Logic <p>The metric uses a three-stage pipeline: structured checks, regex scanning, and LLM fallback.</p> <pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[AI Recommendation]\n        B[Source Data JSON]\n    end\n\n    subgraph OUTCOME[\"\ud83c\udfaf Step 1: Outcome Detection\"]\n        C[Detect AI Decision]\n        D[\"Approve / Refer / Decline\"]\n    end\n\n    subgraph RULES[\"\u2696\ufe0f Step 2: Trigger Detection\"]\n        E[Structured Checks]\n        F[Regex Scan]\n        G[LLM Fallback]\n        H[\"Detected Triggers\"]\n    end\n\n    subgraph VALIDATE[\"\u2713 Step 3: Consistency Check\"]\n        I{Referral + Triggers?}\n        J{Approval + No Triggers?}\n        K[\"Score: 1.0 or 0.0\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    B --&gt; E\n    A --&gt; F\n    E &amp; F --&gt; H\n    D --&gt; I\n    H --&gt; I\n    I --&gt;|Match| K\n    I --&gt;|No Triggers| G\n    G --&gt; H\n    J --&gt; K\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style OUTCOME stroke:#3b82f6,stroke-width:2px\n    style RULES stroke:#f59e0b,stroke-width:2px\n    style VALIDATE stroke:#10b981,stroke-width:2px\n    style K fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre> <p> \u2705 Score = 1.0 Referral/decline WITH triggers detected, OR approval WITH no triggers. </p> <p> \u274c Score = 0.0 Referral/decline WITHOUT triggers, OR approval WITH triggers present. </p> <p>Score Formula</p> <pre><code>score = 1.0 if (is_referral == bool(detected_triggers)) else 0.0\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#referral-triggers","title":"Referral Triggers","text":"<p>The metric detects the following referral triggers through structured data checks and regex patterns:</p>  Structured Rules Regex Rules Trigger Condition Severity bppValue BPP limit &gt; $250,000 Hard bppToSalesRatio BPP / sales &lt; 10% Soft numberOfEmployees Employees &gt; 20 Soft orgEstYear Business &lt; 3 years + building coverage Hard nonOwnedBuildingCoverage Building coverage requested but not owned Soft homeBasedBPP Home-based + contents-only Soft claimsHistory Prior claims count &gt; 0 Hard <p>Hard Severity:</p> <ul> <li><code>convStoreTemp</code> - Convenience/liquor/package store indicators (often tobacco/alcohol/lottery; sometimes 24/7 or fuel)</li> <li><code>claimsHistory</code> - Prior claims mentions</li> <li><code>orgEstYear</code> - New business + building coverage indicators</li> <li><code>bppValue</code> - Excessive BPP mentions</li> </ul> <p>Soft Severity:</p> <ul> <li><code>bppToSalesRatio</code> - Low ratio indicators</li> <li><code>nonOwnedBuildingCoverage</code> - Tenant building coverage / lease (including NNN/triple-net language)</li> <li><code>businessNOC</code> - Not Otherwise Classified</li> <li><code>homeBasedBPP</code> - Home-based business indicators</li> <li><code>numberOfEmployees</code> - High employee count (eligibility review)</li> </ul>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>recommendation_column_name</code> <code>str</code> <code>brief_recommendation</code> Field in additional_output to analyze <p>LLM Fallback</p> <p>When a referral/decline is detected but no triggers are found, the metric uses an LLM classifier to infer the closest trigger category.\\n+\\n+        The LLM classifier prompt is generated from the same <code>TRIGGER_SPECS</code> catalog used by regex detection so trigger descriptions stay in sync.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#code-examples","title":"Code Examples","text":"Basic Usage Multiple Triggers <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_rules import UnderwritingRules\n\nmetric = UnderwritingRules()\n\nitem = DatasetItem(\n    actual_output=\"Recommend Refer due to high BPP coverage request.\",\n    additional_input={\n        \"context_data\": {\n            \"auxData\": {\n                \"rateData\": {\n                    \"output\": {\n                        \"input\": {\n                            \"bop_bpp_limit\": 300000  # &gt; $250k threshold\n                        }\n                    }\n                }\n            }\n        }\n    }\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (referral with trigger present)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.implementations.athena.metrics.recommendation.underwriting_rules import UnderwritingRules\n\nmetric = UnderwritingRules()\n\nitem = DatasetItem(\n    actual_output=\"Decline - prior claims and new business.\",\n    additional_input={\n        \"bop_number_of_claims\": 2,\n        \"bop_business_year_established\": 2024,  # &lt; 3 years\n        \"bop_insure_building\": \"building\"\n    }\n)\n\nresult = await metric.execute(item)\n# Score: 1.0 (decline with multiple triggers)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca UnderwritingRulesResult Structure <pre><code>UnderwritingRulesResult(\n{\n    \"score\": 1.0,\n    \"is_referral\": true,\n    \"outcome_label\": \"Referral\",\n    \"primary_trigger\": \"bppValue\",\n    \"detected_events\": [\n        {\n            \"trigger\": \"bppValue\",\n            \"severity\": \"hard\",\n            \"confidence\": 0.95,\n            \"detection_method\": \"structured\",\n            \"details\": \"BPP limit $300,000 exceeds $250,000 threshold\"\n        }\n    ],\n    \"structured_values\": {\n        \"bpp_limit\": 300000,\n        \"gross_sales\": 1500000\n    }\n}\n)\n</code></pre>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> 1.0 if consistent, 0.0 if not <code>is_referral</code> <code>bool</code> Whether outcome is referral/decline <code>outcome_label</code> <code>str</code> Normalized outcome label <code>primary_trigger</code> <code>str</code> Most significant trigger <code>detected_events</code> <code>List</code> All detected triggers with details <code>structured_values</code> <code>dict</code> Extracted field values"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Consistent Referral (Score: 1.0) <p>Referral Matches Triggers</p> <p>Recommendation:</p> <p>\"Refer to underwriting - BPP coverage of $300,000 exceeds threshold.\"</p> <p>Source Data: <pre><code>{\"bop_bpp_limit\": 300000}\n</code></pre></p> <p>Analysis:</p> Component Finding Outcome Referral Trigger bppValue (BPP &gt; $250k) Match \u2705 Referral with trigger <p>Final Score: <code>1.0</code> </p> \u2705 Scenario 2: Consistent Approval (Score: 1.0) <p>Approval with No Triggers</p> <p>Recommendation:</p> <p>\"Approve - all criteria within guidelines.\"</p> <p>Source Data: <pre><code>{\n    \"bop_bpp_limit\": 150000,\n    \"bop_number_of_claims\": 0,\n    \"bop_number_of_employees\": 10\n}\n</code></pre></p> <p>Analysis:</p> Component Finding Outcome Approval Triggers None detected Match \u2705 Approval with no triggers <p>Final Score: <code>1.0</code> </p> \u274c Scenario 3: Inconsistent (Score: 0.0) <p>Approval Despite Trigger</p> <p>Recommendation:</p> <p>\"Approve this application.\"</p> <p>Source Data: <pre><code>{\"bop_bpp_limit\": 400000}\n</code></pre></p> <p>Analysis:</p> Component Finding Outcome Approval Trigger bppValue (BPP &gt; $250k) Match \u274c Approval despite trigger <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#why-it-matters","title":"Why It Matters","text":"\ud83d\udccb Guideline Compliance <p>Ensures AI follows established underwriting rules and thresholds.</p> \ud83d\udd0d Audit Trail <p>Tracks exactly which triggers led to referral decisions.</p> \u26a0\ufe0f Risk Detection <p>Catches cases where AI approves despite red flags.</p>"},{"location":"metric-registry/athena/recommendation/underwriting_rules/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Underwriting Rules = Is the AI's decision consistent with detected referral triggers?</p> <ul> <li>Use it when: Validating that AI follows underwriting guidelines</li> <li>Score interpretation: 1.0 = consistent, 0.0 = inconsistent</li> <li>Key feature: Multi-stage detection (structured + regex + LLM fallback)</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Decision Quality \u00b7 Refer Reason</p> </li> </ul>"},{"location":"metric-registry/shared/","title":"Shared Metrics","text":"Cross-implementation metrics for common workflows 10 Metrics Slack KPIs <p>Shared metrics are used across implementations. Today this registry focuses on Slack conversation analytics and KPI reporting.</p> Slack Metrics <p>Interaction, engagement, escalation, and compliance signals</p> <code>conversation</code> <code>additional_input</code>"},{"location":"metric-registry/shared/slack/","title":"Slack Metrics","text":"Slack KPIs and conversation analytics 13 Metrics Multi-turn <p>This module provides metrics for analyzing Slack conversations between users and AI assistants (e.g., Athena). These metrics support KPI computation for measuring AI assistant effectiveness in underwriting workflows.</p>"},{"location":"metric-registry/shared/slack/#slack-kpi-metrics","title":"Slack KPI Metrics","text":"Slack Conversation Analyzer <p>All KPI signals in one pass</p> <code>conversation</code> <code>additional_input</code> Slack Interaction Analyzer <p>Interaction rate and MAU signals</p> <code>conversation</code> Thread Engagement Analyzer <p>Engagement depth and response signals</p> <code>conversation</code> Recommendation Analyzer <p>Recommendation extraction for KPIs</p> <code>conversation</code> Acceptance Detector <p>Accepted vs rejected recommendations</p> <code>conversation</code> Override Detector <p>Human overrides of AI recommendations</p> <code>conversation</code> Override Satisfaction Analyzer <p>Quality of override explanations</p> <code>conversation</code> Escalation Detector <p>Detect escalations to humans</p> <code>conversation</code> Frustration Detector <p>Score user frustration levels</p> <code>conversation</code> Intervention Detector <p>Detect human intervention and escalation type</p> <code>conversation</code> <code>additional_input</code> Sentiment Detector <p>Positive/neutral/frustrated/confused sentiment</p> <code>conversation</code> <code>additional_input</code> Resolution Detector <p>Outcome: approved/declined/stalemate/pending</p> <code>conversation</code> <code>additional_input</code> Slack Formatting Compliance <p>Slack mrkdwn formatting checks</p> <code>actual_output</code>"},{"location":"metric-registry/shared/slack/#overview","title":"Overview","text":"Metric Type Description KPI <code>SlackConversationAnalyzer</code> Composite All-in-one analysis combining all metrics below All KPIs <code>SlackInteractionAnalyzer</code> Heuristic Message counts and interaction detection interaction_rate, MAU <code>ThreadEngagementAnalyzer</code> Heuristic Engagement depth and quality signals engagement_rate <code>RecommendationAnalyzer</code> Heuristic AI recommendation extraction acceptance_rate, override_rate <code>EscalationDetector</code> LLM Detects escalation to human team members escalation_rate <code>FrustrationDetector</code> LLM Scores user frustration level frustration_rate <code>AcceptanceDetector</code> LLM Determines if recommendations were accepted acceptance_rate <code>OverrideDetector</code> LLM Detects when humans override AI recommendations override_rate <code>OverrideSatisfactionAnalyzer</code> LLM Scores quality of override explanations override_satisfaction <code>InterventionDetector</code> LLM Detects human intervention and escalation type intervention_rate, stp_rate <code>SentimentDetector</code> LLM Detects user sentiment (positive/neutral/frustrated/confused) sentiment distribution, frustration_rate <code>ResolutionDetector</code> LLM Detects thread outcome (approved/declined/blocked/stalemate) resolution_rate, stalemate_rate <code>SlackFormattingCompliance</code> Heuristic Validates Slack mrkdwn formatting -"},{"location":"metric-registry/shared/slack/#metrics-detail","title":"Metrics Detail","text":""},{"location":"metric-registry/shared/slack/#slackconversationanalyzer-composite","title":"SlackConversationAnalyzer (Composite)","text":"<p>File: <code>composite.py</code></p> <p>Purpose: Perform comprehensive analysis of a Slack conversation in a single pass, extracting all KPI-relevant signals efficiently.</p> <p>What it computes: - Counts AI and human messages to determine if the thread is interactive - Calculates engagement depth (number of back-and-forth exchanges) - Detects AI recommendations and extracts case metadata (ID, priority score) - Uses LLM to classify escalation, frustration, acceptance, override, satisfaction, intervention, sentiment, and resolution - Combines heuristic preprocessing with a single LLM call for efficiency - Produces a nested result structure that can be expanded into 11 separate metric rows for reporting</p> <p>Signals produced: - <code>interaction</code> - Message counts, AI/human participation - <code>engagement</code> - Interaction depth, response lengths, questions - <code>recommendation</code> - AI recommendation type, case ID, priority - <code>escalation</code> - Whether escalated, escalation type/reason - <code>frustration</code> - Frustration score (0-1), indicators, cause - <code>acceptance</code> - Whether recommendation was accepted - <code>override</code> - Whether recommendation was overridden - <code>satisfaction</code> - Quality of override explanation - <code>intervention</code> - Human intervention type, escalation class, STP - <code>sentiment</code> - Overall user sentiment and score - <code>resolution</code> - Final outcome and stale threads</p> <p>Usage: <pre><code>from eval_workbench.shared.metrics.slack.composite import SlackConversationAnalyzer\n\nanalyzer = SlackConversationAnalyzer(\n    frustration_threshold=0.6,\n    satisfaction_threshold=0.7\n)\nresult = await analyzer.execute(dataset_item)\n\n# Convert to rows for reporting\nrows = result.signals.to_rows()\n\n# Get KPI summary\nkpi = result.signals.to_kpi_summary()\n</code></pre></p>"},{"location":"metric-registry/shared/slack/#slackinteractionanalyzer","title":"SlackInteractionAnalyzer","text":"<p>File: <code>interaction.py</code></p> <p>Purpose: Determine whether a Slack thread represents a meaningful AI interaction and track unique users for MAU calculation.</p> <p>What it computes: - Counts the number of AI messages and human messages in a thread - Determines if the thread is \"interactive\" (has both AI and human participation) - Identifies who initiated the conversation (AI or human) - Extracts thread metadata (thread_id, channel_id, sender) for aggregation - Does NOT produce a numeric score - outputs structured signals for KPI computation</p> <p>Signals:</p> Signal Type Description <code>is_interactive</code> bool Has both AI and human participation <code>ai_message_count</code> int Number of AI messages <code>human_message_count</code> int Number of human messages <code>total_turn_count</code> int Total messages in thread <code>is_ai_initiated</code> bool Whether AI sent first message <code>has_human_response</code> bool Whether humans responded to AI <code>thread_id</code> str Slack thread timestamp <code>channel_id</code> str Slack channel ID <code>sender</code> str Original sender (for MAU tracking) <p>KPIs supported:</p> <ul> <li><code>interaction_rate</code> = Interactive threads / Total eligible cases</li> <li><code>MAU</code> = Unique senders in 30 days</li> </ul>"},{"location":"metric-registry/shared/slack/#threadengagementanalyzer","title":"ThreadEngagementAnalyzer","text":"<p>File: <code>engagement.py</code></p> <p>Purpose: Measure how deeply users engage with the AI assistant beyond simple one-off interactions.</p> <p>What it computes: - Calculates interaction depth by counting back-and-forth exchanges (human\u2192AI\u2192human = 1 exchange) - Determines if the thread has multiple human interactions (more than one human message) - Measures average response lengths for both human and AI messages - Counts total questions asked in the thread (indicates information-seeking behavior) - Counts @mentions (indicates collaboration or escalation) - Estimates unique human participants in the thread</p> <p>Signals:</p> Signal Type Description <code>interaction_depth</code> int Number of back-and-forth exchanges <code>has_multiple_interactions</code> bool More than one human message <code>avg_human_response_length</code> float Average human message length (chars) <code>avg_ai_response_length</code> float Average AI message length (chars) <code>question_count</code> int Total questions asked <code>mention_count</code> int Total @mentions <code>unique_participants</code> int Count of unique human participants <p>KPIs supported:</p> <ul> <li><code>engagement_rate</code> = Avg interactions per case or % with multiple interactions</li> </ul>"},{"location":"metric-registry/shared/slack/#recommendationanalyzer","title":"RecommendationAnalyzer","text":"<p>File: <code>recommendation.py</code></p> <p>Purpose: Extract and classify AI recommendations from conversations to enable acceptance and override analysis.</p> <p>What it computes: - Scans AI messages for recommendation patterns (\"Recommend Approve\", \"Recommend Decline\", etc.) - Classifies recommendation type: approve, decline, review, hold, or none - Extracts the turn index where the recommendation was made - Parses confidence levels if present (\"high confidence\", \"confidence: 85%\") - Extracts case identifiers (MGT-BOP-XXXXXXX format) - Extracts priority/base scores from \"Base Score: XX/100\" patterns - Provides the foundation for AcceptanceDetector and OverrideDetector</p> <p>Signals:</p> Signal Type Description <code>has_recommendation</code> bool Whether AI made a recommendation <code>recommendation_type</code> str Type: approve, decline, review, hold, none <code>recommendation_turn_index</code> int Turn where recommendation was made <code>recommendation_confidence</code> float Extracted confidence level (0-1) <code>case_id</code> str Case identifier (e.g., MGT-BOP-123456) <code>case_priority</code> int Priority/base score (0-100) <p>Patterns detected: - \"Recommend Approve/Decline/Review/Hold\" - \"Base Score: XX/100\" - \"Priority Score: XX\" - Case IDs: MGT-BOP-XXXXXXX</p>"},{"location":"metric-registry/shared/slack/#escalationdetector","title":"EscalationDetector","text":"<p>File: <code>escalation.py</code></p> <p>Purpose: Identify when a conversation moves beyond AI assistance to require human team member involvement.</p> <p>What it computes: - Uses LLM to analyze conversation context and detect escalation patterns - Classifies the type of escalation (team mention, explicit handoff, error, complexity) - Identifies the turn where escalation occurred - Extracts @mentioned users who were brought into the conversation - Determines the reason for escalation - Falls back to heuristic pattern matching if LLM fails (detects @mentions after AI interaction, AI error messages) - Returns score of 1.0 if escalated, 0.0 if not</p> <p>Escalation Types:</p> Type Description <code>no_escalation</code> Normal AI-handled conversation <code>team_mention</code> User @mentioned team members <code>explicit_handoff</code> AI explicitly handed off to human <code>error_escalation</code> Escalation due to AI error <code>complexity_escalation</code> Escalation due to case complexity <p>Signals:</p> Signal Type Description <code>is_escalated</code> bool Whether conversation was escalated <code>escalation_type</code> str Type of escalation <code>escalation_turn_index</code> int Turn where escalation occurred <code>escalation_targets</code> list @mentioned users during escalation <code>escalation_reason</code> str Reason for escalation <p>KPIs supported:</p> <ul> <li><code>escalation_rate</code> = Escalated cases / Total AI cases</li> </ul>"},{"location":"metric-registry/shared/slack/#frustrationdetector","title":"FrustrationDetector","text":"<p>File: <code>frustration.py</code></p> <p>Purpose: Measure user satisfaction by detecting frustration signals in their messages.</p> <p>What it computes: - Uses LLM to analyze human messages for frustration indicators - Produces a continuous frustration score from 0.0 (calm) to 1.0 (very frustrated) - Identifies specific frustration indicators (repeated questions, ALL CAPS, multiple punctuation) - Determines the primary cause of frustration (AI error, wrong answer, poor understanding, etc.) - Finds the turn where frustration peaked - Classifies as \"frustrated\" if score &gt;= threshold (default 0.6) - Falls back to heuristic pattern matching if LLM fails</p> <p>Score interpretation:</p> Score Level 0.0-0.2 Calm, positive interaction 0.2-0.4 Mild impatience 0.4-0.6 Moderate frustration 0.6-0.8 Significant frustration 0.8-1.0 Severe frustration <p>Frustration causes: - <code>ai_error</code> - AI made a mistake - <code>slow_response</code> - Response time complaints - <code>wrong_answer</code> - Incorrect information - <code>repeated_questions</code> - User had to repeat themselves - <code>poor_understanding</code> - AI didn't understand - <code>system_issue</code> - Technical problems</p> <p>Signals:</p> Signal Type Description <code>frustration_score</code> float Overall score (0-1) <code>is_frustrated</code> bool Score &gt;= threshold (default 0.6) <code>frustration_indicators</code> list Detected signals <code>peak_frustration_turn</code> int Turn with highest frustration <code>frustration_cause</code> str Primary cause <p>KPIs supported:</p> <ul> <li><code>frustration_rate</code> = Frustrated interactions / Total interactions</li> </ul>"},{"location":"metric-registry/shared/slack/#acceptancedetector","title":"AcceptanceDetector","text":"<p>File: <code>acceptance.py</code></p> <p>Purpose: Determine whether users accepted or rejected AI recommendations to measure recommendation quality.</p> <p>What it computes: - First checks if a recommendation exists in the conversation (uses RecommendationAnalyzer patterns) - Uses LLM to analyze post-recommendation messages for acceptance signals - Classifies acceptance status: accepted, accepted_with_discussion, pending, rejected, modified - Identifies who made the final decision (if detectable) - Calculates turns between recommendation and decision - Maps acceptance status to a score: accepted=1.0, accepted_with_discussion=0.8, modified=0.5, pending/rejected=0.0 - Only runs analysis if a recommendation is found</p> <p>Acceptance statuses:</p> Status Description <code>accepted</code> Accepted without change <code>accepted_with_discussion</code> Accepted after team discussion <code>pending</code> No clear resolution <code>rejected</code> Explicitly rejected <code>modified</code> Accepted with modifications <p>Signals:</p> Signal Type Description <code>acceptance_status</code> str Status from above <code>is_accepted</code> bool accepted or accepted_with_discussion <code>acceptance_turn_index</code> int Turn where decision was made <code>decision_maker</code> str Who made the final decision <code>turns_to_decision</code> int Turns between recommendation and decision <p>KPIs supported:</p> <ul> <li><code>acceptance_rate</code> = Recommendations accepted / Total recommendations</li> </ul>"},{"location":"metric-registry/shared/slack/#overridedetector","title":"OverrideDetector","text":"<p>File: <code>override.py</code></p> <p>Purpose: Detect when human underwriters make decisions that contradict AI recommendations, indicating potential AI improvement opportunities.</p> <p>What it computes: - First checks if a recommendation exists in the conversation - Uses LLM to analyze if the human's final decision contradicts the AI recommendation - Classifies override type: no_override, full_override (opposite decision), partial_override (modified), pending_override - Captures the original AI recommendation and the actual final decision - Extracts the stated reason for the override from the conversation - Categorizes override reasons (additional_info, risk_assessment, policy_exception, etc.) - Returns score of 1.0 if overridden, 0.0 if not - Only runs analysis if a recommendation is found</p> <p>Override types:</p> Type Description <code>no_override</code> AI recommendation followed <code>full_override</code> Completely different decision <code>partial_override</code> Modified recommendation <code>pending_override</code> Discussion suggests override, not confirmed <p>Override reason categories: - <code>additional_info</code> - Based on info AI didn't have - <code>risk_assessment</code> - Different risk evaluation - <code>policy_exception</code> - Policy exception applied - <code>class_code_issue</code> - Class code concerns - <code>rate_issue</code> - Rating/pricing concerns - <code>experience_judgment</code> - Human judgment differs</p> <p>Signals:</p> Signal Type Description <code>is_overridden</code> bool Whether recommendation was overridden <code>override_type</code> str Type of override <code>original_recommendation</code> str What AI recommended <code>final_decision</code> str What was actually decided <code>override_reason</code> str Stated reason for override <code>override_reason_category</code> str Category from above <p>KPIs supported:</p> <ul> <li><code>override_rate</code> = Recommendations overridden / Total recommendations</li> </ul>"},{"location":"metric-registry/shared/slack/#overridesatisfactionanalyzer","title":"OverrideSatisfactionAnalyzer","text":"<p>File: <code>override.py</code></p> <p>Purpose: Evaluate whether override explanations are high-quality and actionable for improving AI recommendations.</p> <p>What it computes: - First runs OverrideDetector to check if an override occurred - Only analyzes threads where an override was detected - Uses LLM to evaluate the quality of the override explanation on three dimensions:   - Clear reason: Is there a specific, understandable justification?   - Supporting evidence: Does the explanation cite specific data or information?   - Actionable: Could this feedback help improve future AI recommendations? - Produces a satisfaction score from 0.0 (poor explanation) to 1.0 (excellent explanation) - Classifies as \"satisfactory\" if score &gt;= threshold (default 0.7) - Extracts improvement suggestions that could be used to retrain or improve the AI</p> <p>Quality criteria: - <code>has_clear_reason</code> - Specific, understandable justification - <code>has_supporting_evidence</code> - Cites specific information/data - <code>is_actionable</code> - Could help improve AI recommendations</p> <p>Score interpretation:</p> Score Quality 0.0-0.3 Vague or no reason given 0.3-0.5 Basic reason, lacks detail 0.5-0.7 Good reason with context 0.7-0.9 Clear reason with evidence 0.9-1.0 Excellent, comprehensive <p>Signals:</p> Signal Type Description <code>satisfaction_score</code> float Quality score (0-1) <code>is_satisfactory</code> bool Score &gt;= threshold (default 0.7) <code>has_clear_reason</code> bool Has stated reason <code>has_supporting_evidence</code> bool Has evidence <code>is_actionable</code> bool Provides actionable feedback <code>improvement_suggestions</code> list Suggestions for AI improvement <p>KPIs supported:</p> <ul> <li><code>override_satisfaction</code> = Satisfactory overrides / Total overrides</li> </ul>"},{"location":"metric-registry/shared/slack/#interventiondetector","title":"InterventionDetector","text":"<p>File: <code>intervention.py</code></p> <p>Purpose: Detect whether a human intervened in the thread and categorize the type of intervention and escalation.</p> <p>What it computes: - Uses LLM to classify intervention category (e.g., correction, missing context, approval) - Maps intervention into escalation type: <code>hard</code> / <code>soft</code> / <code>authority</code> / <code>none</code> - Flags STP (straight-through processing) when there is no human intervention - Extracts friction point and issue details when available</p> <p>Signals:</p> Signal Type Description <code>has_intervention</code> bool Human intervened <code>intervention_type</code> str Category of intervention <code>escalation_type</code> str hard/soft/authority/none <code>is_stp</code> bool Straight-through (no intervention/escalation) <code>friction_point</code> str Concept causing friction (optional) <code>issue_details</code> str Technical details (optional) <p>KPIs supported: - <code>intervention_rate</code> = Threads with intervention / Total threads - <code>stp_rate</code> = Threads with no intervention / Total threads</p>"},{"location":"metric-registry/shared/slack/#sentimentdetector","title":"SentimentDetector","text":"<p>File: <code>sentiment.py</code></p> <p>Purpose: Measure user sentiment in the thread and provide a score (negative \u2192 positive).</p> <p>What it computes: - Classifies sentiment: positive / neutral / frustrated / confused - Produces a calibrated <code>sentiment_score</code> (0.0-1.0) - Extracts frustration indicators and peak sentiment turn when available</p> <p>Signals:</p> Signal Type Description <code>sentiment</code> str Sentiment label <code>sentiment_score</code> float 0=negative, 1=positive <code>is_frustrated</code> bool Frustrated flag <code>frustration_indicators</code> list Detected negative signals <p>KPIs supported: - Sentiment distribution (positive/neutral/frustrated/confused) - <code>frustration_rate</code> (via sentiment/frustrated flag)</p>"},{"location":"metric-registry/shared/slack/#resolutiondetector","title":"ResolutionDetector","text":"<p>File: <code>resolution.py</code></p> <p>Purpose: Determine how the thread ended (approved/declined/blocked/needs_info/stalemate/pending) and whether it is resolved.</p> <p>What it computes: - Classifies final status and resolution type - Detects stalemate based on inactivity (when timestamps are provided in <code>additional_input</code>) - Optionally computes time-to-resolution</p> <p>Signals:</p> Signal Type Description <code>final_status</code> str approved/declined/blocked/needs_info/stalemate/pending <code>is_resolved</code> bool Resolved flag <code>is_stalemate</code> bool Inactive beyond threshold <code>time_to_resolution_seconds</code> float Time from first to last message (optional) <p>KPIs supported: - <code>resolution_rate</code> = Resolved / Total - <code>stalemate_rate</code> = Stalemates / Total</p>"},{"location":"metric-registry/shared/slack/#slackformattingcompliance","title":"SlackFormattingCompliance","text":"<p>File: <code>slack_compliance.py</code></p> <p>Purpose: Ensure AI-generated messages use correct Slack mrkdwn formatting that renders properly in Slack.</p> <p>What it computes: - Scans the AI's output text for formatting patterns that break in Slack - Detects <code>**bold**</code> (Slack uses single <code>*bold*</code> instead) - Detects <code># Headers</code> (not supported in Slack mrkdwn) - Detects unwrapped money/percentages that should be in backticks for clarity - Returns a compliance score where 1.0 = fully compliant, lower scores indicate issues - Lists specific formatting issues found with context snippets</p> <p>Checks performed: - No <code>**bold**</code> (Slack uses single <code>*bold*</code>) - No <code># Headers</code> (not supported in Slack) - Money/percentages should be in backticks</p> <p>Signals:</p> Signal Type Description <code>score</code> float Compliance score (1.0 = fully compliant) <code>issues</code> list List of formatting issues found"},{"location":"metric-registry/shared/slack/#utility-functions","title":"Utility Functions","text":"<p>File: <code>utils.py</code></p> <p>Common utilities used across metrics:</p> Function Description <code>parse_slack_metadata()</code> Extract thread_ts, channel_id, sender from additional_input <code>extract_mentions()</code> Extract @mentions from message text <code>get_human_messages()</code> Filter human messages from conversation <code>get_ai_messages()</code> Filter AI messages from conversation <code>find_recommendation_turn()</code> Find turn containing AI recommendation <code>extract_recommendation_type()</code> Extract approve/decline/review/hold <code>extract_case_id()</code> Extract case ID (MGT-BOP-XXXXXXX) <code>extract_priority_score()</code> Extract base/priority score <code>count_questions()</code> Count questions in text <code>build_transcript()</code> Build plain text transcript from conversation"},{"location":"metric-registry/shared/slack/acceptance/","title":"Acceptance Detector","text":"Detect whether AI recommendations were accepted LLM-Powered Classification Slack"},{"location":"metric-registry/shared/slack/acceptance/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Acceptance score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>conversation</code> Slack thread messages <p>What It Measures</p> <p>Acceptance Detector determines whether AI recommendations in Slack conversations were accepted by the human participants. It identifies the recommendation turn, analyzes post-recommendation messages, and classifies the acceptance status.</p> Score Interpretation 1.0  Recommendation explicitly accepted 0.5  Implicit or unclear acceptance 0.0  Recommendation rejected or overridden \u2705 Use When <ul> <li>Tracking recommendation adoption</li> <li>Measuring AI influence</li> <li>Understanding user behavior</li> <li>Building acceptance rate KPIs</li> </ul> \u274c Don't Use When <ul> <li>No recommendation present</li> <li>Thread is AI-only</li> <li>Measuring quality (use Decision Quality)</li> <li>Override analysis needed (use Override)</li> </ul>"},{"location":"metric-registry/shared/slack/acceptance/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.acceptance import AcceptanceDetector\n\nmetric = AcceptanceDetector()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Recommend Approve based on clean history.\"},\n        {\"role\": \"user\", \"content\": \"Thanks, proceeding with approval.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Accepted: {result.signals.is_accepted}\")\nprint(f\"Status: {result.signals.acceptance_status}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/acceptance/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca AcceptanceResult Structure <pre><code>AcceptanceResult(\n{\n    \"acceptance_status\": \"accepted\",\n    \"is_accepted\": true,\n    \"acceptance_turn_index\": 1,\n    \"decision_maker\": \"user\",\n    \"turns_to_decision\": 1,\n    \"has_recommendation\": true\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/acceptance/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>acceptance_status</code> <code>str</code> accepted, rejected, unclear <code>is_accepted</code> <code>bool</code> Whether recommendation was accepted <code>acceptance_turn_index</code> <code>int</code> Turn where acceptance occurred <code>decision_maker</code> <code>str</code> Who made the acceptance decision <code>turns_to_decision</code> <code>int</code> Messages between recommendation and decision <code>has_recommendation</code> <code>bool</code> Whether a recommendation was found"},{"location":"metric-registry/shared/slack/acceptance/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Acceptance Detector = Did the user accept the AI's recommendation?</p> <ul> <li>Use it when: Tracking whether AI recommendations are followed</li> <li>Score interpretation: 1.0 = accepted, 0.0 = rejected</li> <li>Key output: <code>is_accepted</code> boolean</li> </ul> <ul> <li> <p> Related Metrics</p> <p> Override Detector \u00b7 Recommendation Analyzer</p> </li> </ul>"},{"location":"metric-registry/shared/slack/composite/","title":"Slack Conversation Analyzer","text":"Comprehensive multi-metric analysis in a single pass LLM-Powered Composite Slack"},{"location":"metric-registry/shared/slack/composite/#at-a-glance","title":"At a Glance","text":"\ud83d\udcca Score Range <code>\u2014</code> Analysis only \u26a1 Default Threshold <code>\u2014</code> Varies by sub-metric \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>The Slack Conversation Analyzer runs multiple KPI metrics in a single optimized pass. It analyzes interaction patterns, engagement depth, recommendations, escalations, sentiment, and more\u2014returning a comprehensive signal structure for reporting.</p> Sub-Metric Type Description Interaction Analysis Message counts and patterns Engagement Analysis Depth and quality of exchanges Recommendation Analysis AI recommendation detection Escalation Classification Human escalation detection Frustration Score User frustration level Acceptance Classification Recommendation acceptance Override Classification Human override detection Intervention Classification Human intervention type Sentiment Score User sentiment analysis Resolution Classification Thread resolution status \u2705 Use When <ul> <li>Running multiple Slack metrics</li> <li>Building comprehensive dashboards</li> <li>Batch processing conversations</li> <li>Optimizing LLM call costs</li> </ul> \u274c Don't Use When <ul> <li>Only need one specific metric</li> <li>Custom metric configuration needed</li> <li>Non-Slack conversations</li> </ul>"},{"location":"metric-registry/shared/slack/composite/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>metrics</code> <code>list[str]</code> All Which metrics to run <code>frustration_threshold</code> <code>float</code> <code>0.6</code> Frustration score threshold <code>satisfaction_threshold</code> <code>float</code> <code>0.7</code> Satisfaction score threshold <code>sentiment_threshold</code> <code>float</code> <code>0.4</code> Sentiment score threshold <p>Available Metrics: <code>interaction</code>, <code>engagement</code>, <code>recommendation</code>, <code>escalation</code>, <code>frustration</code>, <code>acceptance</code>, <code>override</code>, <code>satisfaction</code>, <code>intervention</code>, <code>sentiment</code>, <code>resolution</code></p>"},{"location":"metric-registry/shared/slack/composite/#code-examples","title":"Code Examples","text":"Basic Usage Selective Metrics <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.composite import SlackConversationAnalyzer\n\nmetric = SlackConversationAnalyzer()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Recommend Approve.\"},\n        {\"role\": \"user\", \"content\": \"Thanks!\"},\n    ]\n)\n\nresult = await metric.execute(item)\n# Access all sub-metric signals\nprint(result.signals.interaction)\nprint(result.signals.sentiment)\nprint(result.signals.resolution)\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.composite import SlackConversationAnalyzer\n\n# Only run specific metrics\nmetric = SlackConversationAnalyzer(\n    metrics=[\"interaction\", \"sentiment\", \"resolution\"],\n    sentiment_threshold=0.4,\n)\n\nitem = DatasetItem(conversation=[...])\nresult = await metric.execute(item)\n</code></pre>"},{"location":"metric-registry/shared/slack/composite/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca CompositeResult Structure <pre><code>CompositeResult(\n{\n    \"interaction\": {\n        \"ai_message_count\": 2,\n        \"human_message_count\": 3,\n        \"is_interactive\": true\n    },\n    \"engagement\": {\n        \"interaction_depth\": 5,\n        \"question_count\": 2\n    },\n    \"recommendation\": {\n        \"has_recommendation\": true,\n        \"recommendation_type\": \"approve\"\n    },\n    \"sentiment\": {\n        \"sentiment\": \"positive\",\n        \"sentiment_score\": 0.75\n    },\n    \"resolution\": {\n        \"is_resolved\": true,\n        \"final_status\": \"approved\"\n    }\n    // ... other metrics\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/composite/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Slack Conversation Analyzer = All Slack KPI metrics in one efficient pass</p> <ul> <li>Use it when: You need multiple Slack metrics together</li> <li>Score interpretation: Varies by sub-metric</li> <li>Key benefit: Single LLM call for multiple insights</li> </ul> <ul> <li> <p> All Sub-Metrics</p> <p> Interaction \u00b7 Engagement \u00b7 Sentiment \u00b7 Resolution \u00b7 Override \u00b7 Escalation</p> </li> </ul>"},{"location":"metric-registry/shared/slack/engagement/","title":"Thread Engagement Analyzer","text":"Measure engagement depth within Slack threads Rule-Based Analysis Slack"},{"location":"metric-registry/shared/slack/engagement/#at-a-glance","title":"At a Glance","text":"\ud83d\udcca Score Range <code>\u2014</code> Analysis only \u26a1 Default Threshold <code>\u2014</code> Not applicable \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>Thread Engagement Analyzer measures the depth and quality of engagement within Slack threads. It counts back-and-forth exchanges, tracks response lengths, detects questions and @mentions, and identifies unique participants.</p> Signal Description <code>interaction_depth</code> Number of back-and-forth exchanges <code>question_count</code> Questions asked in the thread <code>mention_count</code> @mentions in the thread <code>unique_participants</code> Distinct users involved \u2705 Use When <ul> <li>Measuring conversation depth</li> <li>Identifying highly interactive threads</li> <li>Tracking participation patterns</li> <li>Building engagement KPIs</li> </ul> \u274c Don't Use When <ul> <li>Need quality assessment</li> <li>Single-message threads</li> <li>Measuring sentiment</li> </ul>"},{"location":"metric-registry/shared/slack/engagement/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.engagement import ThreadEngagementAnalyzer\n\nmetric = ThreadEngagementAnalyzer()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Here's my analysis.\"},\n        {\"role\": \"user\", \"content\": \"Can you clarify?\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, here's more detail.\"},\n        {\"role\": \"user\", \"content\": \"Thanks!\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Depth: {result.signals.interaction_depth}\")\nprint(f\"Questions: {result.signals.question_count}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/engagement/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca ThreadEngagementResult Structure <pre><code>ThreadEngagementResult(\n{\n    \"interaction_depth\": 4,\n    \"has_multiple_interactions\": true,\n    \"avg_human_response_length\": 45,\n    \"avg_ai_response_length\": 120,\n    \"question_count\": 1,\n    \"mention_count\": 0,\n    \"unique_participants\": 2\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/engagement/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>interaction_depth</code> <code>int</code> Back-and-forth exchange count <code>has_multiple_interactions</code> <code>bool</code> More than one exchange <code>avg_human_response_length</code> <code>int</code> Average human message length <code>avg_ai_response_length</code> <code>int</code> Average AI message length <code>question_count</code> <code>int</code> Questions detected <code>mention_count</code> <code>int</code> @mentions detected <code>unique_participants</code> <code>int</code> Distinct participants"},{"location":"metric-registry/shared/slack/engagement/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Thread Engagement Analyzer = How deep and interactive is this conversation?</p> <ul> <li>Use it when: Measuring conversation engagement patterns</li> <li>Output type: Analysis signals (no score)</li> <li>Key signals: <code>interaction_depth</code>, <code>question_count</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Interaction \u00b7 Sentiment \u00b7 Resolution</p> </li> </ul>"},{"location":"metric-registry/shared/slack/escalation/","title":"Escalation Detector","text":"Detect when conversations escalate to human team members LLM-Powered Classification Slack"},{"location":"metric-registry/shared/slack/escalation/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Classification score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>conversation</code> Slack thread messages <p>What It Measures</p> <p>Escalation Detector identifies when Slack conversations escalate to human team members. It analyzes @mentions, conversation flow, and message content to classify the type and reason for escalation.</p> Score Interpretation 1.0  Clear escalation detected 0.5  Possible escalation 0.0  No escalation \u2705 Use When <ul> <li>Tracking escalation rates</li> <li>Identifying AI limitations</li> <li>Measuring automation success</li> <li>Understanding escalation patterns</li> </ul> \u274c Don't Use When <ul> <li>No human participants possible</li> <li>All threads are human-handled</li> <li>Need detailed intervention type (use Intervention)</li> </ul>"},{"location":"metric-registry/shared/slack/escalation/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.escalation import EscalationDetector\n\nmetric = EscalationDetector()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"I'm unable to process this request.\"},\n        {\"role\": \"user\", \"content\": \"@team can someone help with this?\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Escalated: {result.signals.is_escalated}\")\nprint(f\"Type: {result.signals.escalation_type}\")\nprint(f\"Targets: {result.signals.escalation_targets}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/escalation/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca EscalationResult Structure <pre><code>EscalationResult(\n{\n    \"is_escalated\": true,\n    \"escalation_type\": \"team_mention\",\n    \"escalation_turn_index\": 1,\n    \"escalation_targets\": [\"@team\"],\n    \"escalation_reason\": \"User requested human assistance\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/escalation/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>is_escalated</code> <code>bool</code> Whether escalation occurred <code>escalation_type</code> <code>str</code> Type of escalation <code>escalation_turn_index</code> <code>int</code> Turn where escalation occurred <code>escalation_targets</code> <code>List[str]</code> Who was escalated to <code>escalation_reason</code> <code>str</code> Reason for escalation"},{"location":"metric-registry/shared/slack/escalation/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Escalation Detector = Did this conversation escalate to a human?</p> <ul> <li>Use it when: Tracking when AI needs human help</li> <li>Score interpretation: 1.0 = escalated, 0.0 = no escalation</li> <li>Key signals: <code>is_escalated</code>, <code>escalation_type</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Intervention \u00b7 Frustration \u00b7 Resolution</p> </li> </ul>"},{"location":"metric-registry/shared/slack/frustration/","title":"Frustration Detector","text":"Score user frustration levels in conversations LLM-Powered Score Slack"},{"location":"metric-registry/shared/slack/frustration/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Frustration level \u26a1 Default Threshold <code>0.6</code> Frustrated flag cutoff \ud83d\udccb Required Inputs <code>conversation</code> Slack thread messages <p>What It Measures</p> <p>Frustration Detector scores user frustration levels in Slack conversations. It analyzes human messages for frustration indicators, identifies the cause, and flags conversations where frustration exceeds the threshold.</p> Score Interpretation 0.0-0.3  Calm, satisfied user 0.4-0.6  Mild frustration 0.7-1.0  High frustration \u2705 Use When <ul> <li>Monitoring user experience</li> <li>Detecting problematic interactions</li> <li>Building frustration KPIs</li> <li>Alerting on high frustration</li> </ul> \u274c Don't Use When <ul> <li>No human messages in thread</li> <li>Need broader sentiment (use Sentiment)</li> <li>Analyzing AI-only responses</li> </ul>"},{"location":"metric-registry/shared/slack/frustration/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.frustration import FrustrationDetector\n\nmetric = FrustrationDetector()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Processing your request.\"},\n        {\"role\": \"user\", \"content\": \"This is the third time I've asked!\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.signals.frustration_score}\")\nprint(f\"Frustrated: {result.signals.is_frustrated}\")\nprint(f\"Cause: {result.signals.frustration_cause}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/frustration/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca FrustrationResult Structure <pre><code>FrustrationResult(\n{\n    \"frustration_score\": 0.75,\n    \"is_frustrated\": true,\n    \"frustration_indicators\": [\"repetition\", \"exclamation\"],\n    \"peak_frustration_turn\": 1,\n    \"frustration_cause\": \"Repeated requests without resolution\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/frustration/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>frustration_score</code> <code>float</code> Frustration level 0.0-1.0 <code>is_frustrated</code> <code>bool</code> Score exceeds threshold <code>frustration_indicators</code> <code>List[str]</code> Detected frustration signals <code>peak_frustration_turn</code> <code>int</code> Highest frustration turn <code>frustration_cause</code> <code>str</code> Identified cause"},{"location":"metric-registry/shared/slack/frustration/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Frustration Detector = How frustrated is the user?</p> <ul> <li>Use it when: Monitoring user frustration in conversations</li> <li>Score interpretation: Higher = more frustrated</li> <li>Key signals: <code>frustration_score</code>, <code>is_frustrated</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Sentiment \u00b7 Escalation \u00b7 Intervention</p> </li> </ul>"},{"location":"metric-registry/shared/slack/interaction/","title":"Slack Interaction Analyzer","text":"Extract interaction signals from Slack threads Rule-Based Analysis Slack"},{"location":"metric-registry/shared/slack/interaction/#at-a-glance","title":"At a Glance","text":"\ud83d\udcca Score Range <code>\u2014</code> Analysis only \u26a1 Default Threshold <code>\u2014</code> Not applicable \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>Slack Interaction Analyzer extracts basic interaction signals from Slack threads for KPI aggregation. It counts messages by role, determines interactivity, and extracts thread metadata.</p> Signal Description <code>ai_message_count</code> Number of AI messages <code>human_message_count</code> Number of human messages <code>is_interactive</code> Whether the thread has back-and-forth <code>is_ai_initiated</code> Whether AI started the thread \u2705 Use When <ul> <li>Building KPI dashboards</li> <li>Counting interactions</li> <li>Identifying interactive vs. one-way threads</li> <li>Extracting thread metadata</li> </ul> \u274c Don't Use When <ul> <li>Need engagement depth (use Engagement)</li> <li>Need quality analysis</li> <li>Need sentiment analysis</li> </ul>"},{"location":"metric-registry/shared/slack/interaction/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.interaction import SlackInteractionAnalyzer\n\nmetric = SlackInteractionAnalyzer()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Hello, how can I help?\"},\n        {\"role\": \"user\", \"content\": \"I need help with a quote.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, here's the analysis.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"AI Messages: {result.signals.ai_message_count}\")\nprint(f\"Human Messages: {result.signals.human_message_count}\")\nprint(f\"Interactive: {result.signals.is_interactive}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/interaction/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca SlackInteractionResult Structure <pre><code>SlackInteractionResult(\n{\n    \"ai_message_count\": 2,\n    \"human_message_count\": 1,\n    \"total_turn_count\": 3,\n    \"is_interactive\": true,\n    \"is_ai_initiated\": true,\n    \"has_human_response\": true,\n    \"thread_id\": \"thread_123\",\n    \"channel_id\": \"channel_456\",\n    \"sender\": \"athena_bot\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/interaction/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>ai_message_count</code> <code>int</code> AI message count <code>human_message_count</code> <code>int</code> Human message count <code>total_turn_count</code> <code>int</code> Total messages <code>is_interactive</code> <code>bool</code> Has human participation <code>is_ai_initiated</code> <code>bool</code> AI sent first message <code>has_human_response</code> <code>bool</code> Human replied <code>thread_id</code> <code>str</code> Thread identifier <code>channel_id</code> <code>str</code> Channel identifier <code>sender</code> <code>str</code> Initiating sender"},{"location":"metric-registry/shared/slack/interaction/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Slack Interaction Analyzer = Basic interaction counts and metadata</p> <ul> <li>Use it when: Need simple interaction metrics for dashboards</li> <li>Output type: Analysis signals (no score)</li> <li>Key signals: <code>ai_message_count</code>, <code>human_message_count</code>, <code>is_interactive</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Engagement \u00b7 Recommendation \u00b7 Resolution</p> </li> </ul>"},{"location":"metric-registry/shared/slack/intervention/","title":"Intervention Detector","text":"Detect and classify human intervention types LLM-Powered Classification Slack"},{"location":"metric-registry/shared/slack/intervention/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Classification score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>Intervention Detector identifies when a human intervenes in an AI-driven Slack thread and classifies the intervention type. It distinguishes between soft escalations (clarifications), hard escalations (corrections), authority escalations (approvals), and STP (straight-through processing with no human involvement).</p> Escalation Type Description none No intervention - STP workflow soft Clarification or minor adjustment hard Correction or significant change authority Approval or authorization required \u2705 Use When <ul> <li>Measuring STP rates</li> <li>Categorizing intervention types</li> <li>Identifying friction points</li> <li>Understanding human involvement</li> </ul> \u274c Don't Use When <ul> <li>Only need escalation detection (use Escalation)</li> <li>Measuring sentiment</li> <li>All threads require human input</li> </ul>"},{"location":"metric-registry/shared/slack/intervention/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.intervention import InterventionDetector\n\nmetric = InterventionDetector()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Recommend Approve.\"},\n        {\"role\": \"user\", \"content\": \"I need to override - decline due to roof age.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Has Intervention: {result.signals.has_intervention}\")\nprint(f\"Type: {result.signals.intervention_type}\")\nprint(f\"Escalation: {result.signals.escalation_type}\")\nprint(f\"Is STP: {result.signals.is_stp}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/intervention/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca InterventionResult Structure <pre><code>InterventionResult(\n{\n    \"has_intervention\": true,\n    \"intervention_type\": \"override\",\n    \"escalation_type\": \"hard\",\n    \"is_hard_escalation\": true,\n    \"is_soft_escalation\": false,\n    \"is_authority_escalation\": false,\n    \"is_stp\": false,\n    \"human_message_count\": 1,\n    \"friction_point\": \"AI recommendation contradicted by user\",\n    \"issue_details\": \"User disagrees with approve decision\",\n    \"intervention_summary\": \"Override to decline\",\n    \"reasoning\": \"User explicitly overrode AI recommendation\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/intervention/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>has_intervention</code> <code>bool</code> Whether human intervened <code>intervention_type</code> <code>str</code> Type of intervention <code>escalation_type</code> <code>str</code> hard, soft, authority, or none <code>is_hard_escalation</code> <code>bool</code> Correction/significant change <code>is_soft_escalation</code> <code>bool</code> Clarification/minor adjustment <code>is_authority_escalation</code> <code>bool</code> Approval required <code>is_stp</code> <code>bool</code> Straight-through processing <code>friction_point</code> <code>str</code> Where friction occurred <code>issue_details</code> <code>str</code> Details of the issue"},{"location":"metric-registry/shared/slack/intervention/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Intervention Detector = What type of human intervention occurred?</p> <ul> <li>Use it when: Classifying intervention types and measuring STP</li> <li>Score interpretation: 1.0 = intervention, 0.0 = STP</li> <li>Key signals: <code>intervention_type</code>, <code>escalation_type</code>, <code>is_stp</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Escalation \u00b7 Override \u00b7 Resolution</p> </li> </ul>"},{"location":"metric-registry/shared/slack/override/","title":"Override Detector","text":"Detect when humans override AI recommendations LLM-Powered Classification Slack"},{"location":"metric-registry/shared/slack/override/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Classification score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>conversation</code> Slack thread messages <p>What It Measures</p> <p>Override Detector identifies when humans override AI recommendations in Slack conversations. It detects the recommendation, identifies the override, extracts the original vs. final decision, and categorizes the override reason.</p> Score Interpretation 1.0  Clear override detected 0.5  Possible override 0.0  No override \u2705 Use When <ul> <li>Tracking override rates</li> <li>Understanding AI vs. human decisions</li> <li>Identifying improvement areas</li> <li>Building override KPIs</li> </ul> \u274c Don't Use When <ul> <li>No AI recommendation present</li> <li>Need acceptance tracking (use Acceptance)</li> <li>Measuring general intervention (use Intervention)</li> </ul>"},{"location":"metric-registry/shared/slack/override/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.override import OverrideDetector\n\nmetric = OverrideDetector()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Recommend Approve based on the data.\"},\n        {\"role\": \"user\", \"content\": \"We will decline anyway due to prior relationship.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Overridden: {result.signals.is_overridden}\")\nprint(f\"Original: {result.signals.original_recommendation}\")\nprint(f\"Final: {result.signals.final_decision}\")\nprint(f\"Reason: {result.signals.override_reason}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/override/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca OverrideResult Structure <pre><code>OverrideResult(\n{\n    \"is_overridden\": true,\n    \"override_type\": \"decision_change\",\n    \"original_recommendation\": \"approve\",\n    \"final_decision\": \"decline\",\n    \"override_reason\": \"Prior relationship concerns\",\n    \"override_reason_category\": \"business_judgment\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/override/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>is_overridden</code> <code>bool</code> Whether override occurred <code>override_type</code> <code>str</code> Type of override <code>original_recommendation</code> <code>str</code> AI's recommendation <code>final_decision</code> <code>str</code> Human's final decision <code>override_reason</code> <code>str</code> Stated reason for override <code>override_reason_category</code> <code>str</code> Categorized reason"},{"location":"metric-registry/shared/slack/override/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Override Detector = Did the human override the AI's recommendation?</p> <ul> <li>Use it when: Tracking when AI recommendations are changed</li> <li>Score interpretation: 1.0 = overridden, 0.0 = no override</li> <li>Key signals: <code>is_overridden</code>, <code>original_recommendation</code>, <code>final_decision</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Acceptance \u00b7 Override Satisfaction \u00b7 Intervention</p> </li> </ul>"},{"location":"metric-registry/shared/slack/override_satisfaction/","title":"Override Satisfaction Analyzer","text":"Score the quality of override explanations LLM-Powered Score Slack"},{"location":"metric-registry/shared/slack/override_satisfaction/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Satisfaction score \u26a1 Default Threshold <code>0.7</code> Satisfactory cutoff \ud83d\udccb Required Inputs <code>conversation</code> Slack thread with override <p>What It Measures</p> <p>Override Satisfaction Analyzer scores the quality of override explanations in Slack conversations. It evaluates whether overrides have clear reasons, supporting evidence, and actionable guidance. Only runs when an override is detected.</p> Score Interpretation 0.8-1.0  Well-documented override 0.5-0.7  Adequate explanation &lt; 0.5  Poor or missing explanation \u2705 Use When <ul> <li>Ensuring overrides are documented</li> <li>Building audit trails</li> <li>Improving override quality</li> <li>Compliance requirements</li> </ul> \u274c Don't Use When <ul> <li>No override present</li> <li>Only detecting overrides (use Override)</li> <li>Documentation not required</li> </ul>"},{"location":"metric-registry/shared/slack/override_satisfaction/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>satisfaction_threshold</code> <code>float</code> <code>0.7</code> Minimum score for satisfactory"},{"location":"metric-registry/shared/slack/override_satisfaction/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.override import OverrideSatisfactionAnalyzer\n\nmetric = OverrideSatisfactionAnalyzer()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Recommend Approve.\"},\n        {\"role\": \"user\", \"content\": \"Override to Decline - roof is 25 years old per inspection report, exceeds our 20-year guideline.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.signals.satisfaction_score}\")\nprint(f\"Satisfactory: {result.signals.is_satisfactory}\")\nprint(f\"Has Clear Reason: {result.signals.has_clear_reason}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/override_satisfaction/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca SatisfactionResult Structure <pre><code>SatisfactionResult(\n{\n    \"satisfaction_score\": 0.85,\n    \"is_satisfactory\": true,\n    \"has_clear_reason\": true,\n    \"has_supporting_evidence\": true,\n    \"is_actionable\": true,\n    \"improvement_suggestions\": []\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/override_satisfaction/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>satisfaction_score</code> <code>float</code> Quality score 0.0-1.0 <code>is_satisfactory</code> <code>bool</code> Meets threshold <code>has_clear_reason</code> <code>bool</code> Override reason is clear <code>has_supporting_evidence</code> <code>bool</code> Evidence provided <code>is_actionable</code> <code>bool</code> Contains actionable info <code>improvement_suggestions</code> <code>List[str]</code> How to improve"},{"location":"metric-registry/shared/slack/override_satisfaction/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Override Satisfaction Analyzer = Was the override well-documented?</p> <ul> <li>Use it when: Evaluating override explanation quality</li> <li>Score interpretation: Higher = better documentation</li> <li>Key signals: <code>satisfaction_score</code>, <code>has_clear_reason</code>, <code>has_supporting_evidence</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Override \u00b7 Acceptance \u00b7 Intervention</p> </li> </ul>"},{"location":"metric-registry/shared/slack/recommendation/","title":"Recommendation Analyzer","text":"Extract AI recommendations from Slack conversations Rule-Based Analysis Slack"},{"location":"metric-registry/shared/slack/recommendation/#at-a-glance","title":"At a Glance","text":"\ud83d\udcca Score Range <code>\u2014</code> Analysis only \u26a1 Default Threshold <code>\u2014</code> Not applicable \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>Recommendation Analyzer extracts AI recommendations from Slack conversations for KPI aggregation. It identifies recommendation type, turn index, confidence, and extracts case identifiers and priority scores.</p> Signal Description <code>has_recommendation</code> Whether AI made a recommendation <code>recommendation_type</code> approve, decline, refer, etc. <code>recommendation_confidence</code> AI's confidence level <code>case_id</code> Extracted case identifier \u2705 Use When <ul> <li>Tracking recommendation patterns</li> <li>Building KPI dashboards</li> <li>Extracting case metadata</li> <li>Analyzing recommendation distribution</li> </ul> \u274c Don't Use When <ul> <li>Need acceptance tracking (use Acceptance)</li> <li>Need override detection (use Override)</li> <li>Evaluating quality</li> </ul>"},{"location":"metric-registry/shared/slack/recommendation/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.recommendation import RecommendationAnalyzer\n\nmetric = RecommendationAnalyzer()\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Case #12345: Recommend Approve with high confidence.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Has Recommendation: {result.signals.has_recommendation}\")\nprint(f\"Type: {result.signals.recommendation_type}\")\nprint(f\"Case ID: {result.signals.case_id}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/recommendation/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca RecommendationResult Structure <pre><code>RecommendationResult(\n{\n    \"has_recommendation\": true,\n    \"recommendation_type\": \"approve\",\n    \"recommendation_turn_index\": 0,\n    \"recommendation_confidence\": 0.95,\n    \"case_id\": \"12345\",\n    \"case_priority\": \"high\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/recommendation/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>has_recommendation</code> <code>bool</code> Recommendation found <code>recommendation_type</code> <code>str</code> Type of recommendation <code>recommendation_turn_index</code> <code>int</code> Turn with recommendation <code>recommendation_confidence</code> <code>float</code> AI's confidence <code>case_id</code> <code>str</code> Extracted case ID <code>case_priority</code> <code>str</code> Case priority level"},{"location":"metric-registry/shared/slack/recommendation/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Recommendation Analyzer = What did the AI recommend?</p> <ul> <li>Use it when: Extracting recommendation metadata</li> <li>Output type: Analysis signals (no score)</li> <li>Key signals: <code>has_recommendation</code>, <code>recommendation_type</code>, <code>case_id</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Acceptance \u00b7 Override \u00b7 Resolution</p> </li> </ul>"},{"location":"metric-registry/shared/slack/resolution/","title":"Resolution Detector","text":"Determine the final outcome and resolution status LLM-Powered Classification Slack"},{"location":"metric-registry/shared/slack/resolution/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Classification score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>Resolution Detector determines the final outcome of a Slack thread and whether the conversation is resolved. It classifies the status (approved, declined, blocked, needs_info, stalemate, pending) and detects stalemates based on inactivity.</p> Status Description approved Case approved and resolved declined Case declined and resolved blocked Blocked pending external action needs_info Waiting for information stalemate Inactive for extended period pending Still in progress \u2705 Use When <ul> <li>Tracking resolution rates</li> <li>Identifying stale threads</li> <li>Measuring time to resolution</li> <li>Building outcome KPIs</li> </ul> \u274c Don't Use When <ul> <li>Only need recommendation (use Recommendation)</li> <li>Thread just started</li> <li>Measuring sentiment</li> </ul>"},{"location":"metric-registry/shared/slack/resolution/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>stalemate_hours</code> <code>float</code> <code>72.0</code> Inactivity threshold for stalemate"},{"location":"metric-registry/shared/slack/resolution/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.resolution import ResolutionDetector\n\nmetric = ResolutionDetector(stalemate_hours=72.0)\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Recommend Approve.\"},\n        {\"role\": \"user\", \"content\": \"Proceeding with approval. Binding now.\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Status: {result.signals.final_status}\")\nprint(f\"Resolved: {result.signals.is_resolved}\")\nprint(f\"Stalemate: {result.signals.is_stalemate}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/resolution/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca ResolutionResult Structure <pre><code>ResolutionResult(\n{\n    \"final_status\": \"approved\",\n    \"is_resolved\": true,\n    \"resolution_type\": \"explicit\",\n    \"is_stalemate\": false,\n    \"time_to_resolution_seconds\": 3600,\n    \"message_count\": 2,\n    \"reasoning\": \"User explicitly confirmed approval and binding\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/resolution/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_status</code> <code>str</code> Final outcome status <code>is_resolved</code> <code>bool</code> Whether resolved <code>resolution_type</code> <code>str</code> How it was resolved <code>is_stalemate</code> <code>bool</code> Inactive too long <code>time_to_resolution_seconds</code> <code>int</code> Time to resolve <code>message_count</code> <code>int</code> Total messages <code>reasoning</code> <code>str</code> Explanation"},{"location":"metric-registry/shared/slack/resolution/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Resolution Detector = What's the final outcome and is it resolved?</p> <ul> <li>Use it when: Tracking thread resolution and outcomes</li> <li>Score interpretation: 1.0 = resolved, 0.0 = unresolved</li> <li>Key signals: <code>final_status</code>, <code>is_resolved</code>, <code>is_stalemate</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Recommendation \u00b7 Acceptance \u00b7 Override</p> </li> </ul>"},{"location":"metric-registry/shared/slack/sentiment/","title":"Sentiment Detector","text":"Detect user sentiment in Slack threads LLM-Powered Score Slack"},{"location":"metric-registry/shared/slack/sentiment/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Higher = more positive \u26a1 Default Threshold <code>0.4</code> Below = frustrated \ud83d\udccb Required Inputs <code>conversation</code> Optional: additional_input <p>What It Measures</p> <p>Sentiment Detector analyzes user sentiment in Slack threads, classifying it as positive, neutral, frustrated, or confused. It returns a calibrated sentiment score and identifies frustration indicators.</p> Score Interpretation 0.7-1.0  Positive sentiment 0.4-0.7  Neutral sentiment 0.0-0.4  Frustrated/negative \u2705 Use When <ul> <li>Monitoring overall user experience</li> <li>Tracking satisfaction trends</li> <li>Building sentiment KPIs</li> <li>Identifying problematic interactions</li> </ul> \u274c Don't Use When <ul> <li>No human messages present</li> <li>Need specific frustration (use Frustration)</li> <li>AI-only threads</li> </ul>"},{"location":"metric-registry/shared/slack/sentiment/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>frustration_threshold</code> <code>float</code> <code>0.4</code> Score below which user is flagged frustrated"},{"location":"metric-registry/shared/slack/sentiment/#code-examples","title":"Code Examples","text":"Basic Usage <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.sentiment import SentimentDetector\n\nmetric = SentimentDetector(frustration_threshold=0.4)\n\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"assistant\", \"content\": \"Here's the recommendation.\"},\n        {\"role\": \"user\", \"content\": \"This still doesn't work??\"},\n    ]\n)\n\nresult = await metric.execute(item)\nprint(f\"Sentiment: {result.signals.sentiment}\")\nprint(f\"Score: {result.signals.sentiment_score}\")\nprint(f\"Frustrated: {result.signals.is_frustrated}\")\n</code></pre>"},{"location":"metric-registry/shared/slack/sentiment/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca SentimentResult Structure <pre><code>SentimentResult(\n{\n    \"sentiment\": \"frustrated\",\n    \"sentiment_score\": 0.3,\n    \"is_frustrated\": true,\n    \"is_positive\": false,\n    \"is_confused\": true,\n    \"frustration_indicators\": [\"question marks\", \"negative phrasing\"],\n    \"peak_sentiment_turn\": 1,\n    \"human_message_count\": 1,\n    \"reasoning\": \"User shows frustration with repeated issues\"\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/sentiment/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>sentiment</code> <code>str</code> positive, neutral, frustrated, confused <code>sentiment_score</code> <code>float</code> Calibrated score 0.0-1.0 <code>is_frustrated</code> <code>bool</code> Score below threshold <code>is_positive</code> <code>bool</code> Positive sentiment <code>is_confused</code> <code>bool</code> User seems confused <code>frustration_indicators</code> <code>List[str]</code> Detected indicators <code>peak_sentiment_turn</code> <code>int</code> Most significant turn"},{"location":"metric-registry/shared/slack/sentiment/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Sentiment Detector = What's the user's overall sentiment?</p> <ul> <li>Use it when: Monitoring user experience and satisfaction</li> <li>Score interpretation: Higher = more positive</li> <li>Key signals: <code>sentiment</code>, <code>sentiment_score</code>, <code>is_frustrated</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Frustration \u00b7 Escalation \u00b7 Resolution</p> </li> </ul>"},{"location":"metric-registry/shared/slack/slack_compliance/","title":"Slack Formatting Compliance","text":"Ensure output adheres to Slack mrkdwn formatting rules Rule-Based Compliance Slack"},{"location":"metric-registry/shared/slack/slack_compliance/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Compliance score \u26a1 Default Threshold <code>1.0</code> No violations allowed \ud83d\udccb Required Inputs <code>actual_output</code> AI response text <p>What It Measures</p> <p>Slack Formatting Compliance ensures AI output adheres to Slack mrkdwn rules. It detects violations like <code>**bold**</code> (should be <code>*bold*</code>), <code># Headers</code> (not supported), and unwrapped numbers/currency that should use backticks.</p> Issue Description Double asterisk <code>**bold**</code> should be <code>*bold*</code> Markdown headers <code># Header</code> not supported Unwrapped values Numbers/currency should use backticks \u2705 Use When <ul> <li>Validating Slack message formatting</li> <li>Ensuring consistent presentation</li> <li>Enforcing style guidelines</li> <li>Pre-send validation</li> </ul> \u274c Don't Use When <ul> <li>Non-Slack output</li> <li>Markdown-based platforms</li> <li>Formatting doesn't matter</li> </ul>"},{"location":"metric-registry/shared/slack/slack_compliance/#code-examples","title":"Code Examples","text":"Basic Usage Compliant Output <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.slack_compliance import SlackFormattingCompliance\n\nmetric = SlackFormattingCompliance()\n\nitem = DatasetItem(\n    actual_output=\"**Header** # Bad $500\"\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.score}\")\nprint(f\"Issues: {result.signals.issues}\")\n</code></pre> <pre><code>from axion.dataset import DatasetItem\nfrom eval_workbench.shared.metrics.slack.slack_compliance import SlackFormattingCompliance\n\nmetric = SlackFormattingCompliance()\n\nitem = DatasetItem(\n    actual_output=\"*Header* with `$500` properly formatted\"\n)\n\nresult = await metric.execute(item)\nprint(f\"Score: {result.score}\")  # 1.0\n</code></pre>"},{"location":"metric-registry/shared/slack/slack_compliance/#metric-diagnostics","title":"Metric Diagnostics","text":"\ud83d\udcca FormattingResult Structure <pre><code>FormattingResult(\n{\n    \"score\": 0.7,\n    \"issues\": [\n        {\n            \"type\": \"double_asterisk\",\n            \"context\": \"**Header**\",\n            \"count\": 1\n        },\n        {\n            \"type\": \"markdown_header\",\n            \"context\": \"# Bad\",\n            \"count\": 1\n        },\n        {\n            \"type\": \"unwrapped_currency\",\n            \"context\": \"$500\",\n            \"count\": 1\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/shared/slack/slack_compliance/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Compliance score (deductions per issue) <code>issues</code> <code>List</code> Detected formatting issues"},{"location":"metric-registry/shared/slack/slack_compliance/#issue-types","title":"Issue Types","text":"Type Deduction Description <code>double_asterisk</code> -0.1 <code>**text**</code> instead of <code>*text*</code> <code>markdown_header</code> -0.1 <code># Header</code> used <code>unwrapped_number</code> -0.05 Numbers not in backticks <code>unwrapped_currency</code> -0.05 Currency not in backticks"},{"location":"metric-registry/shared/slack/slack_compliance/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Slack Formatting Compliance = Does the output follow Slack mrkdwn rules?</p> <ul> <li>Use it when: Validating Slack message formatting</li> <li>Score interpretation: 1.0 = compliant, lower = violations</li> <li>Key signals: <code>score</code>, <code>issues</code></li> </ul> <ul> <li> <p> Related Metrics</p> <p> Interaction \u00b7 Recommendation</p> </li> </ul>"}]}